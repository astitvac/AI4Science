{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔬 Aavishkar.ai Expert System Notebook\n",
    "\n",
    "<img src=\"https://github.com/astitvac/AI4Science/raw/main/assets/AA_Main_Banner.jpg\" alt=\"Aavishkar.ai Banner\" width=\"600\"/>\n",
    "\n",
    "### <span style=\"color:#6C5CE7;\">AI for Science</span>\n",
    "<small><i>Democratizing advanced AI capabilities for scientific research</i></small>\n",
    "\n",
    "---\n",
    "\n",
    "## 👋 Welcome to Aavishkar.ai Expert Systems!\n",
    "\n",
    "<small>This notebook is part of the <b>Aavishkar.ai AI4Science</b> initiative, which develops LLM-based expert systems to enhance scientific research workflows. Our expert systems formalize scientific cognitive processes using Large Language Models, structured knowledge representations, and interactive interfaces.</small>\n",
    "\n",
    "### 🧠 About This Expert System\n",
    "\n",
    "<small>This notebook implements one of the five scientific cognitive archetypes developed by Aavishkar.ai:</small>\n",
    "\n",
    "<small>\n",
    "1. 📚 <b>Literature Synthesist</b>: Identifies patterns, contradictions, and knowledge gaps across research corpora<br>\n",
    "2. 🧪 <b>Experimental Architect</b>: Translates abstract hypotheses into methodologically sound experimental designs<br>\n",
    "3. 📊 <b>Analytical Navigator</b>: Constructs adaptive analytical pathways through complex datasets<br>\n",
    "4. 📝 <b>Research Documentarian</b>: Structures and articulates scientific findings and methodologies<br>\n",
    "5. 🔄 <b>Interdisciplinary Translator</b>: Establishes conceptual bridges between disparate knowledge domains\n",
    "</small>\n",
    "\n",
    "### 👥 Who Can Use This?\n",
    "\n",
    "<small>\n",
    "Aavishkar.ai tools are designed for all practitioners of hypothesis-driven science:<br>\n",
    "• 🎓 Academic researchers and students<br>\n",
    "• 🏢 Commercial/industrial researchers<br>\n",
    "• 🏛️ Government scientists<br>\n",
    "• 🔭 Citizen scientists<br>\n",
    "• 🧩 Independent researchers\n",
    "</small>\n",
    "\n",
    "<small>No matter your technical background or institutional affiliation, this notebook provides accessible AI capabilities for rigorous scientific work.</small>\n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ Setup Instructions\n",
    "\n",
    "<small>\n",
    "\n",
    "**Google Colab**\n",
    "* Click on \"Runtime\" in the menu\n",
    "* Select \"Run all\" to install dependencies and initialize the system\n",
    "* Ensure you have your API keys ready for the LLM provider\n",
    "\n",
    "**Local Environment**\n",
    "* Ensure you have Python 3.8+ installed\n",
    "* Install dependencies by running the installation cell below\n",
    "* Set up your API keys as instructed in the initialization section\n",
    "\n",
    "**Prerequisites**\n",
    "* Python 3.8+\n",
    "* API key for OpenAI or Google Vertex AI\n",
    "* Basic familiarity with Jupyter notebooks\n",
    "\n",
    "</small>\n",
    "\n",
    "---\n",
    "\n",
    "### 📜 License\n",
    "\n",
    "<small>This project is licensed under the <b>MIT License</b></small>\n",
    "\n",
    "<small>\n",
    "<details>\n",
    "<summary>View License Text</summary>\n",
    "MIT License<br><br>\n",
    "Copyright (c) 2023-2024 Aavishkar.ai<br><br>\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:<br><br>\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "</details>\n",
    "</small>\n",
    "\n",
    "<small>\n",
    "\n",
    "### 🔗 Connect with Aavishkar.ai\n",
    "* 📦 **GitHub**: [github.com/astitvac/AI4Science](https://github.com/astitvac/AI4Science)\n",
    "* 🌐 **Website**: [aavishkar.ai](https://aavishkar.ai)\n",
    "* 💬 **Community**: [Discord](https://discord.gg/aavishkar)\n",
    "* 🤝 **Contribute**: [Contribution Guidelines](https://github.com/astitvac/AI4Science/tree/main/Contributing)\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚙️ Installation\n",
    "<small>\n",
    "This cell installs all required dependencies for this expert system notebook. The installation process uses uv for faster package management when available, with automatic fallback to standard pip.\n",
    "Key components being installed:\n",
    "LLM frameworks: LangChain and provider-specific libraries\n",
    "Data modeling: Pydantic\n",
    "UI: Gradio\n",
    "Core utilities: Data processing and visualization libraries\n",
    "Troubleshooting tips:\n",
    "If you encounter errors, try running the cell again\n",
    "For persistent issues, check your Python version (3.8+ required)\n",
    "In Colab, restart the runtime if packages aren't recognized after installation\n",
    "Note: Initial installation may take 1-2 minutes to complete. A confirmation message will appear when successful.\n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation\n",
    "import sys, os, subprocess, time\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# All required packages (no version constraints for better future-proofing)\n",
    "ALL_PACKAGES = {\n",
    "    \"core\": \"langchain pydantic python-dotenv uuid\",\n",
    "    \"providers\": \"langchain-openai langchain-google-vertexai langchain-community\",\n",
    "    \"ui\": \"gradio\",\n",
    "    \"data\": \"numpy pandas matplotlib plotly networkx\",\n",
    "    \"documents\": \"pypdf PyPDF2 pillow\",\n",
    "    \"vectors\": \"chromadb sentence-transformers scikit-learn\",\n",
    "    \"parallel\": \"joblib\"\n",
    "}\n",
    "\n",
    "# Environment detection\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "def show(msg, type=\"info\"):\n",
    "    \"\"\"Display styled message\"\"\"\n",
    "    colors = {\"info\": \"#3a7bd5\", \"success\": \"#00c853\", \"warning\": \"#f57c00\", \"error\": \"#d50000\"}\n",
    "    icons = {\"info\": \"ℹ️\", \"success\": \"✅\", \"warning\": \"⚠️\", \"error\": \"❌\"}\n",
    "    display(HTML(f\"<div style='color:white; background:{colors[type]}; padding:5px; margin:2px 0; border-radius:3px'>{icons[type]} {msg}</div>\"))\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"Install all packages using uv when possible, with minimal messaging\"\"\"\n",
    "    start = time.time()\n",
    "    show(\"Starting installation...\", \"info\")\n",
    "    \n",
    "    # Try to use uv for faster installation\n",
    "    try:\n",
    "        subprocess.run(\"pip install -q uv\", shell=True, check=True, timeout=30)\n",
    "        installer = \"uv pip\"\n",
    "    except:\n",
    "        installer = \"pip\"\n",
    "    \n",
    "    # Install each category\n",
    "    success_count = 0\n",
    "    total_categories = len(ALL_PACKAGES)\n",
    "    \n",
    "    for category, packages in ALL_PACKAGES.items():\n",
    "        try:\n",
    "            # Install entire category at once for speed\n",
    "            cmd = f\"{installer} install -q {packages}\"\n",
    "            result = subprocess.run(cmd, shell=True, capture_output=True, timeout=120)\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                success_count += 1\n",
    "        except Exception:\n",
    "            pass  # Silent failure, will be reflected in final success rate\n",
    "    \n",
    "    # Simple verification of core packages\n",
    "    try:\n",
    "        import langchain\n",
    "        import pydantic\n",
    "        import gradio\n",
    "        verification = \"with verification\"\n",
    "    except ImportError:\n",
    "        verification = \"with partial verification failures\"\n",
    "    \n",
    "    # Single completion message with success rate\n",
    "    elapsed = time.time() - start\n",
    "    success_rate = int((success_count / total_categories) * 100)\n",
    "    show(f\"Installation completed in {elapsed:.1f}s ({success_rate}% success) {verification}\", \n",
    "         \"success\" if success_rate > 80 else \"warning\")\n",
    "    \n",
    "    return success_rate > 80\n",
    "\n",
    "# Run installation\n",
    "install_packages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Initialization\n",
    "\n",
    "<small>This section configures the LLM provider, API keys, and core components needed for this expert system. The implementation follows a modular architecture that supports multiple AI providers and environments.</small>\n",
    "\n",
    "## Purpose\n",
    "\n",
    "<small>The initialization process:\n",
    "1. **Sets up environment variables** including API keys\n",
    "2. **Configures the LLM provider** with appropriate models and settings\n",
    "3. **Initializes specialized capabilities** when needed (e.g., vision, embedding)\n",
    "4. **Validates the environment** to ensure all requirements are met\n",
    "</small>\n",
    "\n",
    "## Configuration Options\n",
    "\n",
    "<small>\n",
    "You can customize the initialization by adjusting these parameters:\n",
    "\n",
    "| Parameter | Description | Default |\n",
    "|-----------|-------------|---------|\n",
    "| **Provider** | AI service to use (OpenAI, Google, etc.) | OpenAI |\n",
    "| **Model** | Specific model name | Depends on provider |\n",
    "| **Temperature** | Creativity level (0.0-1.0) | 0.7 |\n",
    "| **Features** | Additional capabilities to enable | None |\n",
    "\n",
    "**💡 Tip**: For reproducible results, use lower temperature values (0.0-0.3).\n",
    "</small>\n",
    "\n",
    "## Provider Support\n",
    "\n",
    "<small>\n",
    "This notebook supports these LLM providers:\n",
    "\n",
    "- **OpenAI**: GPT-4, GPT-3.5-Turbo\n",
    "- **Google**: Gemini Pro, PaLM\n",
    "- **Anthropic**: Claude (optional)\n",
    "- **Local**: Ollama with various models (optional)\n",
    "\n",
    "**Note**: Different providers may have varying capabilities and pricing structures.\n",
    "</small>\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "<small>\n",
    "**For Google Colab:**\n",
    "1. Store your API keys in Colab Secrets\n",
    "2. Select your provider from the dropdown\n",
    "3. Run the initialization cell\n",
    "\n",
    "**For Local Environment:**\n",
    "1. Create a `.env` file with your API keys\n",
    "2. Select your provider\n",
    "3. Run the initialization cell\n",
    "\n",
    "**API Key Variables:**\n",
    "- OpenAI: `OPENAI_API_KEY`\n",
    "- Google: `GOOGLE_API_KEY`\n",
    "- Anthropic: `ANTHROPIC_API_KEY`\n",
    "</small>\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "<small>\n",
    "Common issues:\n",
    "- **Authentication errors**: Check your API key is correctly set\n",
    "- **Model unavailability**: Ensure you have access to the specified model\n",
    "- **Import errors**: Run the installation cell first\n",
    "- **Memory issues**: Select a smaller model or reduce context length\n",
    "\n",
    "The initialization cell includes diagnostics that will help identify any configuration problems.\n",
    "</small>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 LLM Setup\n",
    "import os, sys\n",
    "from IPython.display import Markdown, display\n",
    "from typing import Dict, Any, Tuple, Optional\n",
    "\n",
    "# Colab form fields for configuration\n",
    "# @title LLM Configuration\n",
    "api_key = \"\" # @param {type:\"string\"}\n",
    "model = \"gpt-4o\" # @param [\"gpt-4o\", \"gpt-4-turbo\", \"gpt-4\", \"gpt-3.5-turbo\"]\n",
    "embedding_model = \"text-embedding-3-small\" # @param [\"text-embedding-3-small\", \"text-embedding-3-large\", \"text-embedding-ada-002\"]\n",
    "temperature = 0.7 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
    "debug = False # @param {type:\"boolean\"}\n",
    "\n",
    "# Environment detection\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "def show(msg, type=\"info\"):\n",
    "    \"\"\"Display styled message\"\"\"\n",
    "    if type == \"debug\" and not debug:\n",
    "        return\n",
    "    colors = {\"success\": \"#00C853\", \"info\": \"#2196F3\", \"warning\": \"#FF9800\", \"error\": \"#F44336\", \"debug\": \"#9C27B0\"}\n",
    "    icons = {\"success\": \"✅\", \"info\": \"ℹ️\", \"warning\": \"⚠️\", \"error\": \"❌\", \"debug\": \"🔍\"}\n",
    "    display(Markdown(f\"<div style='padding:8px;border-radius:4px;background:{colors[type]};color:white'>{icons[type]} {msg}</div>\"))\n",
    "\n",
    "def get_api_key() -> Optional[str]:\n",
    "    \"\"\"Get API key from various possible sources\"\"\"\n",
    "    # Check form input first\n",
    "    key = api_key\n",
    "    \n",
    "    # Try Colab secret if empty and in Colab\n",
    "    if not key and IN_COLAB:\n",
    "        try:\n",
    "            from google.colab import userdata\n",
    "            key = userdata.get('openai_api_key')\n",
    "            if key:\n",
    "                show(\"API key loaded from Colab secret\", \"success\")\n",
    "        except Exception as e:\n",
    "            show(f\"Error accessing Colab secrets: {e}\", \"debug\")\n",
    "    \n",
    "    # Try environment variable\n",
    "    if not key:\n",
    "        key = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
    "        if key:\n",
    "            show(\"API key loaded from environment variable\", \"debug\")\n",
    "    \n",
    "    # Try .env file\n",
    "    if not key:\n",
    "        try:\n",
    "            from dotenv import load_dotenv\n",
    "            load_dotenv()\n",
    "            key = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
    "            if key:\n",
    "                show(\"API key loaded from .env file\", \"debug\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Final check and request if needed\n",
    "    if not key:\n",
    "        if IN_COLAB:\n",
    "            show(\"\"\"\n",
    "            No API key found. Either:\n",
    "            1. Add it in the form field above\n",
    "            2. Set a Colab secret named 'openai_api_key'\n",
    "            \"\"\", \"warning\")\n",
    "        else:\n",
    "            show(\"No API key found. Add it in the form field or set OPENAI_API_KEY environment variable\", \"warning\")\n",
    "        return None\n",
    "        \n",
    "    return key\n",
    "\n",
    "# === PROVIDER-SPECIFIC: OPENAI ===\n",
    "def initialize_models(api_key: str) -> Tuple[Optional[Any], Optional[Any]]:\n",
    "    \"\"\"Initialize OpenAI models with the provided API key\"\"\"\n",
    "    from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "    \n",
    "    # Set environment variable for consistency\n",
    "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "    \n",
    "    try:\n",
    "        llm = ChatOpenAI(\n",
    "            model_name=model,\n",
    "            temperature=temperature,\n",
    "            openai_api_key=api_key\n",
    "        )\n",
    "        \n",
    "        embeddings = OpenAIEmbeddings(\n",
    "            model=embedding_model,\n",
    "            openai_api_key=api_key\n",
    "        )\n",
    "        \n",
    "        show(f\"OpenAI initialized with {model} and {embedding_model}\", \"success\")\n",
    "        return llm, embeddings\n",
    "        \n",
    "    except Exception as e:\n",
    "        show(f\"Error initializing OpenAI: {e}\", \"error\")\n",
    "        return None, None\n",
    "# === END PROVIDER-SPECIFIC ===\n",
    "\n",
    "def initialize_llm() -> Tuple[Optional[Any], Optional[Any]]:\n",
    "    \"\"\"Main function to set up and initialize LLM\"\"\"\n",
    "    show(\"Initializing LLM...\", \"info\")\n",
    "    \n",
    "    # Get API key\n",
    "    key = get_api_key()\n",
    "    if not key:\n",
    "        return None, None\n",
    "    \n",
    "    # Initialize models\n",
    "    llm, embeddings = initialize_models(key)\n",
    "    \n",
    "    if llm and embeddings:\n",
    "        show(\"Initialization complete! LLM and embeddings ready to use.\", \"success\")\n",
    "    \n",
    "    return llm, embeddings\n",
    "\n",
    "# Run initialization\n",
    "llm, embeddings = initialize_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🛠️ Core Utilities\n",
    "\"\"\"\n",
    "Core utilities for Aavishkar.ai expert systems.\n",
    "Includes logging, caching, error handling, JSON parsing, and state management.\n",
    "\"\"\"\n",
    "\n",
    "import os, json, time, hashlib, functools, uuid\n",
    "from typing import Dict, Any, Optional, Callable, Union, List, Type\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# === GLOBAL SETTINGS ===\n",
    "DEBUG_MODE = False\n",
    "CACHE_ENABLED = True\n",
    "CACHE_DIR = \"./cache\"\n",
    "STATES_DIR = \"./states\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "os.makedirs(STATES_DIR, exist_ok=True)\n",
    "\n",
    "# === DISPLAY & ERROR HANDLING ===\n",
    "def show(msg: str, level: str = \"info\") -> None:\n",
    "    \"\"\"Display formatted message with appropriate styling.\n",
    "    \n",
    "    Args:\n",
    "        msg: Message to display\n",
    "        level: Message level (success, info, warning, error, debug)\n",
    "    \"\"\"\n",
    "    colors = {\"success\": \"#00C853\", \"info\": \"#2196F3\", \"warning\": \"#FF9800\", \"error\": \"#F44336\", \"debug\": \"#9C27B0\"}\n",
    "    icons = {\"success\": \"✅\", \"info\": \"ℹ️\", \"warning\": \"⚠️\", \"error\": \"❌\", \"debug\": \"🔍\"}\n",
    "    \n",
    "    if level == \"debug\" and not DEBUG_MODE:\n",
    "        return\n",
    "        \n",
    "    color = colors.get(level, colors[\"info\"])\n",
    "    icon = icons.get(level, icons[\"info\"])\n",
    "    display(Markdown(f\"<div style='padding:6px;border-radius:4px;background:{color};color:white'>{icon} {msg}</div>\"))\n",
    "\n",
    "def retry(max_attempts: int = 3, delay: float = 1.0) -> Callable:\n",
    "    \"\"\"Decorator for retrying functions with exponential backoff.\"\"\"\n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            for attempt in range(1, max_attempts + 1):\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    if attempt == max_attempts:\n",
    "                        raise\n",
    "                    wait = delay * (2 ** (attempt - 1))\n",
    "                    show(f\"Attempt {attempt} failed: {str(e)}. Retrying in {wait:.1f}s...\", \"warning\")\n",
    "                    time.sleep(wait)\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# === CACHE SYSTEM ===\n",
    "def cache_key(**kwargs) -> str:\n",
    "    \"\"\"Generate a cache key from input parameters.\"\"\"\n",
    "    serialized = json.dumps({k: v for k, v in kwargs.items() if v is not None}, sort_keys=True)\n",
    "    return hashlib.md5(serialized.encode()).hexdigest()\n",
    "\n",
    "def get_cache(key: str) -> Optional[Any]:\n",
    "    \"\"\"Get item from cache if available and not expired.\"\"\"\n",
    "    if not CACHE_ENABLED:\n",
    "        return None\n",
    "        \n",
    "    path = os.path.join(CACHE_DIR, f\"{key}.json\")\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        # Check if expired (default: 1 day)\n",
    "        if time.time() - data.get(\"timestamp\", 0) > 86400:\n",
    "            return None\n",
    "            \n",
    "        return data.get(\"value\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def set_cache(key, value):\n",
    "    \"\"\"Store value in cache with current timestamp.\"\"\"\n",
    "    if not CACHE_ENABLED:\n",
    "        return\n",
    "        \n",
    "    path = os.path.join(CACHE_DIR, f\"{key}.json\")\n",
    "    try:\n",
    "        # Handle Pydantic models by converting to dictionaries\n",
    "        def serialize_pydantic(obj):\n",
    "            if hasattr(obj, 'model_dump'):  # Pydantic v2 models use model_dump\n",
    "                return obj.model_dump()\n",
    "            elif hasattr(obj, 'dict'):      # Older Pydantic models use dict()\n",
    "                return obj.dict()\n",
    "            elif isinstance(obj, list):\n",
    "                return [serialize_pydantic(item) for item in obj]\n",
    "            elif isinstance(obj, dict):\n",
    "                return {k: serialize_pydantic(v) for k, v in obj.items()}\n",
    "            return obj\n",
    "            \n",
    "        serialized_value = serialize_pydantic(value)\n",
    "        \n",
    "        with open(path, 'w') as f:\n",
    "            json.dump({\"timestamp\": time.time(), \"value\": serialized_value}, f)\n",
    "            \n",
    "    except Exception as e:\n",
    "        show(f\"Cache write error: {str(e)}\", \"debug\")\n",
    "\n",
    "def clear_cache(older_than: Optional[int] = None) -> int:\n",
    "    \"\"\"Clear cache entries, optionally only those older than specified seconds.\n",
    "    \n",
    "    Returns:\n",
    "        Number of entries cleared\n",
    "    \"\"\"\n",
    "    if not os.path.exists(CACHE_DIR):\n",
    "        return 0\n",
    "        \n",
    "    count = 0\n",
    "    for filename in os.listdir(CACHE_DIR):\n",
    "        if not filename.endswith('.json'):\n",
    "            continue\n",
    "            \n",
    "        path = os.path.join(CACHE_DIR, filename)\n",
    "        \n",
    "        if older_than:\n",
    "            try:\n",
    "                with open(path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                if time.time() - data.get(\"timestamp\", 0) <= older_than:\n",
    "                    continue\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        try:\n",
    "            os.remove(path)\n",
    "            count += 1\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    return count\n",
    "\n",
    "# === LLM & JSON HELPERS ===\n",
    "def call_llm_with_cache(llm, prompt: str, **kwargs) -> Any:\n",
    "    \"\"\"Call LLM with caching to avoid redundant API calls.\"\"\"\n",
    "    if CACHE_ENABLED:\n",
    "        key = cache_key(prompt=prompt, **kwargs)\n",
    "        cached = get_cache(key)\n",
    "        if cached:\n",
    "            show(\"Using cached response\", \"debug\")\n",
    "            return cached\n",
    "    \n",
    "    response = llm.invoke(prompt, **kwargs)\n",
    "    \n",
    "    if CACHE_ENABLED:\n",
    "        set_cache(key, response)\n",
    "    \n",
    "    return response\n",
    "\n",
    "def parse_json_safely(text: str, default: Any = None) -> Any:\n",
    "    \"\"\"Extract and parse JSON from text with multiple fallback strategies.\"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Try direct parsing first\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Try to extract JSON blocks\n",
    "    try:\n",
    "        # Try code blocks with JSON\n",
    "        if \"```json\" in text:\n",
    "            json_block = text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "            return json.loads(json_block)\n",
    "            \n",
    "        # Try any code blocks\n",
    "        if \"```\" in text:\n",
    "            code_block = text.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "            if code_block.strip().startswith((\"{\", \"[\")):\n",
    "                return json.loads(code_block)\n",
    "        \n",
    "        # Try regex patterns for JSON objects/arrays\n",
    "        patterns = [\n",
    "            r'\\{[\\s\\S]*?\\}',  # JSON objects\n",
    "            r'\\[[\\s\\S]*?\\]'   # JSON arrays\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            matches = re.findall(pattern, text)\n",
    "            for match in matches:\n",
    "                try:\n",
    "                    return json.loads(match)\n",
    "                except:\n",
    "                    continue\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return default\n",
    "\n",
    "# === STATE MANAGEMENT SYSTEM ===\n",
    "class ExpertSystemState:\n",
    "    \"\"\"State container for expert system notebooks.\n",
    "    \n",
    "    This class manages the entire system state, providing methods for\n",
    "    initialization, updates, persistence, and interaction with the UI.\n",
    "    \n",
    "    Attributes:\n",
    "        session_id: Unique identifier for this state session\n",
    "        config: Configuration parameters\n",
    "        data: Main data store\n",
    "        history: Operation history for tracking changes\n",
    "        status: Current status information\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Optional[Dict[str, Any]] = None):\n",
    "        \"\"\"Initialize a new state container.\n",
    "        \n",
    "        Args:\n",
    "            config: Optional configuration parameters\n",
    "        \"\"\"\n",
    "        self.session_id = f\"session_{int(time.time())}\"\n",
    "        self.config = config or {}\n",
    "        self.data = {}  # Main data store\n",
    "        self.history = []  # Operation history\n",
    "        self.status = {\"initialized\": True, \"last_updated\": time.time()}\n",
    "        \n",
    "    def update(self, key: str, value: Any, track_history: bool = True) -> Any:\n",
    "        \"\"\"Update a state element with change tracking.\n",
    "        \n",
    "        Args:\n",
    "            key: Data key to update\n",
    "            value: New value to store\n",
    "            track_history: Whether to record this change in history\n",
    "            \n",
    "        Returns:\n",
    "            The stored value\n",
    "        \"\"\"\n",
    "        old_value = self.data.get(key)\n",
    "        self.data[key] = value\n",
    "        \n",
    "        if track_history:\n",
    "            self.history.append({\n",
    "                \"timestamp\": time.time(),\n",
    "                \"operation\": \"update\",\n",
    "                \"key\": key,\n",
    "                \"old_value_type\": type(old_value).__name__,\n",
    "                \"new_value_type\": type(value).__name__\n",
    "            })\n",
    "        \n",
    "        self.status[\"last_updated\"] = time.time()\n",
    "        return value\n",
    "        \n",
    "    def get(self, key: str, default: Any = None) -> Any:\n",
    "        \"\"\"Retrieve a state element.\n",
    "        \n",
    "        Args:\n",
    "            key: Data key to retrieve\n",
    "            default: Default value if key doesn't exist\n",
    "            \n",
    "        Returns:\n",
    "            The stored value or default\n",
    "        \"\"\"\n",
    "        return self.data.get(key, default)\n",
    "        \n",
    "    def save(self, path: Optional[str] = None) -> str:\n",
    "        \"\"\"Save state to disk.\n",
    "        \n",
    "        Args:\n",
    "            path: Optional file path, defaults to a timestamped file\n",
    "            \n",
    "        Returns:\n",
    "            Path where state was saved\n",
    "        \"\"\"\n",
    "        path = path or os.path.join(STATES_DIR, f\"{self.session_id}.json\")\n",
    "        \n",
    "        # Prepare serializable data\n",
    "        export_data = {\n",
    "            \"session_id\": self.session_id,\n",
    "            \"timestamp\": time.time(),\n",
    "            \"config\": self.config,\n",
    "            \"data\": {},\n",
    "            \"status\": self.status\n",
    "        }\n",
    "        \n",
    "        # Serialize data using our existing helper\n",
    "        for key, value in self.data.items():\n",
    "            if key not in [\"history\"]:  # Skip history to keep file size manageable\n",
    "                try:\n",
    "                    export_data[\"data\"][key] = serialize_pydantic(value)\n",
    "                except Exception as e:\n",
    "                    show(f\"Error serializing {key}: {str(e)}\", \"warning\")\n",
    "        \n",
    "        # Save to file\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "            with open(path, \"w\") as f:\n",
    "                json.dump(export_data, f, indent=2)\n",
    "            show(f\"State saved to {path}\", \"success\")\n",
    "        except Exception as e:\n",
    "            show(f\"Error saving state: {str(e)}\", \"error\")\n",
    "            \n",
    "        return path\n",
    "        \n",
    "    @classmethod\n",
    "    def load(cls, path: str) -> 'ExpertSystemState':\n",
    "        \"\"\"Load state from disk.\n",
    "        \n",
    "        Args:\n",
    "            path: File path to load from\n",
    "            \n",
    "        Returns:\n",
    "            Reconstructed state object\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(path, \"r\") as f:\n",
    "                import_data = json.load(f)\n",
    "            \n",
    "            # Create new state\n",
    "            state = cls(import_data.get(\"config\"))\n",
    "            state.session_id = import_data.get(\"session_id\", state.session_id)\n",
    "            state.status = import_data.get(\"status\", state.status)\n",
    "            \n",
    "            # Load data\n",
    "            for key, value in import_data.get(\"data\", {}).items():\n",
    "                state.data[key] = value\n",
    "                \n",
    "            show(f\"State loaded from {path}\", \"success\")\n",
    "            return state\n",
    "            \n",
    "        except Exception as e:\n",
    "            show(f\"Error loading state: {str(e)}\", \"error\")\n",
    "            return cls()  # Return a new empty state on error\n",
    "        \n",
    "    def to_ui_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert state to UI-friendly dictionary.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary representation for UI\n",
    "        \"\"\"\n",
    "        result = {\n",
    "            \"session_id\": self.session_id,\n",
    "            \"status\": self.status,\n",
    "            \"last_updated\": self.status.get(\"last_updated\")\n",
    "        }\n",
    "        \n",
    "        # Add serializable data for UI\n",
    "        for key, value in self.data.items():\n",
    "            try:\n",
    "                result[key] = serialize_pydantic(value)\n",
    "            except:\n",
    "                # Skip values that can't be serialized for UI\n",
    "                pass\n",
    "                \n",
    "        return result\n",
    "        \n",
    "    @classmethod\n",
    "    def from_ui_dict(cls, ui_dict: Dict[str, Any]) -> 'ExpertSystemState':\n",
    "        \"\"\"Reconstruct state from UI dictionary.\n",
    "        \n",
    "        Args:\n",
    "            ui_dict: Dictionary from UI\n",
    "            \n",
    "        Returns:\n",
    "            Reconstructed state object\n",
    "        \"\"\"\n",
    "        state = cls()\n",
    "        state.session_id = ui_dict.get(\"session_id\", state.session_id)\n",
    "        state.status = ui_dict.get(\"status\", state.status)\n",
    "        \n",
    "        # Copy data (excluding metadata fields)\n",
    "        for key, value in ui_dict.items():\n",
    "            if key not in [\"session_id\", \"status\", \"last_updated\"]:\n",
    "                state.data[key] = value\n",
    "                \n",
    "        return state\n",
    "\n",
    "def serialize_pydantic(obj: Any) -> Any:\n",
    "    \"\"\"Serialize Pydantic models and other complex objects to JSON-compatible format.\n",
    "    \n",
    "    Args:\n",
    "        obj: Object to serialize\n",
    "        \n",
    "    Returns:\n",
    "        JSON-serializable representation\n",
    "    \"\"\"\n",
    "    if hasattr(obj, 'model_dump'):  # Pydantic v2 models\n",
    "        return obj.model_dump()\n",
    "    elif hasattr(obj, 'dict'):      # Older Pydantic models\n",
    "        return obj.dict()\n",
    "    elif isinstance(obj, list):\n",
    "        return [serialize_pydantic(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: serialize_pydantic(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, (str, int, float, bool, type(None))):\n",
    "        return obj\n",
    "    else:\n",
    "        # Try to convert to dict if possible\n",
    "        try:\n",
    "            return dict(obj)\n",
    "        except:\n",
    "            # Fall back to string representation\n",
    "            return str(obj)\n",
    "\n",
    "def deserialize_pydantic(data: Any, model_class: Optional[Type] = None) -> Any:\n",
    "    \"\"\"Deserialize data into Pydantic models if model_class is provided.\n",
    "    \n",
    "    Args:\n",
    "        data: Data to deserialize\n",
    "        model_class: Optional Pydantic model class\n",
    "        \n",
    "    Returns:\n",
    "        Deserialized object\n",
    "    \"\"\"\n",
    "    if model_class is not None:\n",
    "        if isinstance(data, list):\n",
    "            return [model_class(**item) for item in data]\n",
    "        elif isinstance(data, dict):\n",
    "            return model_class(**data)\n",
    "        else:\n",
    "            return data\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Initialization\n",
    "show(\"Core utilities and state management initialized\", \"success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 Literature Synthesis: Multi-Document Analysis\n",
    "\n",
    "## Purpose\n",
    "Extracts, connects, and synthesizes knowledge across multiple scientific papers to identify patterns, agreements, contradictions, and research opportunities across an entire scientific literature corpus.\n",
    "\n",
    "## Core Functions\n",
    "\n",
    "* **Multi-Document Information Extraction**\n",
    "  * Identifies domain-specific elements (scientific claims, methodologies, contributions, research directions)\n",
    "  * Tracks source provenance for all extracted information\n",
    "  * Preserves context and maintains cross-document connections\n",
    "\n",
    "* **Cross-Document Knowledge Integration**\n",
    "  * Identifies common concepts and findings across papers\n",
    "  * Maps relationships between elements from different documents\n",
    "  * Detects supporting, contradicting, and extending relationships\n",
    "\n",
    "* **Hierarchical Research Synthesis**\n",
    "  * Generates category-specific syntheses (claims, methods, contributions, directions)\n",
    "  * Creates comprehensive overview across all documents\n",
    "  * Highlights consensus patterns and disagreements in the literature\n",
    "\n",
    "* **Interactive Visualization**\n",
    "  * Color-codes elements by document source\n",
    "  * Visualizes cross-document relationships\n",
    "  * Supports filtering by document, element type, and importance\n",
    "\n",
    "* **Progressive Processing**\n",
    "  * Processes documents incrementally with clear progress tracking\n",
    "  * Allows adding new documents to existing analysis\n",
    "  * Preserves session state with save/load capabilities\n",
    "\n",
    "## Input\n",
    "\n",
    "* Multiple PDF uploads, plain text, DOIs, or ArXiv IDs\n",
    "* Supports processing of document collections of varying sizes\n",
    "* Best results with complete papers containing clear section structure\n",
    "\n",
    "## Usage\n",
    "\n",
    "1. Run setup cells (installation and initialization)\n",
    "2. Upload multiple scientific papers to analyze\n",
    "3. Monitor processing progress across documents\n",
    "4. Explore extracted elements by category or document source\n",
    "5. Review cross-document relationships and patterns\n",
    "6. Examine the hierarchical synthesis of the entire document collection\n",
    "\n",
    "**Note**: This multi-document analyzer builds on the original LitSynth system, adding cross-document analysis and synthesis capabilities.\n",
    "\n",
    "---\n",
    "\n",
    "*Implementation of the Literature Synthesist cognitive archetype from Aavishkar.ai*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Models\n",
    "\n",
    "## Purpose\n",
    "\n",
    "<small>\n",
    "This section defines the structured data representations that power our Literature Synthesis system. These Pydantic models perform two essential functions:\n",
    "\n",
    "1. **Represent Knowledge**: Define how scientific concepts, relationships, and documents are structured\n",
    "2. **Control System Behavior**: Configure how the system processes and analyzes content\n",
    "</small>\n",
    "\n",
    "## How It Works\n",
    "\n",
    "<small>\n",
    "Our system uses Pydantic models to ensure data validation and clear structure. Think of these models as \"smart containers\" that:\n",
    "\n",
    "- Validate data to prevent errors\n",
    "- Provide helpful error messages when something is wrong\n",
    "- Include documentation for each field\n",
    "- Support extensibility for specialized needs\n",
    "</small>\n",
    "\n",
    "## Core Models\n",
    "\n",
    "<small>\n",
    "Our implementation uses these key models:\n",
    "\n",
    "| Model | Purpose |\n",
    "|-------|---------|\n",
    "| **LitSynthConfig** | Consolidated configuration for all system parameters |\n",
    "| **Concept** | Scientific concepts extracted from literature |\n",
    "| **Relationship** | Connections between scientific concepts |\n",
    "| **ResearchGap** | Identified research gaps and opportunities |\n",
    "| **LiteratureSynthesisOutput** | Complete analysis results container |\n",
    "\n",
    "We've simplified the configuration into a single model (`LitSynthConfig`) to make customization easier. You can adjust parameters by modifying the `config` variable in the code cell.\n",
    "</small>\n",
    "\n",
    "## Customization\n",
    "\n",
    "<small>\n",
    "To customize the system behavior, simply modify the config variable:\n",
    "\n",
    "```python\n",
    "# Example: Increase sensitivity to detect more concepts\n",
    "config.extraction_confidence = 0.6\n",
    "config.max_concepts = 40\n",
    "\n",
    "# Example: Focus only on high-importance concepts\n",
    "config.min_concept_importance = \"high\"\n",
    "```\n",
    "\n",
    "This approach allows you to tune the system's behavior without changing the core models or implementation.\n",
    "</small>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📋 Data Models\n",
    "\"\"\"\n",
    "This section defines the data structures for the LitSynth-Multidoc system.\n",
    "These models support multi-document analysis with source tracking, cross-document\n",
    "relationships, and hierarchical synthesis capabilities.\n",
    "\n",
    "CUSTOMIZATION TIPS:\n",
    "1. Adjust domain-specific fields to match your research area\n",
    "2. Modify importance thresholds based on your analysis priorities\n",
    "3. Extend relationship types for your specific scientific domain\n",
    "4. Customize state persistence options for your environment\n",
    "\"\"\"\n",
    "\n",
    "from pydantic import BaseModel, Field, field_validator, ConfigDict\n",
    "from typing import List, Dict, Optional, Literal, Any, Set, Union, TypeVar, Generic, Type\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import hashlib\n",
    "import time\n",
    "from IPython.display import Markdown, display\n",
    "from pathlib import Path\n",
    "\n",
    "def show(message, type=\"info\"):\n",
    "    \"\"\"Display styled message\"\"\"\n",
    "    colors = {\"success\": \"#00C853\", \"info\": \"#2196F3\", \"warning\": \"#FF9800\", \"error\": \"#F44336\", \"debug\": \"#9C27B0\"}\n",
    "    icons = {\"success\": \"✅\", \"info\": \"ℹ️\", \"warning\": \"⚠️\", \"error\": \"❌\", \"debug\": \"🔍\"}\n",
    "    display(Markdown(f\"<div style='padding:8px;border-radius:4px;background:{colors[type]};color:white'>{icons[type]} {message}</div>\"))\n",
    "\n",
    "# === ENVIRONMENT SETUP ===\n",
    "\n",
    "# Create directories for cache and state\n",
    "BASE_DIR = Path(\"LitSynthMulti_data\")\n",
    "CACHE_DIR = BASE_DIR / \"cache\"\n",
    "STATES_DIR = BASE_DIR / \"states\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "STATES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === INPUT MODELS ===\n",
    "\n",
    "class DocumentSource(BaseModel):\n",
    "    \"\"\"Input document with metadata and processing status.\n",
    "    \n",
    "    Represents a document in the analysis collection with tracking information\n",
    "    for multi-document processing and synthesis.\n",
    "    \n",
    "    Attributes:\n",
    "        source_id: Unique identifier for the document\n",
    "        title: Document title (extracted or provided)\n",
    "        authors: List of authors (when available)\n",
    "        source_type: Type of document source (pdf, text, etc.)\n",
    "        content: Document content or reference path\n",
    "        metadata: Additional document information\n",
    "        processed: Whether the document has been processed\n",
    "    \"\"\"\n",
    "    source_id: str = Field(default_factory=lambda: f\"doc_{uuid.uuid4().hex[:8]}\")\n",
    "    title: Optional[str] = None\n",
    "    authors: List[str] = Field(default_factory=list)\n",
    "    source_type: Literal[\"pdf\", \"text\", \"url\", \"doi\", \"arxiv\"]\n",
    "    content: str\n",
    "    metadata: Dict[str, Any] = Field(default_factory=dict)\n",
    "    processed: bool = False\n",
    "    \n",
    "    @field_validator('source_id')\n",
    "    @classmethod\n",
    "    def validate_source_id(cls, v):\n",
    "        \"\"\"Ensure source_id is properly formatted.\"\"\"\n",
    "        if not v or not isinstance(v, str) or len(v) < 5:\n",
    "            return f\"doc_{uuid.uuid4().hex[:8]}\"\n",
    "        return v\n",
    "    \n",
    "    def hash_content(self) -> str:\n",
    "        \"\"\"Generate a hash of document content for deduplication.\"\"\"\n",
    "        if not self.content:\n",
    "            return \"\"\n",
    "        content_sample = self.content[:min(1000, len(self.content))]\n",
    "        return hashlib.md5(content_sample.encode('utf-8')).hexdigest()[:10]\n",
    "\n",
    "# === KNOWLEDGE REPRESENTATION MODELS ===\n",
    "\n",
    "class ScientificClaim(BaseModel):\n",
    "    \"\"\"Scientific claim extracted from literature.\n",
    "    \n",
    "    Represents a key scientific claim with source tracking for\n",
    "    multi-document analysis and synthesis.\n",
    "    \n",
    "    Attributes:\n",
    "        claim_id: Unique identifier for the claim\n",
    "        claim_text: The textual content of the claim\n",
    "        claim_type: Classification of the claim type\n",
    "        importance: How important this claim is\n",
    "        evidence: Supporting evidence for the claim\n",
    "        source_documents: List of document IDs containing this claim\n",
    "        confidence: Confidence score for extraction\n",
    "    \"\"\"\n",
    "    claim_id: str = Field(default_factory=lambda: f\"claim_{uuid.uuid4().hex[:8]}\")\n",
    "    claim_text: str\n",
    "    claim_type: Literal[\"hypothesis\", \"finding\", \"assertion\"] = \"finding\"\n",
    "    importance: Literal[\"high\", \"medium\", \"low\"] = \"medium\"\n",
    "    evidence: Optional[str] = None\n",
    "    source_documents: List[str] = Field(default_factory=list)\n",
    "    confidence: float = Field(default=0.8, ge=0.0, le=1.0)\n",
    "    \n",
    "    model_config = ConfigDict(\n",
    "        json_schema_extra={\n",
    "            \"example\": {\n",
    "                \"claim_id\": \"claim_a1b2c3d4\",\n",
    "                \"claim_text\": \"Increased temperature accelerates the reaction rate by a factor of 2.5\",\n",
    "                \"claim_type\": \"finding\",\n",
    "                \"importance\": \"high\",\n",
    "                \"evidence\": \"Experimental results in Table 2 show rate constants at different temperatures\",\n",
    "                \"source_documents\": [\"doc_12345678\", \"doc_23456789\"],\n",
    "                \"confidence\": 0.9\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "class Methodology(BaseModel):\n",
    "    \"\"\"Research methodology used in the literature.\n",
    "    \n",
    "    Represents a research methodology with source tracking for\n",
    "    multi-document analysis and synthesis.\n",
    "    \n",
    "    Attributes:\n",
    "        method_id: Unique identifier for the methodology\n",
    "        method_name: Name or title of the methodology\n",
    "        description: Detailed description of the methodology\n",
    "        context: Context in which the methodology was used\n",
    "        limitations: Known limitations of the methodology\n",
    "        source_documents: List of document IDs using this methodology\n",
    "        confidence: Confidence score for extraction\n",
    "    \"\"\"\n",
    "    method_id: str = Field(default_factory=lambda: f\"method_{uuid.uuid4().hex[:8]}\")\n",
    "    method_name: str\n",
    "    description: str\n",
    "    context: Optional[str] = None\n",
    "    limitations: Optional[str] = None\n",
    "    source_documents: List[str] = Field(default_factory=list)\n",
    "    confidence: float = Field(default=0.8, ge=0.0, le=1.0)\n",
    "    \n",
    "    model_config = ConfigDict(\n",
    "        json_schema_extra={\n",
    "            \"example\": {\n",
    "                \"method_id\": \"method_a1b2c3d4\",\n",
    "                \"method_name\": \"CRISPR-Cas9 Gene Editing\",\n",
    "                \"description\": \"A precise gene editing technique using Cas9 nuclease guided by RNA\",\n",
    "                \"context\": \"Used to modify gene expression in mouse embryos\",\n",
    "                \"limitations\": \"Potential off-target effects and delivery challenges\",\n",
    "                \"source_documents\": [\"doc_12345678\", \"doc_87654321\"],\n",
    "                \"confidence\": 0.85\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "class KeyContribution(BaseModel):\n",
    "    \"\"\"Key contribution or finding from the literature.\n",
    "    \n",
    "    Represents a significant research contribution with connections\n",
    "    to claims and methods across documents.\n",
    "    \n",
    "    Attributes:\n",
    "        contribution_id: Unique identifier for the contribution\n",
    "        contribution_text: The textual content of the contribution\n",
    "        contribution_type: Classification of the contribution type\n",
    "        importance: How important this contribution is\n",
    "        related_claims: List of related claim IDs\n",
    "        related_methods: List of related methodology IDs\n",
    "        source_documents: List of document IDs containing this contribution\n",
    "        confidence: Confidence score for extraction\n",
    "    \"\"\"\n",
    "    contribution_id: str = Field(default_factory=lambda: f\"contrib_{uuid.uuid4().hex[:8]}\")\n",
    "    contribution_text: str\n",
    "    contribution_type: Literal[\"theoretical\", \"empirical\", \"methodological\", \"practical\"] = \"empirical\"\n",
    "    importance: Literal[\"high\", \"medium\", \"low\"] = \"medium\"\n",
    "    related_claims: List[str] = Field(default_factory=list)\n",
    "    related_methods: List[str] = Field(default_factory=list)\n",
    "    source_documents: List[str] = Field(default_factory=list)\n",
    "    confidence: float = Field(default=0.8, ge=0.0, le=1.0)\n",
    "\n",
    "class ResearchDirection(BaseModel):\n",
    "    \"\"\"Future research direction identified in the literature.\n",
    "    \n",
    "    Represents a potential future research direction with connections\n",
    "    to claims and contributions across documents.\n",
    "    \n",
    "    Attributes:\n",
    "        direction_id: Unique identifier for the research direction\n",
    "        direction_text: The textual content of the research direction\n",
    "        rationale: Reasoning behind this research direction\n",
    "        related_claims: List of related claim IDs\n",
    "        related_contributions: List of related contribution IDs\n",
    "        source_documents: List of document IDs suggesting this direction\n",
    "        importance: How important this research direction is\n",
    "        confidence: Confidence score for extraction\n",
    "    \"\"\"\n",
    "    direction_id: str = Field(default_factory=lambda: f\"direction_{uuid.uuid4().hex[:8]}\")\n",
    "    direction_text: str\n",
    "    rationale: Optional[str] = None\n",
    "    related_claims: List[str] = Field(default_factory=list)\n",
    "    related_contributions: List[str] = Field(default_factory=list)\n",
    "    source_documents: List[str] = Field(default_factory=list)\n",
    "    importance: Literal[\"high\", \"medium\", \"low\"] = \"medium\"\n",
    "    confidence: float = Field(default=0.7, ge=0.0, le=1.0)\n",
    "\n",
    "# === CROSS-DOCUMENT RELATIONSHIP MODEL ===\n",
    "\n",
    "class CrossDocumentRelationship(BaseModel):\n",
    "    \"\"\"Relationship between elements across documents.\n",
    "    \n",
    "    Represents connections between knowledge elements from different\n",
    "    documents, identifying patterns across the literature.\n",
    "    \n",
    "    Attributes:\n",
    "        relationship_id: Unique identifier for the relationship\n",
    "        source_element_id: ID of the source element\n",
    "        source_element_type: Type of the source element\n",
    "        target_element_id: ID of the target element\n",
    "        target_element_type: Type of the target element\n",
    "        relationship_type: Type of relationship between elements\n",
    "        evidence: Supporting evidence for this relationship\n",
    "        confidence: Confidence score for this relationship\n",
    "    \"\"\"\n",
    "    relationship_id: str = Field(default_factory=lambda: f\"rel_{uuid.uuid4().hex[:8]}\")\n",
    "    source_element_id: str\n",
    "    source_element_type: Literal[\"claim\", \"methodology\", \"contribution\", \"direction\"]\n",
    "    target_element_id: str\n",
    "    target_element_type: Literal[\"claim\", \"methodology\", \"contribution\", \"direction\"]\n",
    "    relationship_type: Literal[\"supports\", \"contradicts\", \"extends\", \"cites\", \"replicates\"] = \"supports\"\n",
    "    evidence: Optional[str] = None\n",
    "    confidence: float = Field(default=0.7, ge=0.0, le=1.0)\n",
    "\n",
    "# === SYNTHESIS MODELS ===\n",
    "\n",
    "class CategorySynthesis(BaseModel):\n",
    "    \"\"\"Synthesized analysis for a specific category across documents.\n",
    "    \n",
    "    Represents the synthesis output for a specific knowledge category,\n",
    "    summarizing patterns across multiple documents.\n",
    "    \n",
    "    Attributes:\n",
    "        category: Category of elements being synthesized\n",
    "        synthesis_text: The synthesized text summary\n",
    "        element_count: Count of elements in this category\n",
    "        document_count: Count of documents included in synthesis\n",
    "        timestamp: When the synthesis was generated\n",
    "    \"\"\"\n",
    "    category: Literal[\"claims\", \"methodologies\", \"contributions\", \"directions\"]\n",
    "    synthesis_text: str\n",
    "    element_count: int\n",
    "    document_count: int\n",
    "    timestamp: datetime = Field(default_factory=datetime.now)\n",
    "\n",
    "# === COMPLETE OUTPUT MODEL ===\n",
    "\n",
    "class MultiDocSynthesisOutput(BaseModel):\n",
    "    \"\"\"Complete output from multi-document literature synthesis.\n",
    "    \n",
    "    Comprehensive container for all results from multi-document\n",
    "    analysis, including source documents, extracted elements,\n",
    "    cross-document relationships, and syntheses.\n",
    "    \n",
    "    Attributes:\n",
    "        analysis_id: Unique identifier for this analysis\n",
    "        documents: List of source documents\n",
    "        claims: List of extracted scientific claims\n",
    "        methodologies: List of extracted methodologies\n",
    "        contributions: List of extracted key contributions\n",
    "        research_directions: List of extracted research directions\n",
    "        cross_document_relationships: List of relationships between elements\n",
    "        category_syntheses: Dictionary of category-specific syntheses\n",
    "        overall_synthesis: Overall synthesis across all documents\n",
    "        timestamp: When the analysis was completed\n",
    "    \"\"\"\n",
    "    analysis_id: str = Field(default_factory=lambda: f\"analysis_{int(time.time())}\")\n",
    "    documents: List[DocumentSource] = Field(default_factory=list)\n",
    "    claims: List[ScientificClaim] = Field(default_factory=list)\n",
    "    methodologies: List[Methodology] = Field(default_factory=list)\n",
    "    contributions: List[KeyContribution] = Field(default_factory=list)\n",
    "    research_directions: List[ResearchDirection] = Field(default_factory=list)\n",
    "    cross_document_relationships: List[CrossDocumentRelationship] = Field(default_factory=list)\n",
    "    category_syntheses: Dict[str, CategorySynthesis] = Field(default_factory=dict)\n",
    "    overall_synthesis: Optional[str] = None\n",
    "    timestamp: datetime = Field(default_factory=datetime.now)\n",
    "    \n",
    "    def get_document_by_id(self, doc_id: str) -> Optional[DocumentSource]:\n",
    "        \"\"\"Helper method to retrieve a document by ID.\"\"\"\n",
    "        for doc in self.documents:\n",
    "            if doc.source_id == doc_id:\n",
    "                return doc\n",
    "        return None\n",
    "        \n",
    "    def get_elements_for_document(self, doc_id: str, category: str) -> List[Any]:\n",
    "        \"\"\"Helper method to retrieve all elements of a category from a specific document.\"\"\"\n",
    "        if category == \"claims\":\n",
    "            return [c for c in self.claims if doc_id in c.source_documents]\n",
    "        elif category == \"methodologies\":\n",
    "            return [m for m in self.methodologies if doc_id in m.source_documents]\n",
    "        elif category == \"contributions\":\n",
    "            return [c for c in self.contributions if doc_id in c.source_documents]\n",
    "        elif category == \"directions\":\n",
    "            return [d for d in self.research_directions if doc_id in d.source_documents]\n",
    "        return []\n",
    "\n",
    "# === CONFIGURATION MODEL ===\n",
    "\n",
    "class LitSynthMultiConfig(BaseModel):\n",
    "    \"\"\"Configuration for the LitSynth-Multidoc system.\n",
    "    \n",
    "    Controls system behavior and processing parameters for\n",
    "    multi-document analysis and synthesis.\n",
    "    \n",
    "    Attributes:\n",
    "        text_chunk_size: Characters per text chunk\n",
    "        text_chunk_overlap: Overlap between chunks\n",
    "        min_importance: Minimum importance level to include\n",
    "        extraction_confidence: Minimum extraction confidence\n",
    "        max_claims: Maximum claims per document\n",
    "        max_methodologies: Maximum methodologies per document\n",
    "        max_contributions: Maximum contributions per document\n",
    "        max_directions: Maximum research directions per document\n",
    "        relationship_confidence: Minimum relationship confidence\n",
    "        max_cross_relationships: Maximum cross-document relationships\n",
    "        similarity_threshold: Threshold for element similarity detection\n",
    "        parallel_processing: Whether to process documents in parallel\n",
    "        scientific_domain: Scientific domain for specialized analysis\n",
    "    \"\"\"\n",
    "    # Text Processing Parameters\n",
    "    text_chunk_size: int = Field(\n",
    "        default=4000, ge=500, le=8000, \n",
    "        description=\"Characters per text chunk\"\n",
    "    )\n",
    "    text_chunk_overlap: int = Field(\n",
    "        default=100, ge=50, le=1000,\n",
    "        description=\"Overlap between chunks\"\n",
    "    )\n",
    "    \n",
    "    # Element Extraction Parameters\n",
    "    min_importance: Literal[\"low\", \"medium\", \"high\"] = Field(\n",
    "        default=\"medium\", \n",
    "        description=\"Minimum importance level to include\"\n",
    "    )\n",
    "    extraction_confidence: float = Field(\n",
    "        default=0.7, ge=0.0, le=1.0,\n",
    "        description=\"Minimum extraction confidence\"\n",
    "    )\n",
    "    \n",
    "    # Category Limits\n",
    "    max_claims: int = Field(\n",
    "        default=20, ge=5, le=100,\n",
    "        description=\"Maximum claims to extract per document\"\n",
    "    )\n",
    "    max_methodologies: int = Field(\n",
    "        default=10, ge=3, le=50,\n",
    "        description=\"Maximum methodologies to extract per document\"\n",
    "    )\n",
    "    max_contributions: int = Field(\n",
    "        default=15, ge=3, le=50,\n",
    "        description=\"Maximum contributions to extract per document\"\n",
    "    )\n",
    "    max_directions: int = Field(\n",
    "        default=10, ge=3, le=50,\n",
    "        description=\"Maximum research directions to extract per document\"\n",
    "    )\n",
    "    \n",
    "    # Cross-Document Parameters\n",
    "    relationship_confidence: float = Field(\n",
    "        default=0.6, ge=0.0, le=1.0,\n",
    "        description=\"Minimum relationship confidence\"\n",
    "    )\n",
    "    max_cross_relationships: int = Field(\n",
    "        default=50, ge=10, le=200,\n",
    "        description=\"Maximum cross-document relationships\"\n",
    "    )\n",
    "    similarity_threshold: float = Field(\n",
    "        default=0.75, ge=0.5, le=0.95,\n",
    "        description=\"Threshold for element similarity detection\"\n",
    "    )\n",
    "    \n",
    "    # Processing Parameters\n",
    "    parallel_processing: bool = Field(\n",
    "        default=False,\n",
    "        description=\"Process documents in parallel when possible\"\n",
    "    )\n",
    "    \n",
    "    # Customization\n",
    "    scientific_domain: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=\"Scientific domain for specialized analysis\"\n",
    "    )\n",
    "    \n",
    "    @field_validator('text_chunk_overlap')\n",
    "    @classmethod\n",
    "    def validate_overlap(cls, v, info):\n",
    "        \"\"\"Ensure overlap is less than chunk size.\"\"\"\n",
    "        if 'text_chunk_size' in info.data and v >= info.data['text_chunk_size']:\n",
    "            raise ValueError(\"text_chunk_overlap must be less than text_chunk_size\")\n",
    "        return v\n",
    "\n",
    "# === STATE MANAGEMENT CLASSES ===\n",
    "\n",
    "# Helper functions for serialization and deserialization\n",
    "def serialize_pydantic(obj):\n",
    "    \"\"\"Convert Pydantic models and other complex objects to JSON-compatible format.\"\"\"\n",
    "    if hasattr(obj, 'model_dump'):\n",
    "        return obj.model_dump()\n",
    "    elif isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [serialize_pydantic(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: serialize_pydantic(v) for k, v in obj.items()}\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def deserialize_pydantic(data, model_class=None):\n",
    "    \"\"\"Reconstruct Pydantic models from data.\"\"\"\n",
    "    if model_class and data and isinstance(data, dict):\n",
    "        return model_class.model_validate(data)\n",
    "    elif isinstance(data, list):\n",
    "        return [deserialize_pydantic(item, None) for item in data]\n",
    "    elif isinstance(data, dict):\n",
    "        return {k: deserialize_pydantic(v, None) for k, v in data.items()}\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "class ExpertSystemState(BaseModel):\n",
    "    \"\"\"Base state management class for expert systems.\n",
    "    \n",
    "    Provides core state management capabilities for expert systems,\n",
    "    including state updates, persistence, and UI integration.\n",
    "    \n",
    "    Attributes:\n",
    "        session_id: Unique identifier for this session\n",
    "        config: Configuration parameters\n",
    "        data: Main data container for the expert system\n",
    "        history: History of state changes for tracking/undo\n",
    "        status: Current status information\n",
    "    \"\"\"\n",
    "    session_id: str = Field(default_factory=lambda: f\"session_{time.strftime('%Y%m%d_%H%M%S')}\")\n",
    "    config: Dict[str, Any] = Field(default_factory=dict)\n",
    "    data: Dict[str, Any] = Field(default_factory=dict)\n",
    "    history: List[Dict[str, Any]] = Field(default_factory=list, exclude=True)\n",
    "    status: Dict[str, Any] = Field(default_factory=lambda: {\"status\": \"initialized\", \"last_updated\": datetime.now()})\n",
    "    \n",
    "    def update(self, key: str, value: Any, track_history: bool = True) -> None:\n",
    "        \"\"\"Update state with new data.\n",
    "        \n",
    "        Args:\n",
    "            key: The state key to update\n",
    "            value: The new value to store\n",
    "            track_history: Whether to record this change in history\n",
    "        \"\"\"\n",
    "        if track_history:\n",
    "            # Save previous state in history\n",
    "            if key in self.data:\n",
    "                self.history.append({\n",
    "                    \"key\": key,\n",
    "                    \"prev_value\": self.data.get(key),\n",
    "                    \"timestamp\": datetime.now()\n",
    "                })\n",
    "                # Limit history size\n",
    "                if len(self.history) > 100:\n",
    "                    self.history = self.history[-100:]\n",
    "        \n",
    "        # Update the state\n",
    "        self.data[key] = value\n",
    "        \n",
    "        # Update status\n",
    "        self.status.update({\n",
    "            \"status\": \"updated\",\n",
    "            \"last_key_updated\": key,\n",
    "            \"last_updated\": datetime.now()\n",
    "        })\n",
    "    \n",
    "    def get(self, key: str, default: Any = None) -> Any:\n",
    "        \"\"\"Get value from state.\n",
    "        \n",
    "        Args:\n",
    "            key: The state key to retrieve\n",
    "            default: Default value if key doesn't exist\n",
    "            \n",
    "        Returns:\n",
    "            The value from state or default\n",
    "        \"\"\"\n",
    "        return self.data.get(key, default)\n",
    "    \n",
    "    def save(self, filepath: Optional[str] = None) -> str:\n",
    "        \"\"\"Save state to file.\n",
    "        \n",
    "        Args:\n",
    "            filepath: Optional custom filepath\n",
    "            \n",
    "        Returns:\n",
    "            Path to the saved file\n",
    "        \"\"\"\n",
    "        if not filepath:\n",
    "            filepath = STATES_DIR / f\"{self.session_id}.json\"\n",
    "        \n",
    "        # Prepare state for serialization\n",
    "        state_dict = serialize_pydantic(self.model_dump(exclude={\"history\"}))\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(state_dict, f, indent=2)\n",
    "        \n",
    "        self.status.update({\n",
    "            \"status\": \"saved\",\n",
    "            \"save_location\": str(filepath),\n",
    "            \"last_updated\": datetime.now()\n",
    "        })\n",
    "        \n",
    "        return str(filepath)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filepath: str) -> 'ExpertSystemState':\n",
    "        \"\"\"Load state from file.\n",
    "        \n",
    "        Args:\n",
    "            filepath: Path to the saved state file\n",
    "            \n",
    "        Returns:\n",
    "            Loaded state instance\n",
    "        \"\"\"\n",
    "        with open(filepath, 'r') as f:\n",
    "            state_dict = json.load(f)\n",
    "        \n",
    "        # Create new instance from saved data\n",
    "        instance = cls.model_validate(state_dict)\n",
    "        \n",
    "        instance.status.update({\n",
    "            \"status\": \"loaded\",\n",
    "            \"load_source\": filepath,\n",
    "            \"last_updated\": datetime.now()\n",
    "        })\n",
    "        \n",
    "        return instance\n",
    "    \n",
    "    def to_ui_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert state to UI-friendly dictionary.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary representation for UI components\n",
    "        \"\"\"\n",
    "        # Create UI-friendly representation for display\n",
    "        return {\n",
    "            \"session_id\": self.session_id,\n",
    "            \"status\": self.status.get(\"status\", \"unknown\"),\n",
    "            \"last_updated\": self.status.get(\"last_updated\", datetime.now()).isoformat(),\n",
    "            \"data_keys\": list(self.data.keys()),\n",
    "            \"config_summary\": self.config\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_ui_dict(cls, ui_dict: Dict[str, Any]) -> 'ExpertSystemState':\n",
    "        \"\"\"Create state from UI dictionary.\n",
    "        \n",
    "        Args:\n",
    "            ui_dict: Dictionary from UI components\n",
    "            \n",
    "        Returns:\n",
    "            New state instance\n",
    "        \"\"\"\n",
    "        # This is a simplified implementation\n",
    "        # In practice, you would map UI values to proper state structure\n",
    "        return cls(\n",
    "            session_id=ui_dict.get(\"session_id\", f\"session_{time.strftime('%Y%m%d_%H%M%S')}\"),\n",
    "            config=ui_dict.get(\"config\", {}),\n",
    "            data=ui_dict.get(\"data\", {}),\n",
    "            status={\n",
    "                \"status\": \"created_from_ui\",\n",
    "                \"last_updated\": datetime.now()\n",
    "            }\n",
    "        )\n",
    "\n",
    "class LitSynthMultiState(ExpertSystemState):\n",
    "    \"\"\"State management for LitSynth-Multidoc system.\n",
    "    \n",
    "    Extends the base expert system state with document-specific\n",
    "    state management for multi-document analysis workflows.\n",
    "    \n",
    "    This class provides methods for tracking document processing,\n",
    "    managing extraction results, and handling synthesis generation.\n",
    "    \"\"\"\n",
    "    def __init__(self, **data):\n",
    "        \"\"\"Initialize with default LitSynth-specific state.\"\"\"\n",
    "        super().__init__(**data)\n",
    "        \n",
    "        # Initialize default configuration if not provided\n",
    "        if not self.config:\n",
    "            self.config = LitSynthMultiConfig().model_dump()\n",
    "        \n",
    "        # Initialize data containers if not present\n",
    "        if 'documents' not in self.data:\n",
    "            self.data['documents'] = []\n",
    "        \n",
    "        if 'claims' not in self.data:\n",
    "            self.data['claims'] = []\n",
    "            \n",
    "        if 'methodologies' not in self.data:\n",
    "            self.data['methodologies'] = []\n",
    "            \n",
    "        if 'contributions' not in self.data:\n",
    "            self.data['contributions'] = []\n",
    "            \n",
    "        if 'research_directions' not in self.data:\n",
    "            self.data['research_directions'] = []\n",
    "            \n",
    "        if 'cross_document_relationships' not in self.data:\n",
    "            self.data['cross_document_relationships'] = []\n",
    "            \n",
    "        if 'category_syntheses' not in self.data:\n",
    "            self.data['category_syntheses'] = {}\n",
    "            \n",
    "        if 'overall_synthesis' not in self.data:\n",
    "            self.data['overall_synthesis'] = None\n",
    "    \n",
    "    def add_document(self, document: DocumentSource) -> bool:\n",
    "        \"\"\"Add a document to the collection.\n",
    "        \n",
    "        Args:\n",
    "            document: The document to add\n",
    "            \n",
    "        Returns:\n",
    "            True if document was added, False if duplicate\n",
    "        \"\"\"\n",
    "        # Check for duplicates\n",
    "        doc_hash = document.hash_content()\n",
    "        for existing_doc in self.data['documents']:\n",
    "            if existing_doc.hash_content() == doc_hash:\n",
    "                show(f\"Document '{document.title or 'Untitled'}' appears to be a duplicate\", \"warning\")\n",
    "                return False\n",
    "        \n",
    "        # Add the document\n",
    "        documents = self.data['documents']\n",
    "        documents.append(document)\n",
    "        self.update('documents', documents)\n",
    "        \n",
    "        show(f\"Added document: {document.title or 'Untitled'}\", \"success\")\n",
    "        return True\n",
    "    \n",
    "    def update_extraction_results(self, \n",
    "                                 document_id: str, \n",
    "                                 category: str, \n",
    "                                 results: List[Any]) -> None:\n",
    "        \"\"\"Update extraction results for a document and category.\n",
    "        \n",
    "        Args:\n",
    "            document_id: ID of the document\n",
    "            category: Category of extracted elements\n",
    "            results: List of extraction results\n",
    "        \"\"\"\n",
    "        if category not in ['claims', 'methodologies', 'contributions', 'research_directions']:\n",
    "            show(f\"Invalid category: {category}\", \"error\")\n",
    "            return\n",
    "        \n",
    "        # Get current elements\n",
    "        current = self.data[category]\n",
    "        \n",
    "        # Remove existing elements for this document\n",
    "        filtered = [item for item in current if document_id not in item.source_documents]\n",
    "        \n",
    "        # Add new results\n",
    "        combined = filtered + results\n",
    "        \n",
    "        # Update state\n",
    "        self.update(category, combined)\n",
    "        \n",
    "        show(f\"Updated {len(results)} {category} for document {document_id}\", \"success\")\n",
    "    \n",
    "    def mark_document_processed(self, document_id: str) -> None:\n",
    "        \"\"\"Mark a document as processed.\n",
    "        \n",
    "        Args:\n",
    "            document_id: ID of the document to mark\n",
    "        \"\"\"\n",
    "        documents = self.data['documents']\n",
    "        for i, doc in enumerate(documents):\n",
    "            if doc.source_id == document_id:\n",
    "                doc.processed = True\n",
    "                documents[i] = doc\n",
    "                break\n",
    "        \n",
    "        self.update('documents', documents)\n",
    "    \n",
    "    def regenerate_cross_document_analysis(self) -> None:\n",
    "        \"\"\"Regenerate cross-document relationships and syntheses.\"\"\"\n",
    "        # This would trigger the cross-document analysis chain\n",
    "        # For now we just update the status\n",
    "        self.status.update({\n",
    "            \"status\": \"cross_document_analysis_needed\",\n",
    "            \"last_updated\": datetime.now()\n",
    "        })\n",
    "        \n",
    "        show(\"Cross-document analysis needs to be regenerated\", \"info\")\n",
    "    \n",
    "    def get_processed_document_count(self) -> int:\n",
    "        \"\"\"Get count of processed documents.\n",
    "        \n",
    "        Returns:\n",
    "            Number of processed documents\n",
    "        \"\"\"\n",
    "        return sum(1 for doc in self.data['documents'] if doc.processed)\n",
    "    \n",
    "    def get_progress_percentage(self) -> int:\n",
    "        \"\"\"Calculate overall processing progress.\n",
    "        \n",
    "        Returns:\n",
    "            Progress percentage (0-100)\n",
    "        \"\"\"\n",
    "        docs = self.data['documents']\n",
    "        if not docs:\n",
    "            return 0\n",
    "        \n",
    "        processed = self.get_processed_document_count()\n",
    "        return int((processed / len(docs)) * 100)\n",
    "    \n",
    "    def to_output_model(self) -> MultiDocSynthesisOutput:\n",
    "        \"\"\"Convert state to MultiDocSynthesisOutput model.\n",
    "        \n",
    "        Returns:\n",
    "            Complete analysis results in MultiDocSynthesisOutput format\n",
    "        \"\"\"\n",
    "        # Helper to convert lists of dictionaries to lists of models\n",
    "        def convert_list(items, model_class):\n",
    "            return [\n",
    "                item if isinstance(item, model_class) \n",
    "                else model_class.model_validate(item)\n",
    "                for item in items\n",
    "            ]\n",
    "        \n",
    "        # Convert category syntheses\n",
    "        category_syntheses = {}\n",
    "        for cat, synth in self.data.get('category_syntheses', {}).items():\n",
    "            if isinstance(synth, CategorySynthesis):\n",
    "                category_syntheses[cat] = synth\n",
    "            elif isinstance(synth, dict):\n",
    "                category_syntheses[cat] = CategorySynthesis.model_validate(synth)\n",
    "        \n",
    "        # Create output model\n",
    "        return MultiDocSynthesisOutput(\n",
    "            analysis_id=self.session_id,\n",
    "            documents=convert_list(self.data.get('documents', []), DocumentSource),\n",
    "            claims=convert_list(self.data.get('claims', []), ScientificClaim),\n",
    "            methodologies=convert_list(self.data.get('methodologies', []), Methodology),\n",
    "            contributions=convert_list(self.data.get('contributions', []), KeyContribution),\n",
    "            research_directions=convert_list(self.data.get('research_directions', []), ResearchDirection),\n",
    "            cross_document_relationships=convert_list(\n",
    "                self.data.get('cross_document_relationships', []), \n",
    "                CrossDocumentRelationship\n",
    "            ),\n",
    "            category_syntheses=category_syntheses,\n",
    "            overall_synthesis=self.data.get('overall_synthesis'),\n",
    "            timestamp=datetime.now()\n",
    "        )\n",
    "\n",
    "# === INITIALIZE STATE ===\n",
    "\n",
    "# Create default configuration\n",
    "config = LitSynthMultiConfig()\n",
    "\n",
    "# Create initial state container\n",
    "state = LitSynthMultiState(config=config.model_dump())\n",
    "\n",
    "# Show confirmation\n",
    "show(\"Data models and state management initialized successfully\", \"success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Functions\n",
    "\n",
    "<small>\n",
    "This section contains the heart of our Literature Synthesis system - the functions that analyze documents, extract key information, and generate insights.\n",
    "\n",
    "## What's Included\n",
    "\n",
    "1. **Prompt Library**: The instructions we give to the AI model\n",
    "2. **Function Definitions**: The code that processes documents and manages the analysis\n",
    "\n",
    "## How to Customize\n",
    "\n",
    "You can easily modify the system's behavior by:\n",
    "\n",
    "- **Changing prompts**: Edit the instructions to focus on specific types of information\n",
    "- **Adjusting parameters**: Fine-tune the analysis by modifying the `config` settings\n",
    "\n",
    "No coding knowledge is required - simply edit the text of prompts in the first code cell below.\n",
    "</small>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts Library\n",
    "\"\"\"\n",
    "This section contains all prompts for the LitSynth-Multidoc system.\n",
    "These prompts are optimized for scientific paper analysis across multiple documents,\n",
    "with specific attention to extracting structured knowledge and cross-document synthesis.\n",
    "\n",
    "CUSTOMIZATION TIPS:\n",
    "1. Keep the output format instructions intact to ensure proper parsing\n",
    "2. Add domain-specific terminology or examples for your research area\n",
    "3. Emphasize particular aspects relevant to your scientific domain\n",
    "4. Update section references to match conventions in your field\n",
    "\"\"\"\n",
    "\n",
    "PROMPTS = {\n",
    "    # === CLAIM EXTRACTION ===\n",
    "    # Purpose: Extract scientific claims with provenance tracking\n",
    "    \"claim_extraction\": \"\"\"\n",
    "    You are a scientific claim extraction specialist. Extract key scientific claims from the following text segment of a scientific paper.\n",
    "    \n",
    "    TEXT:\n",
    "    {text}\n",
    "    \n",
    "    DOCUMENT ID: {document_id}\n",
    "    \n",
    "    INSTRUCTIONS:\n",
    "    1. Focus on extracting clear scientific claims, findings, and assertions\n",
    "    2. Pay special attention to sentences containing empirical findings with statistical significance\n",
    "    3. Look for claims in the Abstract, Results, and Discussion sections\n",
    "    4. Classify each claim as \"hypothesis\", \"finding\", or \"assertion\"\n",
    "    5. Rate importance based on: centrality to the paper's thesis, statistical significance, and novelty\n",
    "    6. Extract supporting evidence when available (statistical results, experiment outcomes, citations)\n",
    "    7. Include the document_id in the source_documents list for provenance tracking\n",
    "    \n",
    "    Return ONLY a JSON array of claims with this structure:\n",
    "    ```json\n",
    "    [\n",
    "      {{\n",
    "        \"claim_id\": \"claim_12345\",\n",
    "        \"claim_text\": \"Clear statement of the scientific claim\",\n",
    "        \"claim_type\": \"hypothesis|finding|assertion\",\n",
    "        \"importance\": \"high|medium|low\",\n",
    "        \"evidence\": \"Supporting evidence from text (optional)\",\n",
    "        \"source_documents\": [\"{document_id}\"],\n",
    "        \"confidence\": 0.8\n",
    "      }}\n",
    "    ]\n",
    "    ```\n",
    "    \n",
    "    CONFIG PARAMETERS:\n",
    "    {config}\n",
    "    \"\"\",\n",
    "    \n",
    "    # === METHODOLOGY EXTRACTION ===\n",
    "    # Purpose: Extract research methodologies with detailed information\n",
    "    \"methodology_extraction\": \"\"\"\n",
    "    You are a research methodology specialist. Extract key research methodologies from the following text segment of a scientific paper.\n",
    "    \n",
    "    TEXT:\n",
    "    {text}\n",
    "    \n",
    "    DOCUMENT ID: {document_id}\n",
    "    \n",
    "    INSTRUCTIONS:\n",
    "    1. Focus on the Methods/Methodology section, but also check Introduction for methodological approaches\n",
    "    2. Extract complete methodology descriptions including experimental designs, analytical techniques, and procedures\n",
    "    3. Identify methodological limitations when mentioned\n",
    "    4. Note the context in which each methodology was applied\n",
    "    5. Include the document_id in the source_documents list for provenance tracking\n",
    "    \n",
    "    Return ONLY a JSON array of methodologies with this structure:\n",
    "    ```json\n",
    "    [\n",
    "      {{\n",
    "        \"method_id\": \"method_12345\",\n",
    "        \"method_name\": \"Name or title of methodology\",\n",
    "        \"description\": \"Detailed description of the methodology\",\n",
    "        \"context\": \"Context in which the methodology was applied (optional)\",\n",
    "        \"limitations\": \"Known limitations of the methodology (optional)\",\n",
    "        \"source_documents\": [\"{document_id}\"],\n",
    "        \"confidence\": 0.8\n",
    "      }}\n",
    "    ]\n",
    "    ```\n",
    "    \n",
    "    CONFIG PARAMETERS:\n",
    "    {config}\n",
    "    \"\"\",\n",
    "    \n",
    "    # === CONTRIBUTION EXTRACTION ===\n",
    "    # Purpose: Extract key contributions and findings\n",
    "    \"contribution_extraction\": \"\"\"\n",
    "    You are a scientific contribution analyst. Extract key contributions from the following text segment of a scientific paper.\n",
    "    \n",
    "    TEXT:\n",
    "    {text}\n",
    "    \n",
    "    DOCUMENT ID: {document_id}\n",
    "    \n",
    "    INSTRUCTIONS:\n",
    "    1. Focus on significant contributions, innovations, and findings\n",
    "    2. Look primarily in Abstract, Introduction, and Conclusion sections\n",
    "    3. Classify each contribution as \"theoretical\", \"empirical\", \"methodological\", or \"practical\"\n",
    "    4. Rate importance based on novelty, impact, and emphasis in the text\n",
    "    5. Note connections to research claims and methods when evident\n",
    "    6. Include the document_id in the source_documents list for provenance tracking\n",
    "    \n",
    "    Return ONLY a JSON array of contributions with this structure:\n",
    "    ```json\n",
    "    [\n",
    "      {{\n",
    "        \"contribution_id\": \"contrib_12345\",\n",
    "        \"contribution_text\": \"Clear statement of the contribution\",\n",
    "        \"contribution_type\": \"theoretical|empirical|methodological|practical\",\n",
    "        \"importance\": \"high|medium|low\",\n",
    "        \"related_claims\": [],\n",
    "        \"related_methods\": [],\n",
    "        \"source_documents\": [\"{document_id}\"],\n",
    "        \"confidence\": 0.8\n",
    "      }}\n",
    "    ]\n",
    "    ```\n",
    "    \n",
    "    CONFIG PARAMETERS:\n",
    "    {config}\n",
    "    \"\"\",\n",
    "    \n",
    "    # === RESEARCH DIRECTION EXTRACTION ===\n",
    "    # Purpose: Extract future research directions\n",
    "    \"direction_extraction\": \"\"\"\n",
    "    You are a research direction analyst. Extract future research directions from the following text segment of a scientific paper.\n",
    "    \n",
    "    TEXT:\n",
    "    {text}\n",
    "    \n",
    "    DOCUMENT ID: {document_id}\n",
    "    \n",
    "    INSTRUCTIONS:\n",
    "    1. Focus on explicit suggestions for future research in Discussion and Conclusion sections\n",
    "    2. Look for phrases like \"future research should\", \"further studies are needed\"\n",
    "    3. Extract implicit research directions from identified limitations and knowledge gaps\n",
    "    4. Include rationale or justification for the research direction when available\n",
    "    5. Note connections to claims or contributions that the direction builds upon\n",
    "    6. Rate importance based on emphasis, specificity, and potential impact\n",
    "    7. Include the document_id in the source_documents list for provenance tracking\n",
    "    \n",
    "    Return ONLY a JSON array of research directions with this structure:\n",
    "    ```json\n",
    "    [\n",
    "      {{\n",
    "        \"direction_id\": \"direction_12345\",\n",
    "        \"direction_text\": \"Clear statement of the research direction\",\n",
    "        \"rationale\": \"Reasoning behind this direction (optional)\",\n",
    "        \"related_claims\": [],\n",
    "        \"related_contributions\": [],\n",
    "        \"source_documents\": [\"{document_id}\"],\n",
    "        \"importance\": \"high|medium|low\",\n",
    "        \"confidence\": 0.7\n",
    "      }}\n",
    "    ]\n",
    "    ```\n",
    "    \n",
    "    CONFIG PARAMETERS:\n",
    "    {config}\n",
    "    \"\"\",\n",
    "    \n",
    "    # === CROSS-DOCUMENT RELATIONSHIP IDENTIFICATION ===\n",
    "    # Purpose: Identify relationships between elements from different documents\n",
    "    \"cross_document_relationship\": \"\"\"\n",
    "    You are a scientific relationship analyst. Identify meaningful relationships between the following scientific elements from potentially different documents.\n",
    "    \n",
    "    ELEMENTS:\n",
    "    {elements}\n",
    "    \n",
    "    INSTRUCTIONS:\n",
    "    1. Analyze the provided elements and identify significant relationships between them\n",
    "    2. Focus on substantive relationships (supports, contradicts, extends, etc.)\n",
    "    3. Only create relationships that are justified by the element content\n",
    "    4. Prioritize relationships between elements from different documents (check the Documents field)\n",
    "    5. Generate a unique relationship_id for each relationship\n",
    "    6. Provide evidence or justification for each identified relationship\n",
    "    7. Assign a confidence score based on the strength of the relationship (0.0-1.0)\n",
    "    \n",
    "    Return ONLY a JSON array of relationships with this structure:\n",
    "    ```json\n",
    "    [\n",
    "      {{\n",
    "        \"relationship_id\": \"rel_[unique_id]\",\n",
    "        \"source_element_id\": \"ID of source element\",\n",
    "        \"source_element_type\": \"claim|methodology|contribution|direction\",\n",
    "        \"target_element_id\": \"ID of target element\",\n",
    "        \"target_element_type\": \"claim|methodology|contribution|direction\",\n",
    "        \"relationship_type\": \"supports|contradicts|extends|cites|replicates\",\n",
    "        \"evidence\": \"Justification for this relationship\",\n",
    "        \"confidence\": 0.8\n",
    "      }}\n",
    "    ]\n",
    "    ```\n",
    "    \"\"\",\n",
    "    \n",
    "    # === CLAIMS SYNTHESIS ===\n",
    "    # Purpose: Synthesize claims across multiple documents\n",
    "    \"claims_synthesis\": \"\"\"\n",
    "    You are a scientific claims synthesist. Create a comprehensive synthesis of scientific claims across multiple documents.\n",
    "    \n",
    "    CLAIMS:\n",
    "    {items}\n",
    "    \n",
    "    DOCUMENT COUNT: {document_count}\n",
    "    \n",
    "    INSTRUCTIONS:\n",
    "    1. Synthesize the key patterns, themes, and findings across all claims\n",
    "    2. Identify areas of consensus where multiple documents make similar claims\n",
    "    3. Highlight contradictions or disagreements between claims from different documents\n",
    "    4. Note the progression of knowledge or evolution of claims if time differences are apparent\n",
    "    5. Evaluate the strength of evidence across claims, noting where evidence is robust or lacking\n",
    "    6. Consider the importance ratings in determining emphasis\n",
    "    7. Use specific references to documents (as cited in source_documents)\n",
    "    \n",
    "    Format your response as a well-organized synthesis of the claims across {document_count} documents.\n",
    "    Include sections for: \n",
    "    - Major Consensus Findings\n",
    "    - Areas of Disagreement or Contradiction\n",
    "    - Emerging Trends\n",
    "    - Strength of Evidence Analysis\n",
    "    \"\"\",\n",
    "    \n",
    "    # === METHODOLOGIES SYNTHESIS ===\n",
    "    # Purpose: Synthesize methodologies across multiple documents\n",
    "    \"methodologies_synthesis\": \"\"\"\n",
    "    You are a research methodology synthesist. Create a comprehensive synthesis of research methodologies across multiple documents.\n",
    "    \n",
    "    METHODOLOGIES:\n",
    "    {items}\n",
    "    \n",
    "    DOCUMENT COUNT: {document_count}\n",
    "    \n",
    "    INSTRUCTIONS:\n",
    "    1. Identify the predominant methodological approaches across the document set\n",
    "    2. Categorize methodologies into broader methodological paradigms or techniques\n",
    "    3. Compare and contrast how similar methodologies are applied across different documents\n",
    "    4. Highlight methodological innovations or unique approaches\n",
    "    5. Analyze common limitations or challenges across methodologies\n",
    "    6. Identify methodological trends or evolutions if apparent\n",
    "    7. Assess the appropriateness and rigor of methodologies for their research contexts\n",
    "    8. Use specific references to documents (as cited in source_documents)\n",
    "    \n",
    "    Format your response as a well-organized synthesis of methodologies across {document_count} documents.\n",
    "    Include sections for:\n",
    "    - Predominant Methodological Approaches\n",
    "    - Methodological Variations and Innovations\n",
    "    - Common Limitations and Challenges\n",
    "    - Methodological Rigor Assessment\n",
    "    \"\"\",\n",
    "    \n",
    "    # === CONTRIBUTIONS SYNTHESIS ===\n",
    "    # Purpose: Synthesize contributions across multiple documents\n",
    "    \"contributions_synthesis\": \"\"\"\n",
    "    You are a scientific contribution synthesist. Create a comprehensive synthesis of research contributions across multiple documents.\n",
    "    \n",
    "    CONTRIBUTIONS:\n",
    "    {items}\n",
    "    \n",
    "    DOCUMENT COUNT: {document_count}\n",
    "    \n",
    "    INSTRUCTIONS:\n",
    "    1. Identify the major theoretical, empirical, methodological, and practical contributions\n",
    "    2. Map how contributions from different documents build upon or complement each other\n",
    "    3. Highlight particularly novel or high-impact contributions\n",
    "    4. Identify patterns in how theoretical and empirical contributions relate\n",
    "    5. Assess the cumulative impact of the contributions on the research field\n",
    "    6. Consider how practical and theoretical contributions align or diverge\n",
    "    7. Note any progression or evolution of contributions if time differences are apparent\n",
    "    8. Use specific references to documents (as cited in source_documents)\n",
    "    \n",
    "    Format your response as a well-organized synthesis of contributions across {document_count} documents.\n",
    "    Include sections for:\n",
    "    - Major Theoretical Advances\n",
    "    - Significant Empirical Findings\n",
    "    - Methodological Innovations\n",
    "    - Practical Applications and Implications\n",
    "    - Cumulative Impact Assessment\n",
    "    \"\"\",\n",
    "    \n",
    "    # === DIRECTIONS SYNTHESIS ===\n",
    "    # Purpose: Synthesize research directions across multiple documents\n",
    "    \"directions_synthesis\": \"\"\"\n",
    "    You are a research direction synthesist. Create a comprehensive synthesis of future research directions across multiple documents.\n",
    "    \n",
    "    RESEARCH DIRECTIONS:\n",
    "    {items}\n",
    "    \n",
    "    DOCUMENT COUNT: {document_count}\n",
    "    \n",
    "    INSTRUCTIONS:\n",
    "    1. Identify common research directions suggested across multiple documents\n",
    "    2. Group related directions into coherent research streams or themes\n",
    "    3. Prioritize directions based on frequency of mention and importance ratings\n",
    "    4. Analyze how directions from different documents complement or extend each other\n",
    "    5. Identify potential integrated research agendas that combine directions from multiple documents\n",
    "    6. Highlight directions that address significant gaps identified across the document set\n",
    "    7. Consider how theoretical and practical directions relate to each other\n",
    "    8. Use specific references to documents (as cited in source_documents)\n",
    "    \n",
    "    Format your response as a well-organized synthesis of research directions across {document_count} documents.\n",
    "    Include sections for:\n",
    "    - Priority Research Streams\n",
    "    - Cross-Cutting Research Opportunities\n",
    "    - Theoretical Development Needs\n",
    "    - Practical and Applied Research Directions\n",
    "    - Integrated Research Agenda Recommendations\n",
    "    \"\"\",\n",
    "    \n",
    "    # === MULTI-DOCUMENT SYNTHESIS ===\n",
    "    # Purpose: Generate comprehensive synthesis across all documents and categories\n",
    "    \"multi_document_synthesis\": \"\"\"\n",
    "    You are a scientific literature synthesist. Create a comprehensive synthesis of research across multiple scientific documents.\n",
    "    \n",
    "    DOCUMENT COUNT: {document_count}\n",
    "    DOCUMENT TITLES: {document_titles}\n",
    "    \n",
    "    CLAIMS SYNTHESIS:\n",
    "    {claims_synthesis}\n",
    "    \n",
    "    METHODOLOGIES SYNTHESIS:\n",
    "    {methods_synthesis}\n",
    "    \n",
    "    CONTRIBUTIONS SYNTHESIS:\n",
    "    {contributions_synthesis}\n",
    "    \n",
    "    RESEARCH DIRECTIONS SYNTHESIS:\n",
    "    {directions_synthesis}\n",
    "    \n",
    "    RELATIONSHIPS IDENTIFIED: {relationship_count}\n",
    "    RELATIONSHIP INFORMATION:\n",
    "    {relationship_info}\n",
    "    \n",
    "    INSTRUCTIONS:\n",
    "    1. Craft a comprehensive, integrated synthesis across all {document_count} documents\n",
    "    2. Begin with an executive summary highlighting the most significant findings\n",
    "    3. Present a conceptual framework that organizes the key insights across documents\n",
    "    4. Analyze how claims, methods, and contributions relate to each other across the literature\n",
    "    5. Highlight areas of consensus and well-established knowledge\n",
    "    6. Identify knowledge controversies, contradictions, or competing perspectives\n",
    "    7. Map the evolution or progression of research if temporal patterns are evident\n",
    "    8. Articulate the most promising integrated research agenda based on identified directions\n",
    "    9. Provide specific citations to documents when discussing findings (use document titles)\n",
    "    10. Consider both theoretical implications and practical applications\n",
    "    \n",
    "    Format your response as a comprehensive research synthesis with the following sections:\n",
    "    1. Executive Summary\n",
    "    2. Conceptual Framework\n",
    "    3. Knowledge Consensus\n",
    "    4. Open Questions and Controversies\n",
    "    5. Methodological Assessment\n",
    "    6. Significant Contributions\n",
    "    7. Integrated Research Agenda\n",
    "    8. Theoretical and Practical Implications\n",
    "    9. Conclusion\n",
    "    \n",
    "    Use references to specific documents throughout, and focus on cross-document patterns and insights.\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# Show confirmation\n",
    "show(\"Prompt library initialized with specialized scientific paper analysis prompts\", \"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core functions\n",
    "\"\"\"\n",
    "This section contains the core functionality for the LitSynth-Multidoc system.\n",
    "Each function is designed to support multi-document analysis with a focus on\n",
    "efficiency and clean architecture.\n",
    "\n",
    "This implementation uses a Retrieval-Augmented Generation (RAG) approach for\n",
    "more efficient and contextually aware extraction of elements from documents.\n",
    "\"\"\"\n",
    "\n",
    "import json, re, os, time, hashlib, uuid\n",
    "from typing import List, Dict, Any, Optional, Type, Union, Tuple, Callable, Literal\n",
    "from pathlib import Path\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document as LangchainDocument\n",
    "from datetime import datetime\n",
    "\n",
    "# ===== DOCUMENT PROCESSING =====\n",
    "\n",
    "def load_document(source_type: str, content: str) -> Dict[str, Any]:\n",
    "    \"\"\"Load document from various sources (text or PDF)\"\"\"\n",
    "    if source_type == \"text\":\n",
    "        return {\"text\": content, \"metadata\": {\"source_type\": \"text\", \"length\": len(content)}}\n",
    "    elif source_type == \"pdf\":\n",
    "        try:\n",
    "            from pypdf import PdfReader\n",
    "            reader = PdfReader(content)\n",
    "            text = \"\\n\\n\".join(page.extract_text() for page in reader.pages)\n",
    "            return {\"text\": text, \"metadata\": {\"source_type\": \"pdf\", \"filename\": os.path.basename(content), \"pages\": len(reader.pages)}}\n",
    "        except Exception as e:\n",
    "            return {\"text\": \"\", \"metadata\": {\"error\": str(e)}}\n",
    "    else:\n",
    "        return {\"text\": \"\", \"metadata\": {\"error\": \"Unsupported source type\"}}\n",
    "\n",
    "def process_doc(source_type: str, content: str, doc_id: str, config: Union[Dict, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Process document: load, chunk, and create vector store for efficient retrieval\n",
    "    \n",
    "    This function combines document loading and chunking into a unified process\n",
    "    that prepares the document for RAG-based extraction.\n",
    "    \"\"\"\n",
    "    # Load document\n",
    "    doc_data = load_document(source_type, content)\n",
    "    text = doc_data.get(\"text\", \"\")\n",
    "    \n",
    "    if not text:\n",
    "        return {\"error\": \"Failed to extract text\", \"vector_store\": None, \"metadata\": doc_data.get(\"metadata\", {})}\n",
    "    \n",
    "    # Get config values\n",
    "    chunk_size = getattr(config, 'text_chunk_size', config.get('text_chunk_size', 2000)) if not isinstance(config, int) else config\n",
    "    overlap = getattr(config, 'text_chunk_overlap', config.get('text_chunk_overlap', 200)) if not isinstance(config, int) else 200\n",
    "    \n",
    "    # Safety bounds for config values\n",
    "    chunk_size = min(max(chunk_size, 1000), 4000)\n",
    "    overlap = min(overlap, chunk_size // 4)\n",
    "    \n",
    "    # Create a text splitter that respects scientific document structure\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "        separators=[\"\\n## \", \"\\n### \", \"\\n# \", \"\\n\\n\", \"\\n\", \". \", \"! \", \"?\"],\n",
    "        length_function=len\n",
    "    )\n",
    "    \n",
    "    # Process document with metadata\n",
    "    langchain_docs = []\n",
    "    \n",
    "    # Try to identify scientific sections first\n",
    "    section_headers = [\n",
    "        r'\\n+\\s*ABSTRACT\\s*\\n+', r'\\n+\\s*INTRODUCTION\\s*\\n+', r'\\n+\\s*METHODS?\\s*\\n+',\n",
    "        r'\\n+\\s*RESULTS\\s*\\n+', r'\\n+\\s*DISCUSSION\\s*\\n+', r'\\n+\\s*CONCLUSION\\s*\\n+',\n",
    "        r'\\n+\\s*REFERENCES\\s*\\n+'\n",
    "    ]\n",
    "    \n",
    "    # Attempt to find section boundaries\n",
    "    section_splits = [0]\n",
    "    for pattern in section_headers:\n",
    "        for match in re.finditer(pattern, text, re.IGNORECASE):\n",
    "            section_splits.append(match.start())\n",
    "    section_splits.append(len(text))\n",
    "    section_splits = sorted(set(section_splits))\n",
    "    \n",
    "    # Extract sections with metadata\n",
    "    if len(section_splits) > 2:\n",
    "        # Document has identifiable sections\n",
    "        for i in range(len(section_splits) - 1):\n",
    "            start, end = section_splits[i], section_splits[i+1]\n",
    "            if end - start > 100:  # Avoid tiny sections\n",
    "                section_text = text[start:end].strip()\n",
    "                \n",
    "                # Identify section type\n",
    "                section_type = \"unknown\"\n",
    "                for pattern in section_headers:\n",
    "                    if re.match(pattern, section_text[:50], re.IGNORECASE):\n",
    "                        section_type = re.match(pattern, section_text[:50], re.IGNORECASE).group(0).strip()\n",
    "                        section_text = re.sub(pattern, \"\", section_text[:50], flags=re.IGNORECASE) + section_text[50:]\n",
    "                        break\n",
    "                \n",
    "                # Split section into chunks\n",
    "                chunks = splitter.split_text(section_text)\n",
    "                \n",
    "                # Create LangChain documents with metadata\n",
    "                for j, chunk in enumerate(chunks):\n",
    "                    langchain_docs.append(\n",
    "                        LangchainDocument(\n",
    "                            page_content=chunk,\n",
    "                            metadata={\n",
    "                                \"doc_id\": doc_id,\n",
    "                                \"section\": section_type,\n",
    "                                \"chunk_id\": f\"{doc_id}_chunk_{i}_{j}\",\n",
    "                                \"section_idx\": i,\n",
    "                                \"chunk_idx\": j\n",
    "                            }\n",
    "                        )\n",
    "                    )\n",
    "    else:\n",
    "        # No clear sections found, process as plain text\n",
    "        chunks = splitter.split_text(text)\n",
    "        for j, chunk in enumerate(chunks):\n",
    "            langchain_docs.append(\n",
    "                LangchainDocument(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\n",
    "                        \"doc_id\": doc_id,\n",
    "                        \"section\": \"unknown\",\n",
    "                        \"chunk_id\": f\"{doc_id}_chunk_{j}\",\n",
    "                        \"chunk_idx\": j\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    # Create vector store with embeddings\n",
    "    try:\n",
    "        if \"embeddings\" in globals() and embeddings is not None:\n",
    "            vector_store = FAISS.from_documents(langchain_docs, embeddings)\n",
    "            show(f\"Vector store created with {len(langchain_docs)} chunks\", \"success\")\n",
    "        else:\n",
    "            show(\"No embedding model available, using text-based retrieval\", \"warning\")\n",
    "            vector_store = None\n",
    "    except Exception as e:\n",
    "        show(f\"Error creating vector store: {str(e)}\", \"error\")\n",
    "        vector_store = None\n",
    "    \n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"vector_store\": vector_store,\n",
    "        \"chunks\": langchain_docs,\n",
    "        \"metadata\": doc_data.get(\"metadata\", {})\n",
    "    }\n",
    "\n",
    "# ===== LLM INTEGRATION =====\n",
    "\n",
    "def create_chain(prompt_name: str, output_model=None):\n",
    "    \"\"\"Create an LCEL chain for a specific LLM task\"\"\"\n",
    "    # Guard against missing prompts\n",
    "    if prompt_name not in PROMPTS:\n",
    "        show(f\"Prompt '{prompt_name}' not found in prompt library\", \"error\")\n",
    "        return None\n",
    "        \n",
    "    template = ChatPromptTemplate.from_template(PROMPTS[prompt_name])\n",
    "    \n",
    "    def parse_output(response):\n",
    "        \"\"\"Parse LLM response with multi-strategy approach\"\"\"\n",
    "        content = response.content if hasattr(response, 'content') else response\n",
    "        \n",
    "        # For text output (synthesis), return directly\n",
    "        if not output_model:\n",
    "            return content\n",
    "        \n",
    "        # For structured output, try multiple parsing strategies:\n",
    "        try:\n",
    "            # 1. Code block extraction (```json or ```)\n",
    "            if isinstance(content, str) and \"```\" in content:\n",
    "                if \"```json\" in content:\n",
    "                    json_block = content.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "                    data = json.loads(json_block)\n",
    "                else:\n",
    "                    code_block = content.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "                    if code_block.strip().startswith(\"[\") and code_block.strip().endswith(\"]\"):\n",
    "                        data = json.loads(code_block)\n",
    "                return [output_model(**item) for item in data]\n",
    "                \n",
    "            # 2. Direct JSON parsing\n",
    "            data = json.loads(content if isinstance(content, str) else content.content)\n",
    "            return [output_model(**item) for item in data]\n",
    "                \n",
    "        except Exception:\n",
    "            # 3. Fallback regex extraction\n",
    "            try:\n",
    "                matches = re.findall(r'\\[(.*?)\\]', content, re.DOTALL)\n",
    "                if matches:\n",
    "                    data = json.loads(f\"[{max(matches, key=len)}]\")\n",
    "                    return [output_model(**item) for item in data]\n",
    "            except Exception as e:\n",
    "                show(f\"All parsing methods failed: {str(e)}\", \"debug\")\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    # Create and return the chain\n",
    "    if \"llm\" in globals():\n",
    "        return template | llm | RunnableLambda(parse_output)\n",
    "    else:\n",
    "        return RunnableLambda(lambda _: [] if output_model else \"Placeholder output (no LLM configured)\")\n",
    "\n",
    "def cached_run(chain, inputs: Dict, key_prefix: str = \"\"):\n",
    "    \"\"\"Run chain with caching to minimize API calls\"\"\"\n",
    "    if not chain: \n",
    "        return [] if 'text' not in key_prefix else \"No chain available\"\n",
    "    \n",
    "    # Map key_prefix to the appropriate model class\n",
    "    model_map = {\n",
    "        \"claims\": ScientificClaim,\n",
    "        \"methodologies\": Methodology,\n",
    "        \"contributions\": KeyContribution,\n",
    "        \"directions\": ResearchDirection,\n",
    "        \"cross_relationships\": CrossDocumentRelationship\n",
    "    }\n",
    "    output_model = model_map.get(key_prefix)\n",
    "    \n",
    "    # Use the existing caching functions from Core Utilities\n",
    "    if 'CACHE_ENABLED' in globals() and CACHE_ENABLED:\n",
    "        # Prepare inputs for caching\n",
    "        cache_inputs = {}\n",
    "        for k, v in inputs.items():\n",
    "            if k == 'text' and isinstance(v, str) and len(v) > 500:\n",
    "                cache_inputs[k] = v[:500]  # Use first 500 chars of text for cache key\n",
    "            elif isinstance(v, (dict, list)) and len(str(v)) > 500:\n",
    "                # Handle large collections by hashing\n",
    "                cache_inputs[k] = hashlib.md5(str(v).encode()).hexdigest()\n",
    "            else:\n",
    "                cache_inputs[k] = v\n",
    "                \n",
    "        # Add prefix to differentiate between similar calls\n",
    "        cache_inputs['_function'] = key_prefix\n",
    "        \n",
    "        # Try to get cached result\n",
    "        try:\n",
    "            key = cache_key(**cache_inputs)\n",
    "            cached_result = get_cache(key)\n",
    "            \n",
    "            if cached_result is not None:\n",
    "                show(f\"Using cached result for {key_prefix}\", \"debug\")\n",
    "                \n",
    "                # Convert dictionaries back to Pydantic models if needed\n",
    "                if output_model and isinstance(cached_result, list) and cached_result and isinstance(cached_result[0], dict):\n",
    "                    try:\n",
    "                        cached_result = [output_model(**item) for item in cached_result]\n",
    "                    except Exception as e:\n",
    "                        show(f\"Model reconstruction error: {str(e)}\", \"debug\")\n",
    "                \n",
    "                return cached_result\n",
    "        except Exception as e:\n",
    "            show(f\"Cache access error: {str(e)}\", \"debug\")\n",
    "    \n",
    "    # Run chain if not in cache or caching disabled\n",
    "    try:\n",
    "        result = chain.invoke(inputs)\n",
    "        \n",
    "        # Try to cache the result\n",
    "        if 'CACHE_ENABLED' in globals() and CACHE_ENABLED:\n",
    "            try:\n",
    "                set_cache(key, result)\n",
    "            except Exception as e:\n",
    "                show(f\"Cache storage error: {str(e)}\", \"debug\")\n",
    "                \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        show(f\"Error in {key_prefix}: {error_msg}\", \"error\")\n",
    "        return [] if 'text' not in key_prefix else f\"Error: {error_msg}\"\n",
    "\n",
    "# ===== DOCUMENT COLLECTION MANAGEMENT =====\n",
    "\n",
    "def load_document_collection(documents: List[DocumentSource]) -> Dict[str, Any]:\n",
    "    \"\"\"Load and prepare multiple documents for processing\"\"\"\n",
    "    collection_stats = {\n",
    "        \"document_count\": len(documents),\n",
    "        \"total_size\": 0,\n",
    "        \"processing_estimate\": 0,\n",
    "        \"formats\": {}\n",
    "    }\n",
    "    \n",
    "    for i, doc in enumerate(documents):\n",
    "        # Load document content\n",
    "        doc_data = load_document(doc.source_type, doc.content)\n",
    "        \n",
    "        # Update document with content and metadata\n",
    "        doc.metadata.update(doc_data.get(\"metadata\", {}))\n",
    "        if not doc.title and \"filename\" in doc.metadata:\n",
    "            doc.title = doc.metadata[\"filename\"]\n",
    "        if not doc.title:\n",
    "            doc.title = f\"Document {i+1}\"\n",
    "            \n",
    "        # Generate document ID if not provided\n",
    "        if not doc.source_id:\n",
    "            doc.source_id = hashlib.md5(doc_data.get(\"text\", \"\")[:1000].encode()).hexdigest()[:10]\n",
    "        \n",
    "        # Update collection stats\n",
    "        collection_stats[\"total_size\"] += len(doc_data.get(\"text\", \"\"))\n",
    "        collection_stats[\"formats\"][doc.source_type] = collection_stats[\"formats\"].get(doc.source_type, 0) + 1\n",
    "    \n",
    "    # Estimate processing time based on document count and size\n",
    "    doc_count = collection_stats[\"document_count\"]\n",
    "    total_size = collection_stats[\"total_size\"]\n",
    "    base_time_per_doc = 20  # seconds - faster with RAG\n",
    "    time_per_10kb = 5       # seconds - faster with RAG\n",
    "    collection_stats[\"processing_estimate\"] = int((doc_count * base_time_per_doc) + (total_size / 10240 * time_per_10kb))\n",
    "    \n",
    "    return collection_stats\n",
    "\n",
    "# ===== ELEMENT EXTRACTION FUNCTIONS (RAG-BASED) =====\n",
    "\n",
    "def get_config_value(config: Union[Dict, Any], key: str, default: Any) -> Any:\n",
    "    \"\"\"Helper to get a value from either dict or object config\"\"\"\n",
    "    if isinstance(config, dict):\n",
    "        return config.get(key, default)\n",
    "    return getattr(config, key, default)\n",
    "\n",
    "def extract_elements_with_rag(\n",
    "    doc_data: Dict[str, Any],\n",
    "    config: Union[Dict, Any], \n",
    "    document_id: str, \n",
    "    element_type: Literal[\"claims\", \"methodologies\", \"contributions\", \"directions\"],\n",
    "    output_model: Type,\n",
    "    prompt_name: str\n",
    ") -> List[Any]:\n",
    "    \"\"\"Extract elements using RAG approach with focused retrieval and context\"\"\"\n",
    "    text = doc_data.get(\"text\", \"\")\n",
    "    vector_store = doc_data.get(\"vector_store\")\n",
    "    chunks = doc_data.get(\"chunks\", [])\n",
    "    \n",
    "    if not text or len(text.strip()) < 20:\n",
    "        show(f\"Text too short for {element_type} extraction\", \"warning\")\n",
    "        return []\n",
    "    \n",
    "    show(f\"Extracting {element_type} using RAG from document ({len(text)} chars)...\", \"info\")\n",
    "    \n",
    "    # Define retrieval queries based on element type\n",
    "    retrieval_queries = {\n",
    "        \"claims\": \"What are the main scientific claims, findings, and assertions in this paper?\",\n",
    "        \"methodologies\": \"What research methodologies, techniques, and approaches are described in this paper?\",\n",
    "        \"contributions\": \"What are the key contributions, innovations, and findings in this paper?\",\n",
    "        \"directions\": \"What future research directions are suggested or implied in this paper?\"\n",
    "    }\n",
    "    \n",
    "    # Define section priorities for each element type\n",
    "    section_priorities = {\n",
    "        \"claims\": [\"abstract\", \"results\", \"discussion\", \"conclusion\"],\n",
    "        \"methodologies\": [\"methods\", \"methodology\", \"materials\", \"experimental\"],\n",
    "        \"contributions\": [\"abstract\", \"introduction\", \"discussion\", \"conclusion\"],\n",
    "        \"directions\": [\"discussion\", \"conclusion\", \"future work\", \"limitations\"]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Get retrieval context\n",
    "        if vector_store is not None:\n",
    "            # Use vector retrieval\n",
    "            query = retrieval_queries.get(element_type, f\"Information about {element_type}\")\n",
    "            retrieved_docs = vector_store.similarity_search(\n",
    "                query,\n",
    "                k=5  # Get top 5 relevant chunks\n",
    "            )\n",
    "            retrieved_texts = [doc.page_content for doc in retrieved_docs]\n",
    "            \n",
    "            # Combine retrieved texts\n",
    "            retrieval_context = \"\\n\\n---\\n\\n\".join(retrieved_texts)\n",
    "        else:\n",
    "            # Fallback: Use prioritized sections or first/last parts\n",
    "            prioritized_chunks = []\n",
    "            priority_sections = section_priorities.get(element_type, [])\n",
    "            \n",
    "            # First, try to find chunks from priority sections\n",
    "            for chunk in chunks:\n",
    "                section = chunk.metadata.get(\"section\", \"\").lower()\n",
    "                for priority in priority_sections:\n",
    "                    if priority in section:\n",
    "                        prioritized_chunks.append(chunk)\n",
    "                        break\n",
    "            \n",
    "            # If no priority chunks found, use document start and end\n",
    "            if not prioritized_chunks and chunks:\n",
    "                # Include beginning chunks\n",
    "                prioritized_chunks.extend(chunks[:2])\n",
    "                # Include ending chunks if document is long enough\n",
    "                if len(chunks) > 4:\n",
    "                    prioritized_chunks.extend(chunks[-2:])\n",
    "            \n",
    "            # Combine texts from prioritized chunks\n",
    "            retrieval_context = \"\\n\\n---\\n\\n\".join([chunk.page_content for chunk in prioritized_chunks])\n",
    "            \n",
    "            # If still empty, use document summary (first and last portions)\n",
    "            if not retrieval_context:\n",
    "                doc_start = text[:2000] if len(text) > 2000 else text\n",
    "                doc_end = text[-2000:] if len(text) > 4000 else \"\"\n",
    "                retrieval_context = f\"{doc_start}\\n\\n[...]\\n\\n{doc_end}\"\n",
    "            \n",
    "        # Create chain for extraction\n",
    "        chain = create_chain(prompt_name, output_model)\n",
    "        \n",
    "        # Convert config to dict if needed\n",
    "        config_dict = config if isinstance(config, dict) else config.model_dump()\n",
    "        \n",
    "        # Execute extraction with retrieved context\n",
    "        elements = cached_run(chain, {\n",
    "            \"text\": retrieval_context, \n",
    "            \"document_id\": document_id, \n",
    "            \"config\": config_dict\n",
    "        }, element_type)\n",
    "        \n",
    "        # Add source document to all elements\n",
    "        source_field_name = \"source_documents\"  # All models use the same field name\n",
    "        for element in elements:\n",
    "            sources = getattr(element, source_field_name, [])\n",
    "            if document_id not in sources:\n",
    "                setattr(element, source_field_name, sources + [document_id])\n",
    "        \n",
    "        # Apply filtering by importance if applicable\n",
    "        if hasattr(output_model, \"importance\"):\n",
    "            importance_map = {\"high\": 3, \"medium\": 2, \"low\": 1}\n",
    "            min_importance = get_config_value(config, \"min_importance\", \"medium\")\n",
    "            min_value = importance_map.get(min_importance, 1)\n",
    "            \n",
    "            if not elements:\n",
    "                show(f\"No {element_type} extracted\", \"warning\")\n",
    "                return []\n",
    "                \n",
    "            elements = [e for e in elements if importance_map.get(getattr(e, \"importance\", \"low\"), 0) >= min_value]\n",
    "            elements.sort(key=lambda e: importance_map.get(getattr(e, \"importance\", \"low\"), 0), reverse=True)\n",
    "        \n",
    "        # Apply maximum limit from config\n",
    "        max_config_key = f\"max_{element_type}\"\n",
    "        max_elements = get_config_value(config, max_config_key, 20)\n",
    "        \n",
    "        show(f\"Extracted {len(elements[:max_elements])} {element_type}\", \"info\")\n",
    "        return elements[:max_elements]\n",
    "        \n",
    "    except Exception as e:\n",
    "        show(f\"Error in {element_type} extraction: {str(e)}\", \"error\")\n",
    "        return []\n",
    "\n",
    "# Element-specific extraction functions using the RAG-based extractor\n",
    "def extract_scientific_claims(doc_data: Union[Dict[str, Any], str], config: Union[Dict, Any], document_id: str) -> List[ScientificClaim]:\n",
    "    \"\"\"Extract scientific claims using RAG with document source tracking\"\"\"\n",
    "    # Handle both RAG and legacy input formats\n",
    "    if isinstance(doc_data, str):\n",
    "        # Legacy mode - convert to minimal doc_data for backward compatibility\n",
    "        doc_data = {\"text\": doc_data, \"vector_store\": None, \"chunks\": []}\n",
    "    \n",
    "    return extract_elements_with_rag(doc_data, config, document_id, \"claims\", ScientificClaim, \"claim_extraction\")\n",
    "\n",
    "def extract_methodologies(doc_data: Union[Dict[str, Any], str], config: Union[Dict, Any], document_id: str) -> List[Methodology]:\n",
    "    \"\"\"Extract research methodologies using RAG with document source tracking\"\"\"\n",
    "    # Handle both RAG and legacy input formats\n",
    "    if isinstance(doc_data, str):\n",
    "        # Legacy mode - convert to minimal doc_data for backward compatibility\n",
    "        doc_data = {\"text\": doc_data, \"vector_store\": None, \"chunks\": []}\n",
    "    \n",
    "    return extract_elements_with_rag(doc_data, config, document_id, \"methodologies\", Methodology, \"methodology_extraction\")\n",
    "\n",
    "def extract_key_contributions(doc_data: Union[Dict[str, Any], str], config: Union[Dict, Any], document_id: str) -> List[KeyContribution]:\n",
    "    \"\"\"Extract key contributions using RAG with document source tracking\"\"\"\n",
    "    # Handle both RAG and legacy input formats\n",
    "    if isinstance(doc_data, str):\n",
    "        # Legacy mode - convert to minimal doc_data for backward compatibility\n",
    "        doc_data = {\"text\": doc_data, \"vector_store\": None, \"chunks\": []}\n",
    "    \n",
    "    return extract_elements_with_rag(doc_data, config, document_id, \"contributions\", KeyContribution, \"contribution_extraction\")\n",
    "\n",
    "def extract_research_directions(doc_data: Union[Dict[str, Any], str], config: Union[Dict, Any], document_id: str) -> List[ResearchDirection]:\n",
    "    \"\"\"Extract research directions using RAG with document source tracking\"\"\"\n",
    "    # Handle both RAG and legacy input formats\n",
    "    if isinstance(doc_data, str):\n",
    "        # Legacy mode - convert to minimal doc_data for backward compatibility\n",
    "        doc_data = {\"text\": doc_data, \"vector_store\": None, \"chunks\": []}\n",
    "    \n",
    "    return extract_elements_with_rag(doc_data, config, document_id, \"directions\", ResearchDirection, \"direction_extraction\")\n",
    "\n",
    "# ===== ELEMENT DEDUPLICATION AND MERGING =====\n",
    "\n",
    "def text_similarity(text1: str, text2: str) -> float:\n",
    "    \"\"\"Calculate simple text similarity using Jaccard index\"\"\"\n",
    "    # Convert to lowercase and remove punctuation\n",
    "    t1 = re.sub(r'[^\\w\\s]', '', text1.lower())\n",
    "    t2 = re.sub(r'[^\\w\\s]', '', text2.lower())\n",
    "    \n",
    "    # Word sets\n",
    "    words1 = set(t1.split())\n",
    "    words2 = set(t2.split())\n",
    "    \n",
    "    # Calculate Jaccard similarity\n",
    "    intersection = len(words1.intersection(words2))\n",
    "    union = len(words1.union(words2))\n",
    "    \n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "def get_element_text(element: Any) -> str:\n",
    "    \"\"\"Extract primary text content from different element types\"\"\"\n",
    "    element_type = type(element).__name__.lower()\n",
    "    \n",
    "    if element_type == \"scientificclaim\":\n",
    "        return element.claim_text\n",
    "    elif element_type == \"methodology\":\n",
    "        return f\"{element.method_name}: {element.description}\"\n",
    "    elif element_type == \"keycontribution\":\n",
    "        return element.contribution_text\n",
    "    elif element_type == \"researchdirection\":\n",
    "        return element.direction_text\n",
    "    return str(element)\n",
    "\n",
    "def merge_source_documents(existing: Any, element: Any) -> None:\n",
    "    \"\"\"Merge source documents from element into existing element\"\"\"\n",
    "    if hasattr(existing, 'source_documents') and hasattr(element, 'source_documents'):\n",
    "        for doc_id in element.source_documents:\n",
    "            if doc_id not in existing.source_documents:\n",
    "                existing.source_documents.append(doc_id)\n",
    "\n",
    "def deduplicate_elements(elements: List[Any], similarity_threshold: float = 0.75) -> List[Any]:\n",
    "    \"\"\"Deduplicate similar elements while preserving source document information\"\"\"\n",
    "    if not elements or len(elements) <= 1:\n",
    "        return elements\n",
    "        \n",
    "    # Use embeddings to calculate similarity when available\n",
    "    if \"embeddings\" in globals() and embeddings is not None:\n",
    "        try:\n",
    "            # Get text to embed for each element\n",
    "            element_texts = [get_element_text(element) for element in elements]\n",
    "            \n",
    "            # Generate embeddings\n",
    "            embedded = embeddings.embed_documents(element_texts)\n",
    "            \n",
    "            # Find similar elements\n",
    "            result = []\n",
    "            added_indices = set()\n",
    "            \n",
    "            for i, element in enumerate(elements):\n",
    "                if i in added_indices:\n",
    "                    continue\n",
    "                    \n",
    "                result.append(element)\n",
    "                added_indices.add(i)\n",
    "                \n",
    "                # Find similar elements\n",
    "                for j in range(i + 1, len(elements)):\n",
    "                    if j in added_indices:\n",
    "                        continue\n",
    "                        \n",
    "                    # Calculate cosine similarity\n",
    "                    similarity = sum(a * b for a, b in zip(embedded[i], embedded[j])) / (\n",
    "                        (sum(a * a for a in embedded[i]) ** 0.5) * \n",
    "                        (sum(b * b for b in embedded[j]) ** 0.5)\n",
    "                    )\n",
    "                    \n",
    "                    if similarity >= similarity_threshold:\n",
    "                        # Merge source documents\n",
    "                        merge_source_documents(element, elements[j])\n",
    "                        added_indices.add(j)\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            show(f\"Error using embeddings for deduplication: {str(e)}\", \"debug\")\n",
    "    \n",
    "    # Fallback implementation using text comparison\n",
    "    result = []\n",
    "    added_texts = set()\n",
    "    \n",
    "    for element in elements:\n",
    "        # Create normalized text for comparison\n",
    "        compare_text = get_element_text(element).lower()\n",
    "            \n",
    "        # Check if we already have a similar element\n",
    "        is_duplicate = False\n",
    "        for existing_text in added_texts:\n",
    "            if text_similarity(compare_text, existing_text) >= similarity_threshold:\n",
    "                is_duplicate = True\n",
    "                # Merge source documents into existing element\n",
    "                for existing in result:\n",
    "                    if get_element_text(existing).lower() == existing_text:\n",
    "                        merge_source_documents(existing, element)\n",
    "                        break\n",
    "                break\n",
    "                \n",
    "        if not is_duplicate:\n",
    "            added_texts.add(compare_text)\n",
    "            result.append(element)\n",
    "            \n",
    "    return result\n",
    "\n",
    "# ===== CROSS-DOCUMENT ANALYSIS =====\n",
    "\n",
    "def format_elements_for_analysis(elements: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"Format elements for cross-document relationship analysis\"\"\"\n",
    "    return \"\\n\\n\".join([\n",
    "        f\"ID: {item.get('element_id', f'element_{i}')}\\n\"\n",
    "        f\"Type: {item.get('type', 'unknown')}\\n\"\n",
    "        f\"Text: {item.get('text', '')}\\n\"\n",
    "        f\"Documents: {', '.join(item.get('documents', []))}\"\n",
    "        for i, item in enumerate(elements)\n",
    "    ])\n",
    "\n",
    "def identify_cross_document_relationships(\n",
    "    claims: List[ScientificClaim], \n",
    "    methodologies: List[Methodology],\n",
    "    contributions: List[KeyContribution],\n",
    "    directions: List[ResearchDirection],\n",
    "    config: Union[Dict, Any]\n",
    ") -> List[CrossDocumentRelationship]:\n",
    "    \"\"\"Identify relationships between elements from different documents\"\"\"\n",
    "    # Create a mapping of all elements for easy lookup\n",
    "    element_map = {}\n",
    "    \n",
    "    # Helper to add elements to map\n",
    "    def add_to_map(elements, element_type, id_field, text_getter):\n",
    "        for element in elements:\n",
    "            if hasattr(element, 'source_documents') and len(element.source_documents) > 0:\n",
    "                element_map[getattr(element, id_field)] = {\n",
    "                    \"element_id\": getattr(element, id_field),\n",
    "                    \"type\": element_type,\n",
    "                    \"text\": text_getter(element),\n",
    "                    \"documents\": element.source_documents\n",
    "                }\n",
    "    \n",
    "    # Add all element types to map\n",
    "    add_to_map(claims, \"claim\", \"claim_id\", lambda e: e.claim_text)\n",
    "    add_to_map(methodologies, \"methodology\", \"method_id\", lambda e: f\"{e.method_name}: {e.description}\")\n",
    "    add_to_map(contributions, \"contribution\", \"contribution_id\", lambda e: e.contribution_text)\n",
    "    add_to_map(directions, \"direction\", \"direction_id\", lambda e: e.direction_text)\n",
    "    \n",
    "    # Get relevant elements (either in multiple docs or potentially related)\n",
    "    cross_doc_elements = [element_id for element_id, info in element_map.items()]\n",
    "    \n",
    "    # Use document combinations for uniqueness\n",
    "    doc_combos = set()\n",
    "    unique_elements = []\n",
    "    \n",
    "    for element_id in cross_doc_elements:\n",
    "        docs = frozenset(element_map[element_id][\"documents\"])\n",
    "        if docs not in doc_combos:\n",
    "            doc_combos.add(docs)\n",
    "            unique_elements.append(element_id)\n",
    "    \n",
    "    # Get config values\n",
    "    relationship_confidence = get_config_value(config, 'relationship_confidence', 0.6)\n",
    "    max_relationships = get_config_value(config, 'max_cross_relationships', 50)\n",
    "    \n",
    "    # Process in batches to avoid LLM context limits\n",
    "    batch_size = 10\n",
    "    relationships = []\n",
    "    \n",
    "    for i in range(0, len(unique_elements), batch_size):\n",
    "        batch = unique_elements[i:i+batch_size]\n",
    "        batch_elements = [element_map[element_id] for element_id in batch]\n",
    "        \n",
    "        # Run relationship analysis chain\n",
    "        chain = create_chain(\"cross_document_relationship\", CrossDocumentRelationship)\n",
    "        batch_relationships = cached_run(chain, {\n",
    "            \"elements\": format_elements_for_analysis(batch_elements)\n",
    "        }, \"cross_relationships\")\n",
    "        \n",
    "        relationships.extend(batch_relationships)\n",
    "    \n",
    "    # Filter and sort by confidence\n",
    "    valid_relationships = [r for r in relationships if r.confidence >= relationship_confidence]\n",
    "    valid_relationships.sort(key=lambda r: r.confidence, reverse=True)\n",
    "    \n",
    "    return valid_relationships[:max_relationships]\n",
    "\n",
    "# ===== SYNTHESIS GENERATION =====\n",
    "\n",
    "def format_items_for_synthesis(items: List[Any], category_type: str) -> str:\n",
    "    \"\"\"Format items for synthesis prompt\"\"\"\n",
    "    formatted = []\n",
    "    \n",
    "    for item in items:\n",
    "        if category_type == \"claims\":\n",
    "            text = [\n",
    "                f\"• {item.claim_text}\",\n",
    "                f\"  - Type: {item.claim_type}\",\n",
    "                f\"  - Importance: {item.importance}\"\n",
    "            ]\n",
    "            if item.evidence:\n",
    "                text.append(f\"  - Evidence: {item.evidence}\")\n",
    "            text.append(f\"  - Source Documents: {', '.join(item.source_documents)}\")\n",
    "            formatted.append(\"\\n\".join(text))\n",
    "            \n",
    "        elif category_type == \"methodologies\":\n",
    "            text = [\n",
    "                f\"• {item.method_name}\",\n",
    "                f\"  - Description: {item.description}\"\n",
    "            ]\n",
    "            if item.context:\n",
    "                text.append(f\"  - Context: {item.context}\")\n",
    "            if item.limitations:\n",
    "                text.append(f\"  - Limitations: {item.limitations}\")\n",
    "            text.append(f\"  - Source Documents: {', '.join(item.source_documents)}\")\n",
    "            formatted.append(\"\\n\".join(text))\n",
    "            \n",
    "        elif category_type == \"contributions\":\n",
    "            text = [\n",
    "                f\"• {item.contribution_text}\",\n",
    "                f\"  - Type: {item.contribution_type}\",\n",
    "                f\"  - Importance: {item.importance}\"\n",
    "            ]\n",
    "            if item.related_claims:\n",
    "                text.append(f\"  - Related Claims: {len(item.related_claims)}\")\n",
    "            if item.related_methods:\n",
    "                text.append(f\"  - Related Methods: {len(item.related_methods)}\")\n",
    "            text.append(f\"  - Source Documents: {', '.join(item.source_documents)}\")\n",
    "            formatted.append(\"\\n\".join(text))\n",
    "            \n",
    "        elif category_type == \"directions\":\n",
    "            text = [\n",
    "                f\"• {item.direction_text}\",\n",
    "                f\"  - Importance: {item.importance}\"\n",
    "            ]\n",
    "            if item.rationale:\n",
    "                text.append(f\"  - Rationale: {item.rationale}\")\n",
    "            text.append(f\"  - Source Documents: {', '.join(item.source_documents)}\")\n",
    "            formatted.append(\"\\n\".join(text))\n",
    "\n",
    "    return \"\\n\\n\".join(formatted)\n",
    "\n",
    "def synthesize_across_category(items: List[Any], category_type: str, document_count: int) -> CategorySynthesis:\n",
    "    \"\"\"Generate synthesis across documents for a specific category\"\"\"\n",
    "    if not items:\n",
    "        return CategorySynthesis(\n",
    "            category=category_type,\n",
    "            synthesis_text=f\"No {category_type} were extracted from the documents.\",\n",
    "            element_count=0,\n",
    "            document_count=document_count\n",
    "        )\n",
    "\n",
    "    # Format items for the prompt\n",
    "    items_text = format_items_for_synthesis(items, category_type)\n",
    "\n",
    "    # Use category-specific prompt\n",
    "    prompt_key = f\"{category_type}_synthesis\"\n",
    "\n",
    "    # Run synthesis chain\n",
    "    chain = create_chain(prompt_key)\n",
    "    synthesis_text = cached_run(chain, {\n",
    "        \"items\": items_text,\n",
    "        \"document_count\": document_count,\n",
    "        \"category\": category_type\n",
    "    }, f\"{category_type}_synthesis\")\n",
    "\n",
    "    return CategorySynthesis(\n",
    "        category=category_type,\n",
    "        synthesis_text=synthesis_text,\n",
    "        element_count=len(items),\n",
    "        document_count=document_count\n",
    "    )\n",
    "\n",
    "def format_relationships_for_synthesis(relationships: List[CrossDocumentRelationship]) -> str:\n",
    "    \"\"\"Format relationships for synthesis prompt\"\"\"\n",
    "    return \"\\n\\n\".join([\n",
    "        f\"• {rel.source_element_type.capitalize()} '{rel.source_element_id}' {rel.relationship_type} \" +\n",
    "        f\"{rel.target_element_type.capitalize()} '{rel.target_element_id}'\" +\n",
    "        (f\"\\n  - Evidence: {rel.evidence}\" if rel.evidence else \"\") +\n",
    "        f\"\\n  - Confidence: {rel.confidence:.2f}\"\n",
    "        for rel in relationships\n",
    "    ])\n",
    "\n",
    "def generate_multi_document_synthesis(\n",
    "    documents: List[DocumentSource],\n",
    "    category_syntheses: Dict[str, CategorySynthesis],\n",
    "    relationships: List[CrossDocumentRelationship]\n",
    ") -> str:\n",
    "    \"\"\"Generate comprehensive synthesis across all documents and categories\"\"\"\n",
    "    # Extract document information\n",
    "    document_titles = [doc.title or f\"Document {i+1}\" for i, doc in enumerate(documents)]\n",
    "\n",
    "    # Get category syntheses (safely)\n",
    "    get_synthesis = lambda cat: category_syntheses.get(cat, CategorySynthesis(\n",
    "        category=cat, synthesis_text=\"\", element_count=0, document_count=len(documents)\n",
    "    )).synthesis_text\n",
    "\n",
    "    claims_synthesis = get_synthesis(\"claims\")\n",
    "    methods_synthesis = get_synthesis(\"methodologies\")\n",
    "    contributions_synthesis = get_synthesis(\"contributions\")\n",
    "    directions_synthesis = get_synthesis(\"directions\")\n",
    "\n",
    "    # Format relationship information\n",
    "    relationship_info = format_relationships_for_synthesis(relationships)\n",
    "\n",
    "    # Create overall synthesis\n",
    "    chain = create_chain(\"multi_document_synthesis\")\n",
    "    synthesis = cached_run(chain, {\n",
    "        \"document_count\": len(documents),\n",
    "        \"document_titles\": document_titles,\n",
    "        \"claims_synthesis\": claims_synthesis,\n",
    "        \"methods_synthesis\": methods_synthesis,\n",
    "        \"contributions_synthesis\": contributions_synthesis,\n",
    "        \"directions_synthesis\": directions_synthesis,\n",
    "        \"relationship_count\": len(relationships),\n",
    "        \"relationship_info\": relationship_info\n",
    "    }, \"final_synthesis\")\n",
    "\n",
    "    return synthesis\n",
    "\n",
    "# ===== MAIN MULTI-DOCUMENT ANALYSIS FUNCTION =====\n",
    "\n",
    "def analyze_multiple_documents(\n",
    "    documents: List[DocumentSource], \n",
    "    config: Union[Dict, Any] = None\n",
    ") -> MultiDocSynthesisOutput:\n",
    "    \"\"\"Complete end-to-end multi-document analysis using RAG approach\"\"\"\n",
    "    if config is None:\n",
    "        config = LitSynthMultiConfig().model_dump()\n",
    "\n",
    "    try:\n",
    "        # 1. Initialize results container\n",
    "        results = MultiDocSynthesisOutput()\n",
    "        results.documents = documents\n",
    "\n",
    "        # 2. Load and prepare document collection\n",
    "        collection_stats = load_document_collection(documents)\n",
    "        show(f\"Processing {len(documents)} documents...\", \"info\")\n",
    "\n",
    "        # 3. Process each document using RAG approach\n",
    "        all_claims = []\n",
    "        all_methodologies = []\n",
    "        all_contributions = []\n",
    "        all_directions = []\n",
    "\n",
    "        for i, doc in enumerate(documents):\n",
    "            show(f\"Processing document {i+1}/{len(documents)}: {doc.title or f'Document {i+1}'}\", \"info\")\n",
    "\n",
    "            # Process document with RAG approach\n",
    "            doc_data = process_doc(doc.source_type, doc.content, doc.source_id, config)\n",
    "\n",
    "            if \"error\" in doc_data:\n",
    "                show(f\"Failed to process document {i+1}: {doc_data['error']}\", \"error\")\n",
    "                continue\n",
    "\n",
    "            # Extract all element types using RAG\n",
    "            doc_claims = extract_scientific_claims(doc_data, config, doc.source_id)\n",
    "            doc_methodologies = extract_methodologies(doc_data, config, doc.source_id)\n",
    "            doc_contributions = extract_key_contributions(doc_data, config, doc.source_id)\n",
    "            doc_directions = extract_research_directions(doc_data, config, doc.source_id)\n",
    "\n",
    "            # Add to collection\n",
    "            all_claims.extend(doc_claims)\n",
    "            all_methodologies.extend(doc_methodologies)\n",
    "            all_contributions.extend(doc_contributions)\n",
    "            all_directions.extend(doc_directions)\n",
    "\n",
    "            # Mark document as processed\n",
    "            doc.processed = True\n",
    "\n",
    "        # 4. Deduplicate across documents\n",
    "        similarity_threshold = get_config_value(config, 'similarity_threshold', 0.75)\n",
    "\n",
    "        claims = deduplicate_elements(all_claims, similarity_threshold)\n",
    "        methodologies = deduplicate_elements(all_methodologies, similarity_threshold)\n",
    "        contributions = deduplicate_elements(all_contributions, similarity_threshold)\n",
    "        directions = deduplicate_elements(all_directions, similarity_threshold)\n",
    "\n",
    "        # 5. Identify cross-document relationships\n",
    "        relationships = identify_cross_document_relationships(\n",
    "            claims, methodologies, contributions, directions, config\n",
    "        )\n",
    "\n",
    "        # 6. Generate category-specific syntheses\n",
    "        category_syntheses = {\n",
    "            \"claims\": synthesize_across_category(claims, \"claims\", len(documents)),\n",
    "            \"methodologies\": synthesize_across_category(methodologies, \"methodologies\", len(documents)),\n",
    "            \"contributions\": synthesize_across_category(contributions, \"contributions\", len(documents)),\n",
    "            \"directions\": synthesize_across_category(directions, \"directions\", len(documents))\n",
    "        }\n",
    "\n",
    "        # 7. Generate overall synthesis\n",
    "        overall_synthesis = generate_multi_document_synthesis(\n",
    "            documents, category_syntheses, relationships\n",
    "        )\n",
    "\n",
    "        # 8. Populate results\n",
    "        results.claims = claims\n",
    "        results.methodologies = methodologies\n",
    "        results.contributions = contributions\n",
    "        results.research_directions = directions\n",
    "        results.cross_document_relationships = relationships\n",
    "        results.category_syntheses = category_syntheses\n",
    "        results.overall_synthesis = overall_synthesis\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        # Return error container\n",
    "        error_output = MultiDocSynthesisOutput(\n",
    "            analysis_id=f\"error_{int(time.time())}\",\n",
    "            documents=documents\n",
    "        )\n",
    "        error_output.overall_synthesis = f\"Analysis error: {str(e)}\"\n",
    "        return error_output\n",
    "\n",
    "# ===== STATE MANAGEMENT UTILITY FUNCTIONS =====\n",
    "\n",
    "def save_session(state: LitSynthMultiState, filepath: Optional[str] = None) -> str:\n",
    "    \"\"\"Save current analysis session to disk\"\"\"\n",
    "    if not filepath:\n",
    "        filepath = STATES_DIR / f\"{state.session_id}.json\"\n",
    "\n",
    "    saved_path = state.save(filepath)\n",
    "    show(f\"Analysis session saved to {saved_path}\", \"success\")\n",
    "    return saved_path\n",
    "\n",
    "def load_session(filepath: str) -> LitSynthMultiState:\n",
    "    \"\"\"Load analysis session from disk\"\"\"\n",
    "    try:\n",
    "        state = LitSynthMultiState.load(filepath)\n",
    "        show(f\"Analysis session loaded from {filepath}\", \"success\")\n",
    "        return state\n",
    "    except Exception as e:\n",
    "        show(f\"Error loading session: {e}\", \"error\")\n",
    "        return LitSynthMultiState()\n",
    "\n",
    "def process_document_from_state(state: LitSynthMultiState, document_id: str) -> bool:\n",
    "    \"\"\"Process a single document from the state using RAG approach\"\"\"\n",
    "    # Find the document in state\n",
    "    document = next((doc for doc in state.data['documents'] if doc.source_id == document_id), None)\n",
    "\n",
    "    if not document:\n",
    "        show(f\"Document {document_id} not found in state\", \"error\")\n",
    "        return False\n",
    "\n",
    "    # Skip if already processed\n",
    "    if document.processed:\n",
    "        show(f\"Document {document.title or document_id} already processed\", \"info\")\n",
    "        return True\n",
    "\n",
    "    try:\n",
    "        # Get config from state\n",
    "        config = state.config if isinstance(state.config, dict) else LitSynthMultiConfig(**state.config).model_dump()\n",
    "\n",
    "        # Process document with RAG approach\n",
    "        doc_data = process_doc(document.source_type, document.content, document.source_id, config)\n",
    "\n",
    "        if \"error\" in doc_data:\n",
    "            show(f\"Failed to process document {document_id}: {doc_data['error']}\", \"error\")\n",
    "            return False\n",
    "\n",
    "        # Extract all element types using RAG\n",
    "        results = {\n",
    "            'claims': extract_scientific_claims(doc_data, config, document.source_id),\n",
    "            'methodologies': extract_methodologies(doc_data, config, document.source_id),\n",
    "            'contributions': extract_key_contributions(doc_data, config, document.source_id),\n",
    "            'research_directions': extract_research_directions(doc_data, config, document.source_id)\n",
    "        }\n",
    "\n",
    "        # Update state with all results\n",
    "        for category, elements in results.items():\n",
    "            state.update_extraction_results(document.source_id, category, elements)\n",
    "\n",
    "        # Mark document as processed and signal cross-document update\n",
    "        state.mark_document_processed(document.source_id)\n",
    "        state.regenerate_cross_document_analysis()\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        show(f\"Error processing document {document_id}: {e}\", \"error\")\n",
    "        return False\n",
    "\n",
    "# Initialization complete\n",
    "show(\"Core functions initialized for multi-document analysis with RAG approach\", \"success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Initialization\n",
    "\"\"\"\n",
    "This cell initializes the LitSynth-Multidoc system, connecting all components\n",
    "defined in previous sections and preparing the system for document analysis.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "try:\n",
    "    # Initialize the Literature Synthesis Multi-Document System\n",
    "    class LitSynthMultiSystem:\n",
    "        \"\"\"Main system class that coordinates all components of the Multi-Document Literature Synthesis system.\"\"\"\n",
    "        \n",
    "        def __init__(self, state=None, config=None):\n",
    "            \"\"\"Initialize the system with state management and configuration.\n",
    "            \n",
    "            Args:\n",
    "                state: Existing state container or None to use global state\n",
    "                config: Configuration parameters or None to use default/global config\n",
    "            \"\"\"\n",
    "            # Use existing state or create new one\n",
    "            self.state = state or globals().get('state', LitSynthMultiState())\n",
    "            \n",
    "            # Use existing config or create new one\n",
    "            if config:\n",
    "                self.state.config = config\n",
    "            elif not self.state.config and 'config' in globals():\n",
    "                self.state.config = globals()['config'].model_dump()\n",
    "            \n",
    "            # System status tracking\n",
    "            self.system_id = f\"litsynth_multi_{int(time.time())}\"\n",
    "            \n",
    "            # Register system in state for tracking\n",
    "            self.state.status.update({\n",
    "                \"system_initialized\": True,\n",
    "                \"system_id\": self.system_id,\n",
    "                \"initialization_time\": time.time()\n",
    "            })\n",
    "        \n",
    "        def analyze_document(self, document: DocumentSource) -> bool:\n",
    "            \"\"\"Process a single document.\n",
    "            \n",
    "            Args:\n",
    "                document: The document to analyze\n",
    "                \n",
    "            Returns:\n",
    "                True if processing was successful\n",
    "            \"\"\"\n",
    "            # Add document to state if not already present\n",
    "            if not any(doc.source_id == document.source_id for doc in self.state.data.get('documents', [])):\n",
    "                self.state.add_document(document)\n",
    "            \n",
    "            # Process the document\n",
    "            return process_document_from_state(self.state, document.source_id)\n",
    "        \n",
    "        def analyze_multiple_documents(self, documents: List[DocumentSource]) -> MultiDocSynthesisOutput:\n",
    "            \"\"\"Process multiple documents at once.\n",
    "            \n",
    "            Args:\n",
    "                documents: List of documents to analyze\n",
    "                \n",
    "            Returns:\n",
    "                Complete multi-document analysis results\n",
    "            \"\"\"\n",
    "            # Use standalone function that adds documents to state\n",
    "            return analyze_multiple_documents(documents, self.state.config)\n",
    "        \n",
    "        def get_cross_document_relationships(self) -> List[CrossDocumentRelationship]:\n",
    "            \"\"\"Get relationships between documents in the current state.\n",
    "            \n",
    "            Returns:\n",
    "                List of cross-document relationships\n",
    "            \"\"\"\n",
    "            # Extract needed information from state\n",
    "            claims = self.state.data.get('claims', [])\n",
    "            methodologies = self.state.data.get('methodologies', [])\n",
    "            contributions = self.state.data.get('contributions', [])\n",
    "            directions = self.state.data.get('research_directions', [])\n",
    "            \n",
    "            # Generate relationships\n",
    "            return identify_cross_document_relationships(\n",
    "                claims, methodologies, contributions, directions, self.state.config\n",
    "            )\n",
    "        \n",
    "        def generate_syntheses(self) -> Dict[str, Any]:\n",
    "            \"\"\"Generate syntheses for all categories.\n",
    "            \n",
    "            Returns:\n",
    "                Dictionary with category syntheses and overall synthesis\n",
    "            \"\"\"\n",
    "            # Get documents and extracted elements\n",
    "            documents = self.state.data.get('documents', [])\n",
    "            claims = self.state.data.get('claims', [])\n",
    "            methodologies = self.state.data.get('methodologies', [])\n",
    "            contributions = self.state.data.get('contributions', [])\n",
    "            directions = self.state.data.get('research_directions', [])\n",
    "            \n",
    "            # Get or generate relationships\n",
    "            relationships = self.state.data.get('cross_document_relationships', [])\n",
    "            if not relationships:\n",
    "                relationships = self.get_cross_document_relationships()\n",
    "                \n",
    "            # Generate category syntheses\n",
    "            category_syntheses = {\n",
    "                \"claims\": synthesize_across_category(claims, \"claims\", len(documents)),\n",
    "                \"methodologies\": synthesize_across_category(methodologies, \"methodologies\", len(documents)),\n",
    "                \"contributions\": synthesize_across_category(contributions, \"contributions\", len(documents)),\n",
    "                \"directions\": synthesize_across_category(directions, \"directions\", len(documents))\n",
    "            }\n",
    "            \n",
    "            # Generate overall synthesis\n",
    "            overall_synthesis = generate_multi_document_synthesis(\n",
    "                documents, category_syntheses, relationships\n",
    "            )\n",
    "            \n",
    "            # Update state\n",
    "            self.state.update('category_syntheses', category_syntheses)\n",
    "            self.state.update('overall_synthesis', overall_synthesis)\n",
    "            self.state.update('cross_document_relationships', relationships)\n",
    "            \n",
    "            return {\n",
    "                \"category_syntheses\": category_syntheses,\n",
    "                \"overall_synthesis\": overall_synthesis\n",
    "            }\n",
    "        \n",
    "        def save_session(self, filepath: Optional[str] = None) -> str:\n",
    "            \"\"\"Save current session to disk.\n",
    "            \n",
    "            Args:\n",
    "                filepath: Optional custom filepath\n",
    "                \n",
    "            Returns:\n",
    "                Path to the saved file\n",
    "            \"\"\"\n",
    "            return save_session(self.state, filepath)\n",
    "        \n",
    "        def load_session(self, filepath: str) -> bool:\n",
    "            \"\"\"Load session from disk.\n",
    "            \n",
    "            Args:\n",
    "                filepath: Path to the saved state file\n",
    "                \n",
    "            Returns:\n",
    "                True if loading was successful\n",
    "            \"\"\"\n",
    "            try:\n",
    "                loaded_state = load_session(filepath)\n",
    "                self.state = loaded_state\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                show(f\"Error loading session: {str(e)}\", \"error\")\n",
    "                return False\n",
    "        \n",
    "        def get_current_results(self) -> MultiDocSynthesisOutput:\n",
    "            \"\"\"Get complete current results.\n",
    "            \n",
    "            Returns:\n",
    "                Complete results in output model format\n",
    "            \"\"\"\n",
    "            return self.state.to_output_model()\n",
    "\n",
    "    # Initialize the system (connects previously defined components)\n",
    "    # Use the global state that was already initialized in the Data Models section\n",
    "    litsynth_multi = LitSynthMultiSystem(state=state)\n",
    "    \n",
    "    # Confirm successful initialization\n",
    "    show(\"LitSynth Multi-Document System initialized successfully\", \"success\")\n",
    "    \n",
    "except Exception as e:\n",
    "    # Use show from Core Utilities for error (with level parameter)\n",
    "    error_msg = f\"System failed to initialize: {str(e)}\"\n",
    "    show(f\"{error_msg}\\nPlease make sure you run the cells in order: Installation-Initialization-Data Models-Core Functions\", \"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UI Structure\n",
    "\"\"\"\n",
    "This cell defines the Gradio UI layout for the LitSynth-Multidoc system.\n",
    "The UI is designed for simplicity and follows the same visual style as the \n",
    "original LitSynth system, with adaptations for multi-document workflows.\n",
    "\"\"\"\n",
    "\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def create_multi_doc_ui():\n",
    "    \"\"\"Create optimized Literature Synthesis UI for multiple documents.\"\"\"\n",
    "    \n",
    "    with gr.Blocks(theme=gr.themes.Default()) as app:\n",
    "        gr.Markdown(\"# 📚 Multi-Document Literature Synthesis Expert System\")\n",
    "        \n",
    "        with gr.Tabs() as tabs:\n",
    "            # === DOCUMENT INPUT TAB ===\n",
    "            with gr.Tab(\"📄 Document Upload\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=3):\n",
    "                        # Multiple PDF Upload\n",
    "                        pdf_input = gr.File(\n",
    "                            label=\"Upload Scientific PDFs (max 5 documents)\", \n",
    "                            file_types=[\".pdf\"],\n",
    "                            file_count=\"multiple\"\n",
    "                        )\n",
    "                        \n",
    "                        with gr.Row():\n",
    "                            analysis_mode = gr.Radio(\n",
    "                                choices=[\"quick\", \"balanced\", \"thorough\"],\n",
    "                                label=\"Analysis Mode\",\n",
    "                                value=\"balanced\",\n",
    "                                info=\"Quick: 30-40s per doc, Balanced: 1-2min per doc, Thorough: 3-5min per doc\"\n",
    "                            )\n",
    "                            analyze_btn = gr.Button(\"Run Analysis\", variant=\"primary\", size=\"lg\")\n",
    "                    \n",
    "                    with gr.Column(scale=2):\n",
    "                        status_box = gr.Textbox(\n",
    "                            label=\"Status\", \n",
    "                            interactive=False,\n",
    "                            value=\"Upload 1-5 PDF documents to begin analysis\"\n",
    "                        )\n",
    "                        progress_bar = gr.Slider(\n",
    "                            minimum=0, maximum=100, value=0, \n",
    "                            label=\"Processing Progress\",\n",
    "                            interactive=False\n",
    "                        )\n",
    "                \n",
    "                with gr.Row():\n",
    "                    document_list = gr.Dataframe(\n",
    "                        headers=[\"Document\", \"Status\", \"Size\", \"Pages\", \"Processing Time\"],\n",
    "                        datatype=[\"str\", \"str\", \"str\", \"number\", \"str\"],\n",
    "                        label=\"Document Queue\"\n",
    "                    )\n",
    "            \n",
    "            # === EXTRACTION RESULTS TAB ===\n",
    "            with gr.Tab(\"🔍 Extraction Results\"):\n",
    "                with gr.Row():\n",
    "                    # Document selection dropdown\n",
    "                    doc_selector = gr.Dropdown(\n",
    "                        choices=[\"All Documents\"], \n",
    "                        value=\"All Documents\",\n",
    "                        label=\"Document Filter\"\n",
    "                    )\n",
    "                    \n",
    "                    importance_filter = gr.Radio(\n",
    "                        choices=[\"all\", \"high\", \"medium\", \"low\"],\n",
    "                        label=\"Importance Filter\",\n",
    "                        value=\"all\"\n",
    "                    )\n",
    "                \n",
    "                with gr.Tabs() as results_tabs:\n",
    "                    # Claims Tab\n",
    "                    with gr.Tab(\"Scientific Claims\"):\n",
    "                        claims_table = gr.DataFrame(\n",
    "                            headers=[\"Claim\", \"Type\", \"Importance\", \"Source Documents\", \"Confidence\"],\n",
    "                            label=\"Extracted Claims\"\n",
    "                        )\n",
    "                    \n",
    "                    # Methodologies Tab\n",
    "                    with gr.Tab(\"Research Methodologies\"):\n",
    "                        methods_table = gr.DataFrame(\n",
    "                            headers=[\"Method Name\", \"Description\", \"Limitations\", \"Source Documents\", \"Confidence\"],\n",
    "                            label=\"Extracted Methodologies\"\n",
    "                        )\n",
    "                    \n",
    "                    # Contributions Tab\n",
    "                    with gr.Tab(\"Key Contributions\"):\n",
    "                        contributions_table = gr.DataFrame(\n",
    "                            headers=[\"Contribution\", \"Type\", \"Importance\", \"Source Documents\", \"Confidence\"],\n",
    "                            label=\"Extracted Contributions\"\n",
    "                        )\n",
    "                    \n",
    "                    # Research Directions Tab\n",
    "                    with gr.Tab(\"Research Directions\"):\n",
    "                        directions_table = gr.DataFrame(\n",
    "                            headers=[\"Direction\", \"Rationale\", \"Importance\", \"Source Documents\", \"Confidence\"],\n",
    "                            label=\"Extracted Research Directions\"\n",
    "                        )\n",
    "                \n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        gr.Markdown(\"### Cross-Document Relationships\")\n",
    "                        relationships_table = gr.DataFrame(\n",
    "                            headers=[\"Source Element\", \"Relationship\", \"Target Element\", \"Evidence\", \"Confidence\"],\n",
    "                            label=\"Identified Relationships\"\n",
    "                        )\n",
    "                    \n",
    "                    with gr.Column():\n",
    "                        relationships_plot = gr.Plot(label=\"Cross-Document Network\")\n",
    "                        with gr.Row():\n",
    "                            min_confidence = gr.Slider(\n",
    "                                minimum=0.0, maximum=1.0, value=0.6, step=0.1,\n",
    "                                label=\"Minimum Confidence\"\n",
    "                            )\n",
    "                            refresh_viz_btn = gr.Button(\"Refresh Visualization\")\n",
    "            \n",
    "            # === SYNTHESIS TAB ===\n",
    "            with gr.Tab(\"📝 Research Synthesis\"):\n",
    "                with gr.Row():\n",
    "                    run_synthesis_btn = gr.Button(\"Generate Synthesis\", variant=\"primary\")\n",
    "                    synthesis_status = gr.Textbox(\n",
    "                        label=\"Synthesis Status\", \n",
    "                        value=\"Click 'Generate Synthesis' to create synthesis across all documents\",\n",
    "                        interactive=False\n",
    "                    )\n",
    "                \n",
    "                with gr.Tabs() as synthesis_tabs:\n",
    "                    # Overall Synthesis\n",
    "                    with gr.Tab(\"Overall Synthesis\"):\n",
    "                        overall_synthesis = gr.Markdown()\n",
    "                    \n",
    "                    # Category Syntheses\n",
    "                    with gr.Tab(\"Claims Synthesis\"):\n",
    "                        claims_synthesis = gr.Markdown()\n",
    "                    \n",
    "                    with gr.Tab(\"Methodologies Synthesis\"):\n",
    "                        methods_synthesis = gr.Markdown()\n",
    "                    \n",
    "                    with gr.Tab(\"Contributions Synthesis\"):\n",
    "                        contributions_synthesis = gr.Markdown()\n",
    "                    \n",
    "                    with gr.Tab(\"Research Directions Synthesis\"):\n",
    "                        directions_synthesis = gr.Markdown()\n",
    "                \n",
    "                with gr.Row():\n",
    "                    export_format = gr.Dropdown(\n",
    "                        choices=[\"Markdown\", \"Text\", \"JSON\"],\n",
    "                        label=\"Export Format\",\n",
    "                        value=\"Markdown\"\n",
    "                    )\n",
    "                    export_btn = gr.Button(\"Export Synthesis\")\n",
    "                    save_session_btn = gr.Button(\"Save Session\")\n",
    "            \n",
    "            # === SETTINGS TAB ===\n",
    "            with gr.Tab(\"⚙️ Settings\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        # Multi-document settings\n",
    "                        gr.Markdown(\"#### Multi-Document Settings\")\n",
    "                        parallel_processing = gr.Checkbox(\n",
    "                            label=\"Enable Parallel Processing\",\n",
    "                            value=False,\n",
    "                            info=\"Process documents in parallel (requires more memory)\"\n",
    "                        )\n",
    "                        \n",
    "                        # Text processing settings\n",
    "                        gr.Markdown(\"#### Text Processing\")\n",
    "                        chunk_size = gr.Slider(\n",
    "                            minimum=1000, maximum=8000, value=4000, step=500,\n",
    "                            label=\"Chunk Size (chars)\",\n",
    "                            info=\"Larger chunks capture more context but process slower\"\n",
    "                        )\n",
    "                        chunk_overlap = gr.Slider(\n",
    "                            minimum=50, maximum=1000, value=150, step=50,\n",
    "                            label=\"Chunk Overlap\"\n",
    "                        )\n",
    "                        \n",
    "                    with gr.Column():\n",
    "                        # Element Extraction Settings\n",
    "                        gr.Markdown(\"#### Element Extraction Settings\")\n",
    "                        min_importance = gr.Dropdown(\n",
    "                            choices=[\"low\", \"medium\", \"high\"],\n",
    "                            label=\"Min Importance\",\n",
    "                            value=\"medium\"\n",
    "                        )\n",
    "                        \n",
    "                        # Category limits\n",
    "                        gr.Markdown(\"#### Maximum Elements per Document\")\n",
    "                        max_claims = gr.Slider(\n",
    "                            minimum=5, maximum=100, value=20, step=5,\n",
    "                            label=\"Max Claims\"\n",
    "                        )\n",
    "                        max_methods = gr.Slider(\n",
    "                            minimum=3, maximum=50, value=10, step=5,\n",
    "                            label=\"Max Methodologies\"\n",
    "                        )\n",
    "                        max_contributions = gr.Slider(\n",
    "                            minimum=3, maximum=50, value=15, step=5,\n",
    "                            label=\"Max Contributions\"\n",
    "                        )\n",
    "                        max_directions = gr.Slider(\n",
    "                            minimum=3, maximum=50, value=10, step=5,\n",
    "                            label=\"Max Research Directions\"\n",
    "                        )\n",
    "                        \n",
    "                    with gr.Column():\n",
    "                        # Cross-Document Settings\n",
    "                        gr.Markdown(\"#### Cross-Document Analysis\")\n",
    "                        relationship_confidence = gr.Slider(\n",
    "                            minimum=0.0, maximum=1.0, value=0.6, step=0.1,\n",
    "                            label=\"Min Relationship Confidence\"\n",
    "                        )\n",
    "                        max_cross_relationships = gr.Slider(\n",
    "                            minimum=10, maximum=200, value=50, step=10,\n",
    "                            label=\"Max Cross-Document Relationships\"\n",
    "                        )\n",
    "                        similarity_threshold = gr.Slider(\n",
    "                            minimum=0.5, maximum=0.95, value=0.75, step=0.05,\n",
    "                            label=\"Element Similarity Threshold\",\n",
    "                            info=\"Higher values require more similarity for deduplication\"\n",
    "                        )\n",
    "                        \n",
    "                        scientific_domain = gr.Textbox(\n",
    "                            label=\"Scientific Domain (Optional)\",\n",
    "                            placeholder=\"e.g., Molecular Biology, Quantum Physics\",\n",
    "                            info=\"Helps tailor analysis to specific domains\"\n",
    "                        )\n",
    "                        \n",
    "                        apply_settings_btn = gr.Button(\"Apply Settings\", variant=\"primary\")\n",
    "                        settings_status = gr.Textbox(label=\"Settings Status\", interactive=False)\n",
    "                        \n",
    "                        # Session Management\n",
    "                        gr.Markdown(\"#### Session Management\")\n",
    "                        with gr.Row():\n",
    "                            load_session_btn = gr.Button(\"Load Session\")\n",
    "                            session_file = gr.File(\n",
    "                                label=\"Session File\",\n",
    "                                file_types=[\".json\"],\n",
    "                                file_count=\"single\"\n",
    "                            )\n",
    "        \n",
    "        # === State Variables ===\n",
    "        system_state = gr.State(None)\n",
    "        results_state = gr.State(None)\n",
    "    \n",
    "    return app\n",
    "\n",
    "# Create the UI\n",
    "multidoc_ui = create_multi_doc_ui()\n",
    "\n",
    "# Display success message in notebook\n",
    "show(\"UI structure for multi-document system defined successfully\", \"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UI Launch\n",
    "\"\"\"Streamlined LitSynth-Multidoc UI launch with RAG-optimized document processing.\"\"\"\n",
    "\n",
    "import tempfile, os, time, json, re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def launch_litsynth_multi_ui():\n",
    "    \"\"\"Launch the Multi-Document Literature Synthesis UI using RAG-based core functions.\"\"\"\n",
    "    \n",
    "    # === Helper Functions (UI-specific only) ===\n",
    "    \n",
    "    def format_document_list(results):\n",
    "        \"\"\"Format document list for display in UI table.\"\"\"\n",
    "        if not results: return pd.DataFrame()\n",
    "        \n",
    "        docs = []\n",
    "        for doc_id, result in results.items():\n",
    "            if not isinstance(result, dict) or doc_id in [\"summary\", \"cross_document_relationships\"]:\n",
    "                continue\n",
    "                \n",
    "            if \"error\" in result:\n",
    "                docs.append({\n",
    "                    \"Document\": result.get(\"title\", doc_id),\n",
    "                    \"Status\": \"Error\",\n",
    "                    \"Size\": \"N/A\",\n",
    "                    \"Pages\": \"N/A\",\n",
    "                    \"Processing Time\": \"N/A\"\n",
    "                })\n",
    "            else:\n",
    "                metadata = result.get(\"metadata\", {})\n",
    "                docs.append({\n",
    "                    \"Document\": result.get(\"title\", doc_id),\n",
    "                    \"Status\": \"Complete\",\n",
    "                    \"Size\": f\"{metadata.get('original_length', 0):,} chars\",\n",
    "                    \"Pages\": metadata.get(\"pages\", \"N/A\"),\n",
    "                    \"Processing Time\": f\"{metadata.get('processing_time', 0):.1f}s\"\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(docs)\n",
    "    \n",
    "    def get_doc_selector_options(results):\n",
    "        \"\"\"Get document selector options for dropdown.\"\"\"\n",
    "        options = [\"All Documents\"]\n",
    "        if results:\n",
    "            for doc_id, result in results.items():\n",
    "                if isinstance(result, dict) and doc_id not in [\"summary\", \"cross_document_relationships\"]:\n",
    "                    options.append(f\"{result.get('title', doc_id)} ({doc_id})\")\n",
    "        return options, \"All Documents\"\n",
    "    \n",
    "    def filter_and_format_elements(results, doc_filter, category, importance_filter=\"all\"):\n",
    "        \"\"\"Filter elements by document and format for display.\"\"\"\n",
    "        if not results: return pd.DataFrame()\n",
    "        \n",
    "        # Get document ID from filter\n",
    "        selected_doc = None\n",
    "        if doc_filter != \"All Documents\":\n",
    "            doc_id_match = re.search(r'\\((.*?)\\)$', doc_filter)\n",
    "            if doc_id_match:\n",
    "                selected_doc = doc_id_match.group(1)\n",
    "        \n",
    "        # Collect elements\n",
    "        elements = []\n",
    "        if selected_doc:\n",
    "            result = results.get(selected_doc, {})\n",
    "            if isinstance(result, dict): elements.extend(result.get(category, []))\n",
    "        else:\n",
    "            for doc_id, result in results.items():\n",
    "                if isinstance(result, dict) and doc_id not in [\"summary\", \"cross_document_relationships\"]:\n",
    "                    elements.extend(result.get(category, []))\n",
    "        \n",
    "        # Filter by importance\n",
    "        if importance_filter != \"all\" and elements and hasattr(elements[0], \"importance\"):\n",
    "            elements = [e for e in elements if e.importance == importance_filter]\n",
    "        \n",
    "        # Format based on category\n",
    "        data = []\n",
    "        for element in elements:\n",
    "            # Get source documents\n",
    "            source_docs = []\n",
    "            for doc_id in element.source_documents:\n",
    "                doc_result = results.get(doc_id, {})\n",
    "                doc_title = doc_result.get(\"title\", doc_id) if isinstance(doc_result, dict) else doc_id\n",
    "                source_docs.append(doc_title)\n",
    "                \n",
    "            # Format based on element type\n",
    "            if category == \"claims\":\n",
    "                data.append({\n",
    "                    \"Claim\": element.claim_text,\n",
    "                    \"Type\": element.claim_type.capitalize(),\n",
    "                    \"Importance\": element.importance.capitalize(),\n",
    "                    \"Source Documents\": \", \".join(source_docs),\n",
    "                    \"Confidence\": f\"{element.confidence:.2f}\"\n",
    "                })\n",
    "            elif category == \"methodologies\":\n",
    "                data.append({\n",
    "                    \"Method Name\": element.method_name,\n",
    "                    \"Description\": element.description,\n",
    "                    \"Limitations\": element.limitations or \"Not specified\",\n",
    "                    \"Source Documents\": \", \".join(source_docs),\n",
    "                    \"Confidence\": f\"{element.confidence:.2f}\"\n",
    "                })\n",
    "            elif category == \"contributions\":\n",
    "                data.append({\n",
    "                    \"Contribution\": element.contribution_text,\n",
    "                    \"Type\": element.contribution_type.capitalize(),\n",
    "                    \"Importance\": element.importance.capitalize(),\n",
    "                    \"Source Documents\": \", \".join(source_docs),\n",
    "                    \"Confidence\": f\"{element.confidence:.2f}\"\n",
    "                })\n",
    "            elif category == \"research_directions\":\n",
    "                data.append({\n",
    "                    \"Direction\": element.direction_text,\n",
    "                    \"Rationale\": element.rationale or \"Not specified\",\n",
    "                    \"Importance\": element.importance.capitalize(),\n",
    "                    \"Source Documents\": \", \".join(source_docs),\n",
    "                    \"Confidence\": f\"{element.confidence:.2f}\"\n",
    "                })\n",
    "                \n",
    "        df = pd.DataFrame(data)\n",
    "        if not df.empty and hasattr(elements[0], \"importance\"):\n",
    "            # Sort by importance\n",
    "            importance_map = {\"High\": 0, \"Medium\": 1, \"Low\": 2}\n",
    "            df[\"_imp\"] = df[\"Importance\"].map(importance_map)\n",
    "            df = df.sort_values(by=[\"_imp\", \"Confidence\"], ascending=[True, False])\n",
    "            df = df.drop(columns=[\"_imp\"])\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    def format_relationships_table(results):\n",
    "        \"\"\"Format relationships for display in UI table.\"\"\"\n",
    "        if not results: return pd.DataFrame()\n",
    "        relationships = results.get(\"cross_document_relationships\", [])\n",
    "        if not relationships: return pd.DataFrame()\n",
    "        \n",
    "        # Map elements to text and type\n",
    "        element_info = {}\n",
    "        for doc_id, result in results.items():\n",
    "            if not isinstance(result, dict) or doc_id in [\"summary\", \"cross_document_relationships\"]:\n",
    "                continue\n",
    "                \n",
    "            for category, id_field, text_field, type_label in [\n",
    "                (\"claims\", \"claim_id\", \"claim_text\", \"Claim\"),\n",
    "                (\"methodologies\", \"method_id\", \"method_name\", \"Method\"),\n",
    "                (\"contributions\", \"contribution_id\", \"contribution_text\", \"Contribution\"),\n",
    "                (\"research_directions\", \"direction_id\", \"direction_text\", \"Direction\")\n",
    "            ]:\n",
    "                for item in result.get(category, []):\n",
    "                    if hasattr(item, id_field) and hasattr(item, text_field):\n",
    "                        item_id = getattr(item, id_field)\n",
    "                        text = getattr(item, text_field)\n",
    "                        element_info[item_id] = {\n",
    "                            \"text\": text[:40] + \"...\" if len(text) > 40 else text,\n",
    "                            \"type\": type_label\n",
    "                        }\n",
    "        \n",
    "        # Format data\n",
    "        data = []\n",
    "        for rel in relationships:\n",
    "            source_info = element_info.get(rel.source_element_id, {\"text\": rel.source_element_id, \"type\": rel.source_element_type.capitalize()})\n",
    "            target_info = element_info.get(rel.target_element_id, {\"text\": rel.target_element_id, \"type\": rel.target_element_type.capitalize()})\n",
    "            \n",
    "            data.append({\n",
    "                \"Source Element\": f\"{source_info['type']}: {source_info['text']}\",\n",
    "                \"Relationship\": rel.relationship_type.capitalize(),\n",
    "                \"Target Element\": f\"{target_info['type']}: {target_info['text']}\",\n",
    "                \"Evidence\": rel.evidence or \"N/A\",\n",
    "                \"Confidence\": f\"{rel.confidence:.2f}\"\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(data).sort_values(by=\"Confidence\", ascending=False)\n",
    "    \n",
    "    # === Main Processing Function (using RAG-based core functions) ===\n",
    "    \n",
    "    def process_documents(files, analysis_mode):\n",
    "        \"\"\"Process documents using RAG-based core analyze_multiple_documents function.\"\"\"\n",
    "        if not files:\n",
    "            yield \"Please upload at least one document.\", None, 0\n",
    "            return\n",
    "        \n",
    "        start_time = time.time()\n",
    "        num_docs = len(files)\n",
    "        yield f\"Processing {num_docs} documents...\", None, 5\n",
    "        \n",
    "        try:\n",
    "            # Create DocumentSource objects from files\n",
    "            documents = []\n",
    "            for i, file in enumerate(files):\n",
    "                file_path = getattr(file, 'name', None)\n",
    "                if not file_path and hasattr(file, 'path'):\n",
    "                    file_path = file.path\n",
    "                \n",
    "                doc = DocumentSource(\n",
    "                    source_id=f\"doc_{i}\",\n",
    "                    title=f\"Document {i+1}\",\n",
    "                    source_type=\"pdf\",\n",
    "                    content=file_path,\n",
    "                    processed=False\n",
    "                )\n",
    "                documents.append(doc)\n",
    "            \n",
    "            # Set config based on analysis mode\n",
    "            config_dict = config.model_dump() if hasattr(config, 'model_dump') else config\n",
    "            if analysis_mode == \"quick\":\n",
    "                config_dict.update({\n",
    "                    \"text_chunk_size\": min(config_dict.get(\"text_chunk_size\", 4000), 2500),\n",
    "                    \"min_importance\": \"high\",\n",
    "                    \"max_claims\": min(config_dict.get(\"max_claims\", 20), 10),\n",
    "                    \"max_methodologies\": min(config_dict.get(\"max_methodologies\", 10), 5),\n",
    "                    \"max_contributions\": min(config_dict.get(\"max_contributions\", 15), 8),\n",
    "                    \"max_directions\": min(config_dict.get(\"max_directions\", 10), 5)\n",
    "                })\n",
    "            elif analysis_mode == \"thorough\":\n",
    "                config_dict.update({\n",
    "                    \"text_chunk_size\": max(config_dict.get(\"text_chunk_size\", 4000), 5000),\n",
    "                    \"min_importance\": \"low\"\n",
    "                })\n",
    "            \n",
    "            # Use the RAG-enabled analyze_multiple_documents function\n",
    "            yield f\"Analyzing documents using RAG-based multi-document analysis...\", None, 20\n",
    "            results = analyze_multiple_documents(documents, config_dict)\n",
    "            \n",
    "            # Transform results to match UI expectations\n",
    "            ui_results = {}\n",
    "            for i, doc in enumerate(documents):\n",
    "                doc_id = doc.source_id\n",
    "                doc_elements = {\n",
    "                    \"document_id\": doc_id,\n",
    "                    \"title\": doc.title,\n",
    "                    \"claims\": results.get_elements_for_document(doc_id, \"claims\"),\n",
    "                    \"methodologies\": results.get_elements_for_document(doc_id, \"methodologies\"),\n",
    "                    \"contributions\": results.get_elements_for_document(doc_id, \"contributions\"),\n",
    "                    \"research_directions\": results.get_elements_for_document(doc_id, \"directions\"),\n",
    "                    \"metadata\": {\n",
    "                        \"original_length\": len(load_document(doc.source_type, doc.content).get(\"text\", \"\")),\n",
    "                        \"processing_time\": (time.time() - start_time) / num_docs,\n",
    "                        \"pages\": doc.metadata.get(\"pages\", \"N/A\")\n",
    "                    }\n",
    "                }\n",
    "                ui_results[doc_id] = doc_elements\n",
    "            \n",
    "            # Add cross-document relationships\n",
    "            ui_results[\"cross_document_relationships\"] = results.cross_document_relationships\n",
    "            \n",
    "            # Add summary stats\n",
    "            ui_results[\"summary\"] = {\n",
    "                \"total_documents\": num_docs,\n",
    "                \"total_processing_time\": round(time.time() - start_time, 2),\n",
    "                \"total_elements_extracted\": (\n",
    "                    len(results.claims) + \n",
    "                    len(results.methodologies) + \n",
    "                    len(results.contributions) + \n",
    "                    len(results.research_directions)\n",
    "                ),\n",
    "                \"analysis_mode\": analysis_mode,\n",
    "                \"cross_document_relationships\": len(results.cross_document_relationships),\n",
    "                \"processing_approach\": \"RAG-based\"  # Indicate that RAG was used\n",
    "            }\n",
    "            \n",
    "            yield f\"Analysis complete: {num_docs} docs in {time.time() - start_time:.2f}s (using RAG)\", ui_results, 100\n",
    "            \n",
    "        except Exception as e:\n",
    "            yield f\"Error in document processing: {str(e)}\", None, 0\n",
    "    \n",
    "    # === Synthesis Function (using core functions) ===\n",
    "    \n",
    "    def generate_synthesis(results):\n",
    "        \"\"\"Generate synthesis using core functions.\"\"\"\n",
    "        if not results: \n",
    "            return {\"error\": \"No results available\"}\n",
    "        \n",
    "        try:\n",
    "            # Collect documents from results\n",
    "            documents = []\n",
    "            elements = {\"claims\": [], \"methodologies\": [], \"contributions\": [], \"research_directions\": []}\n",
    "            \n",
    "            for doc_id, result in results.items():\n",
    "                if not isinstance(result, dict) or doc_id in [\"summary\", \"cross_document_relationships\"]:\n",
    "                    continue\n",
    "                    \n",
    "                doc = DocumentSource(\n",
    "                    source_id=doc_id,\n",
    "                    title=result.get(\"title\", doc_id),\n",
    "                    source_type=\"text\",\n",
    "                    content=\"\",  # Empty content as we're only using for synthesis\n",
    "                    processed=True\n",
    "                )\n",
    "                documents.append(doc)\n",
    "                \n",
    "                # Collect elements\n",
    "                for category in elements:\n",
    "                    elements[category].extend(result.get(category, []))\n",
    "            \n",
    "            # Get relationships\n",
    "            relationships = results.get(\"cross_document_relationships\", [])\n",
    "            \n",
    "            # Generate category syntheses using core functions\n",
    "            category_syntheses = {\n",
    "                \"claims\": synthesize_across_category(elements[\"claims\"], \"claims\", len(documents)),\n",
    "                \"methodologies\": synthesize_across_category(elements[\"methodologies\"], \"methodologies\", len(documents)),\n",
    "                \"contributions\": synthesize_across_category(elements[\"contributions\"], \"contributions\", len(documents)),\n",
    "                \"directions\": synthesize_across_category(elements[\"research_directions\"], \"directions\", len(documents))\n",
    "            }\n",
    "            \n",
    "            # Generate overall synthesis\n",
    "            overall = generate_multi_document_synthesis(documents, category_syntheses, relationships)\n",
    "            \n",
    "            return {\n",
    "                \"overall\": overall,\n",
    "                \"claims\": category_syntheses[\"claims\"].synthesis_text,\n",
    "                \"methodologies\": category_syntheses[\"methodologies\"].synthesis_text,\n",
    "                \"contributions\": category_syntheses[\"contributions\"].synthesis_text,\n",
    "                \"directions\": category_syntheses[\"directions\"].synthesis_text\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"error\": str(e),\n",
    "                \"overall\": f\"Error: {str(e)}\",\n",
    "                \"claims\": \"Synthesis failed\",\n",
    "                \"methodologies\": \"Synthesis failed\",\n",
    "                \"contributions\": \"Synthesis failed\",\n",
    "                \"directions\": \"Synthesis failed\"\n",
    "            }\n",
    "    \n",
    "    # === Settings Update Function ===\n",
    "    \n",
    "    def update_settings(use_rag, parallel, chunk_size, chunk_overlap, min_importance, \n",
    "                       max_claims, max_methods, max_contribs, max_dirs, \n",
    "                       rel_conf, max_rels, sim_threshold, domain):\n",
    "        \"\"\"Update system configuration with RAG toggle.\"\"\"\n",
    "        try:\n",
    "            new_config = LitSynthMultiConfig(\n",
    "                # Add RAG toggle (will be ignored by older versions)\n",
    "                use_rag=use_rag,\n",
    "                text_chunk_size=chunk_size,\n",
    "                text_chunk_overlap=chunk_overlap,\n",
    "                min_importance=min_importance,\n",
    "                extraction_confidence=0.7,\n",
    "                max_claims=max_claims,\n",
    "                max_methodologies=max_methods,\n",
    "                max_contributions=max_contribs,\n",
    "                max_directions=max_dirs,\n",
    "                relationship_confidence=rel_conf,\n",
    "                max_cross_relationships=max_rels,\n",
    "                similarity_threshold=sim_threshold,\n",
    "                parallel_processing=parallel,\n",
    "                scientific_domain=domain if domain else None\n",
    "            )\n",
    "            \n",
    "            # Update global config\n",
    "            global config\n",
    "            config = new_config\n",
    "            \n",
    "            rag_status = \"enabled\" if use_rag else \"disabled\"\n",
    "            return f\"Settings updated: RAG {rag_status}, {chunk_size} chunk size, {min_importance} min importance\"\n",
    "        except Exception as e:\n",
    "            return f\"Error updating configuration: {str(e)}\"\n",
    "    \n",
    "    # === Session Management Functions ===\n",
    "    \n",
    "    def save_current_session(results, synthesis):\n",
    "        \"\"\"Save current session state to file.\"\"\"\n",
    "        if not results:\n",
    "            return \"No session data to save.\"\n",
    "            \n",
    "        try:\n",
    "            session_data = {\n",
    "                \"results\": results,\n",
    "                \"synthesis\": synthesis,\n",
    "                \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"processing_approach\": \"RAG-based\"  # Add RAG indicator\n",
    "            }\n",
    "            \n",
    "            # Save to temporary file\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix='.json', mode='w') as f:\n",
    "                json.dump(session_data, f, default=lambda o: o.__dict__ if hasattr(o, '__dict__') else str(o))\n",
    "                filepath = f.name\n",
    "            \n",
    "            return filepath\n",
    "        except Exception as e:\n",
    "            return f\"Error saving session: {str(e)}\"\n",
    "            \n",
    "    def load_saved_session(file):\n",
    "        \"\"\"Load session from file.\"\"\"\n",
    "        if not file:\n",
    "            return None, \"Please select a session file.\"\n",
    "            \n",
    "        try:\n",
    "            with open(file.name, 'r') as f:\n",
    "                session_data = json.load(f)\n",
    "                \n",
    "            return session_data.get(\"results\"), \"Session loaded successfully.\"\n",
    "        except Exception as e:\n",
    "            return None, f\"Error loading session: {str(e)}\"\n",
    "    \n",
    "    # === Filter Update Function ===\n",
    "    \n",
    "    def filter_update_handler(results, doc_filter, importance):\n",
    "        \"\"\"Update all tables based on filter changes.\"\"\"\n",
    "        return (\n",
    "            filter_and_format_elements(results, doc_filter, \"claims\", importance),\n",
    "            filter_and_format_elements(results, doc_filter, \"methodologies\", importance),\n",
    "            filter_and_format_elements(results, doc_filter, \"contributions\", importance),\n",
    "            filter_and_format_elements(results, doc_filter, \"research_directions\", importance)\n",
    "        )\n",
    "    \n",
    "    # === Create UI and Connect Event Handlers ===\n",
    "    \n",
    "    # Setup Gradio UI components\n",
    "    import gradio as gr\n",
    "    \n",
    "    with gr.Blocks() as app:\n",
    "        gr.Markdown(\"# 📚 Multi-Document Literature Synthesis Expert System\")\n",
    "        \n",
    "        # State containers\n",
    "        results_state = gr.State(None)\n",
    "        synthesis_state = gr.State(None)\n",
    "        \n",
    "        with gr.Tabs() as tabs:\n",
    "            # Document Upload Tab\n",
    "            with gr.Tab(\"📄 Document Upload\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=3):\n",
    "                        pdf_input = gr.File(label=\"Upload Scientific PDFs (max 5)\", file_types=[\".pdf\"], file_count=\"multiple\")\n",
    "                        with gr.Row():\n",
    "                            analysis_mode = gr.Radio(choices=[\"quick\", \"balanced\", \"thorough\"], label=\"Analysis Mode\", value=\"balanced\")\n",
    "                            analyze_btn = gr.Button(\"Run Analysis\", variant=\"primary\")\n",
    "                    with gr.Column(scale=2):\n",
    "                        status_box = gr.Textbox(label=\"Status\", interactive=False, value=\"Upload 1-5 PDF documents to begin analysis\", lines=5)\n",
    "                        progress_bar = gr.Slider(minimum=0, maximum=100, value=0, label=\"Processing Progress\", interactive=False)\n",
    "                document_list = gr.DataFrame(headers=[\"Document\", \"Status\", \"Size\", \"Pages\", \"Processing Time\"])\n",
    "            \n",
    "            # Extraction Results Tab\n",
    "            with gr.Tab(\"🔍 Extraction Results\"):\n",
    "                with gr.Row():\n",
    "                    doc_selector = gr.Dropdown(choices=[\"All Documents\"], value=\"All Documents\", label=\"Document Filter\")\n",
    "                    importance_filter = gr.Radio(choices=[\"all\", \"high\", \"medium\", \"low\"], label=\"Importance Filter\", value=\"all\")\n",
    "                \n",
    "                with gr.Tabs() as results_tabs:\n",
    "                    with gr.Tab(\"Scientific Claims\"):\n",
    "                        claims_table = gr.DataFrame(headers=[\"Claim\", \"Type\", \"Importance\", \"Source Documents\", \"Confidence\"])\n",
    "                    with gr.Tab(\"Research Methodologies\"):\n",
    "                        methods_table = gr.DataFrame(headers=[\"Method Name\", \"Description\", \"Limitations\", \"Source Documents\", \"Confidence\"])\n",
    "                    with gr.Tab(\"Key Contributions\"):\n",
    "                        contributions_table = gr.DataFrame(headers=[\"Contribution\", \"Type\", \"Importance\", \"Source Documents\", \"Confidence\"])\n",
    "                    with gr.Tab(\"Research Directions\"):\n",
    "                        directions_table = gr.DataFrame(headers=[\"Direction\", \"Rationale\", \"Importance\", \"Source Documents\", \"Confidence\"])\n",
    "                \n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        gr.Markdown(\"### Cross-Document Relationships\")\n",
    "                        relationships_table = gr.DataFrame(headers=[\"Source Element\", \"Relationship\", \"Target Element\", \"Evidence\", \"Confidence\"])\n",
    "                    \n",
    "                    with gr.Column():\n",
    "                        relationships_plot = gr.Plot(label=\"Cross-Document Network\")\n",
    "                        with gr.Row():\n",
    "                            min_confidence = gr.Slider(\n",
    "                                minimum=0.0, maximum=1.0, value=0.6, step=0.1,\n",
    "                                label=\"Minimum Confidence\"\n",
    "                            )\n",
    "                            refresh_viz_btn = gr.Button(\"Refresh Visualization\")\n",
    "            \n",
    "            # Synthesis Tab\n",
    "            with gr.Tab(\"📝 Research Synthesis\"):\n",
    "                with gr.Row():\n",
    "                    run_synthesis_btn = gr.Button(\"Generate Synthesis\", variant=\"primary\")\n",
    "                    synthesis_status = gr.Textbox(label=\"Synthesis Status\", value=\"Click 'Generate Synthesis' to create synthesis across all documents\", interactive=False)\n",
    "                \n",
    "                with gr.Tabs() as synthesis_tabs:\n",
    "                    with gr.Tab(\"Overall Synthesis\"):\n",
    "                        overall_synthesis = gr.Markdown()\n",
    "                    with gr.Tab(\"Claims Synthesis\"):\n",
    "                        claims_synthesis = gr.Markdown()\n",
    "                    with gr.Tab(\"Methodologies Synthesis\"):\n",
    "                        methods_synthesis = gr.Markdown()\n",
    "                    with gr.Tab(\"Contributions Synthesis\"):\n",
    "                        contributions_synthesis = gr.Markdown()\n",
    "                    with gr.Tab(\"Research Directions Synthesis\"):\n",
    "                        directions_synthesis = gr.Markdown()\n",
    "                \n",
    "                with gr.Row():\n",
    "                    export_format = gr.Dropdown(\n",
    "                        choices=[\"Markdown\", \"Text\", \"JSON\"],\n",
    "                        label=\"Export Format\",\n",
    "                        value=\"Markdown\"\n",
    "                    )\n",
    "                    export_btn = gr.Button(\"Export Synthesis\")\n",
    "                    save_session_btn = gr.Button(\"Save Session\")\n",
    "            \n",
    "            # Settings Tab\n",
    "            with gr.Tab(\"⚙️ Settings\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        # Add RAG toggle at the top\n",
    "                        use_rag_approach = gr.Checkbox(\n",
    "                            label=\"Use RAG Approach (Recommended)\",\n",
    "                            value=True,\n",
    "                            info=\"Uses embeddings to retrieve relevant sections instead of chunking\"\n",
    "                        )\n",
    "                        parallel_processing = gr.Checkbox(\n",
    "                            label=\"Enable Parallel Processing\",\n",
    "                            value=False,\n",
    "                            info=\"Process documents in parallel (requires more memory)\"\n",
    "                        )\n",
    "                        \n",
    "                        # Text processing settings\n",
    "                        gr.Markdown(\"#### Text Processing\")\n",
    "                        chunk_size = gr.Slider(\n",
    "                            minimum=1000, maximum=8000, value=2000, step=500,\n",
    "                            label=\"Chunk Size (chars)\",\n",
    "                            info=\"Smaller chunks (2000-3000) recommended with RAG for better performance\"\n",
    "                        )\n",
    "                        chunk_overlap = gr.Slider(\n",
    "                            minimum=50, maximum=1000, value=50, step=50,\n",
    "                            label=\"Chunk Overlap\",\n",
    "                            info=\"Minimal overlap recommended with RAG approach\"\n",
    "                        )\n",
    "                        \n",
    "                    with gr.Column():\n",
    "                        # Element Extraction Settings\n",
    "                        gr.Markdown(\"#### Element Extraction Settings\")\n",
    "                        min_importance = gr.Dropdown(\n",
    "                            choices=[\"low\", \"medium\", \"high\"],\n",
    "                            label=\"Min Importance\",\n",
    "                            value=\"medium\"\n",
    "                        )\n",
    "                        \n",
    "                        # Category limits\n",
    "                        gr.Markdown(\"#### Maximum Elements per Document\")\n",
    "                        max_claims = gr.Slider(\n",
    "                            minimum=5, maximum=100, value=20, step=5,\n",
    "                            label=\"Max Claims\"\n",
    "                        )\n",
    "                        max_methods = gr.Slider(\n",
    "                            minimum=3, maximum=50, value=10, step=5,\n",
    "                            label=\"Max Methodologies\"\n",
    "                        )\n",
    "                        max_contributions = gr.Slider(\n",
    "                            minimum=3, maximum=50, value=15, step=5,\n",
    "                            label=\"Max Contributions\"\n",
    "                        )\n",
    "                        max_directions = gr.Slider(\n",
    "                            minimum=3, maximum=50, value=10, step=5,\n",
    "                            label=\"Max Research Directions\"\n",
    "                        )\n",
    "                        \n",
    "                    with gr.Column():\n",
    "                        # Cross-Document Settings\n",
    "                        gr.Markdown(\"#### Cross-Document Analysis\")\n",
    "                        relationship_confidence = gr.Slider(\n",
    "                            minimum=0.0, maximum=1.0, value=0.6, step=0.1,\n",
    "                            label=\"Min Relationship Confidence\"\n",
    "                        )\n",
    "                        max_cross_relationships = gr.Slider(\n",
    "                            minimum=10, maximum=200, value=50, step=10,\n",
    "                            label=\"Max Cross-Document Relationships\"\n",
    "                        )\n",
    "                        similarity_threshold = gr.Slider(\n",
    "                            minimum=0.5, maximum=0.95, value=0.75, step=0.05,\n",
    "                            label=\"Element Similarity Threshold\",\n",
    "                            info=\"Higher values require more similarity for deduplication\"\n",
    "                        )\n",
    "                        \n",
    "                        scientific_domain = gr.Textbox(\n",
    "                            label=\"Scientific Domain (Optional)\",\n",
    "                            placeholder=\"e.g., Molecular Biology, Quantum Physics\",\n",
    "                            info=\"Helps tailor analysis to specific domains\"\n",
    "                        )\n",
    "                        \n",
    "                        apply_settings_btn = gr.Button(\"Apply Settings\", variant=\"primary\")\n",
    "                        settings_status = gr.Textbox(label=\"Settings Status\", interactive=False)\n",
    "                        \n",
    "                        # Session Management\n",
    "                        gr.Markdown(\"#### Session Management\")\n",
    "                        with gr.Row():\n",
    "                            load_session_btn = gr.Button(\"Load Session\")\n",
    "                            session_file = gr.File(\n",
    "                                label=\"Session File\",\n",
    "                                file_types=[\".json\"],\n",
    "                                file_count=\"single\"\n",
    "                            )\n",
    "        \n",
    "        # Event Handlers\n",
    "        analyze_btn.click(\n",
    "            fn=process_documents,  # Now using RAG-based function\n",
    "            inputs=[pdf_input, analysis_mode],\n",
    "            outputs=[status_box, results_state, progress_bar]\n",
    "        ).then(\n",
    "            fn=format_document_list,\n",
    "            inputs=[results_state],\n",
    "            outputs=[document_list]\n",
    "        ).then(\n",
    "            fn=get_doc_selector_options,\n",
    "            inputs=[results_state],\n",
    "            outputs=[doc_selector, doc_selector]\n",
    "        ).then(\n",
    "            fn=lambda r, d, i: filter_and_format_elements(r, d, \"claims\", i),\n",
    "            inputs=[results_state, doc_selector, importance_filter],\n",
    "            outputs=[claims_table]\n",
    "        ).then(\n",
    "            fn=lambda r, d, i: filter_and_format_elements(r, d, \"methodologies\", i),\n",
    "            inputs=[results_state, doc_selector, importance_filter],\n",
    "            outputs=[methods_table]\n",
    "        ).then(\n",
    "            fn=lambda r, d, i: filter_and_format_elements(r, d, \"contributions\", i),\n",
    "            inputs=[results_state, doc_selector, importance_filter],\n",
    "            outputs=[contributions_table]\n",
    "        ).then(\n",
    "            fn=lambda r, d, i: filter_and_format_elements(r, d, \"research_directions\", i),\n",
    "            inputs=[results_state, doc_selector, importance_filter],\n",
    "            outputs=[directions_table]\n",
    "        ).then(\n",
    "            fn=format_relationships_table,\n",
    "            inputs=[results_state],\n",
    "            outputs=[relationships_table]\n",
    "        )\n",
    "        \n",
    "        # Filter update event\n",
    "        for filter_input in [doc_selector, importance_filter]:\n",
    "            filter_input.change(\n",
    "                fn=filter_update_handler,\n",
    "                inputs=[results_state, doc_selector, importance_filter],\n",
    "                outputs=[claims_table, methods_table, contributions_table, directions_table]\n",
    "            )\n",
    "        \n",
    "        # Synthesis events\n",
    "        run_synthesis_btn.click(\n",
    "            fn=lambda r: \"Generating synthesis across all documents...\" if r else \"Please analyze documents first\",\n",
    "            inputs=[results_state],\n",
    "            outputs=[synthesis_status]\n",
    "        ).then(\n",
    "            fn=generate_synthesis,  # Synthesis functions remain unchanged\n",
    "            inputs=[results_state],\n",
    "            outputs=[synthesis_state]\n",
    "        ).then(\n",
    "            fn=lambda s: s.get(\"overall\", \"No overall synthesis available.\") if s else \"No synthesis available.\",\n",
    "            inputs=[synthesis_state],\n",
    "            outputs=[overall_synthesis]\n",
    "        ).then(\n",
    "            fn=lambda s: s.get(\"claims\", \"No claims synthesis available.\") if s else \"No synthesis available.\",\n",
    "            inputs=[synthesis_state],\n",
    "            outputs=[claims_synthesis]\n",
    "        ).then(\n",
    "            fn=lambda s: s.get(\"methodologies\", \"No methodologies synthesis available.\") if s else \"No synthesis available.\",\n",
    "            inputs=[synthesis_state],\n",
    "            outputs=[methods_synthesis]\n",
    "        ).then(\n",
    "            fn=lambda s: s.get(\"contributions\", \"No contributions synthesis available.\") if s else \"No synthesis available.\",\n",
    "            inputs=[synthesis_state],\n",
    "            outputs=[contributions_synthesis]\n",
    "        ).then(\n",
    "            fn=lambda s: s.get(\"directions\", \"No directions synthesis available.\") if s else \"No synthesis available.\",\n",
    "            inputs=[synthesis_state],\n",
    "            outputs=[directions_synthesis]\n",
    "        ).then(\n",
    "            fn=lambda: \"Synthesis complete! Review the tabs above for category-specific syntheses.\",\n",
    "            outputs=[synthesis_status]\n",
    "        )\n",
    "        \n",
    "        # Settings update event - now includes use_rag_approach\n",
    "        apply_settings_btn.click(\n",
    "            fn=update_settings,\n",
    "            inputs=[\n",
    "                use_rag_approach, parallel_processing, chunk_size, chunk_overlap, min_importance,\n",
    "                max_claims, max_methods, max_contributions, max_directions,\n",
    "                relationship_confidence, max_cross_relationships, similarity_threshold, scientific_domain\n",
    "            ],\n",
    "            outputs=[settings_status]\n",
    "        )\n",
    "        \n",
    "        # Session management events\n",
    "        save_session_btn.click(\n",
    "            fn=save_current_session,\n",
    "            inputs=[results_state, synthesis_state],\n",
    "            outputs=[synthesis_status]\n",
    "        )\n",
    "        \n",
    "        load_session_btn.click(\n",
    "            fn=load_saved_session,\n",
    "            inputs=[session_file],\n",
    "            outputs=[results_state, settings_status]\n",
    "        ).then(\n",
    "            fn=format_document_list,\n",
    "            inputs=[results_state],\n",
    "            outputs=[document_list]\n",
    "        ).then(\n",
    "            fn=get_doc_selector_options,\n",
    "            inputs=[results_state],\n",
    "            outputs=[doc_selector, doc_selector]\n",
    "        ).then(\n",
    "            fn=filter_update_handler,\n",
    "            inputs=[results_state, doc_selector, importance_filter],\n",
    "            outputs=[claims_table, methods_table, contributions_table, directions_table]\n",
    "        ).then(\n",
    "            fn=format_relationships_table,\n",
    "            inputs=[results_state],\n",
    "            outputs=[relationships_table]\n",
    "        )\n",
    "    \n",
    "    return app\n",
    "\n",
    "# Initialize the system\n",
    "if 'config' not in globals():\n",
    "    # Create default config with RAG enabled\n",
    "    config = LitSynthMultiConfig(use_rag=True)\n",
    "    \n",
    "# Launch the UI\n",
    "app = launch_litsynth_multi_ui()\n",
    "app.launch(quiet=True, inbrowser=True)\n",
    "\n",
    "# Show success message\n",
    "show(\"Multi-Document Literature Synthesis system with RAG approach launched successfully\", \"success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Temp] Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF Processing Diagnostic Script\n",
    "\n",
    "import os\n",
    "import time\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "\n",
    "def diagnose_pdf_processing(file_path):\n",
    "    \"\"\"Diagnose PDF processing issues with detailed error reporting.\"\"\"\n",
    "    \n",
    "    print(f\"🔍 DIAGNOSTIC LOG: PDF Processing Test\")\n",
    "    print(f\"File path: {file_path}\")\n",
    "    print(f\"File exists: {os.path.exists(file_path)}\")\n",
    "    print(f\"File size: {os.path.getsize(file_path) if os.path.exists(file_path) else 'N/A'} bytes\")\n",
    "    \n",
    "    # 1. Test file object structure (simulating Gradio file object)\n",
    "    print(\"\\n== Testing File Structure ==\")\n",
    "    try:\n",
    "        # Create mock document similar to what Gradio would provide\n",
    "        from types import SimpleNamespace\n",
    "        doc = SimpleNamespace(\n",
    "            source_id=\"test_doc\",\n",
    "            title=\"Test Document\",\n",
    "            source_type=\"pdf\",\n",
    "            content=file_path,\n",
    "            processed=False\n",
    "        )\n",
    "        print(\"✓ Created test document object\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error creating document object: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "    \n",
    "    # 2. Test PDF text extraction\n",
    "    print(\"\\n== Testing PDF Extraction ==\")\n",
    "    try:\n",
    "        # Try different PDF extraction methods\n",
    "        print(\"Method 1: PyPDF2\")\n",
    "        import PyPDF2\n",
    "        with open(file_path, 'rb') as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            num_pages = len(reader.pages)\n",
    "            sample_text = reader.pages[0].extract_text()[:100]\n",
    "            print(f\"✓ Pages found: {num_pages}\")\n",
    "            print(f\"✓ Sample text: {sample_text}...\")\n",
    "    except ImportError:\n",
    "        print(\"✗ PyPDF2 not installed\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ PyPDF2 extraction failed: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nMethod 2: pdfplumber\")\n",
    "        import pdfplumber\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            num_pages = len(pdf.pages)\n",
    "            sample_text = pdf.pages[0].extract_text()[:100]\n",
    "            print(f\"✓ Pages found: {num_pages}\")\n",
    "            print(f\"✓ Sample text: {sample_text}...\")\n",
    "    except ImportError:\n",
    "        print(\"✗ pdfplumber not installed\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ pdfplumber extraction failed: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "    \n",
    "    # 3. Test the actual document processing function if available\n",
    "    print(\"\\n== Testing process_document Function ==\")\n",
    "    try:\n",
    "        if 'process_document' in globals():\n",
    "            # Create a simple status callback\n",
    "            def status_callback(doc_id, status):\n",
    "                print(f\"Status ({doc_id}): {status}\")\n",
    "            \n",
    "            # Run the processing function with detailed error capture\n",
    "            start_time = time.time()\n",
    "            result = process_document(doc, doc.source_id, \"balanced\", status_callback)\n",
    "            duration = time.time() - start_time\n",
    "            \n",
    "            print(f\"✓ Processing completed in {duration:.2f}s\")\n",
    "            \n",
    "            if \"error\" in result:\n",
    "                print(f\"✗ Processing error: {result['error']}\")\n",
    "            else:\n",
    "                print(f\"✓ Extracted elements:\")\n",
    "                print(f\"  - Claims: {len(result.get('claims', []))}\")\n",
    "                print(f\"  - Methodologies: {len(result.get('methodologies', []))}\")\n",
    "                print(f\"  - Contributions: {len(result.get('contributions', []))}\")\n",
    "                print(f\"  - Directions: {len(result.get('research_directions', []))}\")\n",
    "        else:\n",
    "            print(\"✗ process_document function not available in global scope\")\n",
    "            \n",
    "            # Try load_document function if available\n",
    "            if 'load_document' in globals():\n",
    "                print(\"\\nTesting load_document function directly\")\n",
    "                doc_data = load_document(\"pdf\", file_path)\n",
    "                text_length = len(doc_data.get(\"text\", \"\"))\n",
    "                print(f\"✓ Extracted text length: {text_length} chars\")\n",
    "                print(f\"✓ Text sample: {doc_data.get('text', '')[:100]}...\")\n",
    "            else:\n",
    "                print(\"✗ load_document function not available in global scope\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error testing process_document: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "    \n",
    "    print(\"\\n== DIAGNOSTIC COMPLETE ==\")\n",
    "\n",
    "# Run the diagnostic on a user-provided PDF\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = input(\"Enter the path to the PDF file to test: \")\n",
    "    diagnose_pdf_processing(pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified Testing Framework\n",
    "\"\"\"\n",
    "Efficient testing framework for LitSynth-Multidoc that samples a few chunks\n",
    "rather than processing entire documents and displays actual extraction content.\n",
    "\"\"\"\n",
    "\n",
    "import time, random, tempfile, os\n",
    "import gradio as gr\n",
    "from typing import Dict, List, Any\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# ===== TEST UTILITIES =====\n",
    "\n",
    "def show(msg, type=\"info\"):\n",
    "    \"\"\"Display formatted messages in the notebook\"\"\"\n",
    "    colors = {\"success\": \"#00C853\", \"info\": \"#2196F3\", \"warning\": \"#FF9800\", \"error\": \"#F44336\"}\n",
    "    icons = {\"success\": \"✅\", \"info\": \"ℹ️\", \"warning\": \"⚠️\", \"error\": \"❌\"}\n",
    "    display(Markdown(f\"<div style='padding:8px;border-radius:4px;background:{colors[type]};color:white'>{icons[type]} {msg}</div>\"))\n",
    "\n",
    "# ===== SIMPLE TEST RUNNER =====\n",
    "\n",
    "def run_sample_test(func_name, func, *args, **kwargs):\n",
    "    \"\"\"Run a single function test and measure performance\"\"\"\n",
    "    # Force debug mode off\n",
    "    global debug, DEBUG_MODE\n",
    "    debug = False\n",
    "    if 'DEBUG_MODE' in globals():\n",
    "        DEBUG_MODE = False\n",
    "    \n",
    "    start_time = time.time()\n",
    "    error = None\n",
    "    result = None\n",
    "    \n",
    "    try:\n",
    "        result = func(*args, **kwargs)\n",
    "        success = True\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        success = False\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    # Determine if this was an LLM call\n",
    "    is_llm = any(name in func_name for name in [\"extract_\", \"identify_\", \"synthesize_\", \"generate_\"])\n",
    "    \n",
    "    # Print result immediately\n",
    "    status = \"✅ success\" if success else f\"❌ error: {error}\"\n",
    "    \n",
    "    details = \"\"\n",
    "    if success:\n",
    "        if isinstance(result, list):\n",
    "            details = f\"count: {len(result)}\"\n",
    "            \n",
    "            # Display content of first item if available\n",
    "            if len(result) > 0:\n",
    "                if hasattr(result[0], 'claim_text'):\n",
    "                    details += f\", first claim: \\\"{result[0].claim_text[:100]}...\\\"\"\n",
    "                elif hasattr(result[0], 'method_name'):\n",
    "                    details += f\", first method: \\\"{result[0].method_name}\\\"\"\n",
    "                elif hasattr(result[0], 'contribution_text'):\n",
    "                    details += f\", first contribution: \\\"{result[0].contribution_text[:100]}...\\\"\"\n",
    "                elif hasattr(result[0], 'direction_text'):\n",
    "                    details += f\", first direction: \\\"{result[0].direction_text[:100]}...\\\"\"\n",
    "        elif hasattr(result, \"model_dump\"):\n",
    "            details = f\"type: {result.__class__.__name__}\"\n",
    "    \n",
    "    print(f\"{func_name}\\t{status}\\t{duration:.2f}s\\t{details}\")\n",
    "    \n",
    "    return {\n",
    "        \"name\": func_name,\n",
    "        \"success\": success,\n",
    "        \"duration\": duration,\n",
    "        \"is_llm\": is_llm,\n",
    "        \"result\": result,\n",
    "        \"error\": error\n",
    "    }\n",
    "\n",
    "# ===== CORE TEST FUNCTIONS =====\n",
    "\n",
    "def sample_chunks_from_document(document, config, num_chunks=2):\n",
    "    \"\"\"Extract a few sample chunks from a document for testing\"\"\"\n",
    "    # Load document\n",
    "    doc_data = load_document(document.source_type, document.content)\n",
    "    text = doc_data.get(\"text\", \"\")\n",
    "    \n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    # Create chunks\n",
    "    all_chunks = chunk_text(text, config)\n",
    "    \n",
    "    # Select random chunks (or all if fewer than requested)\n",
    "    if len(all_chunks) <= num_chunks:\n",
    "        return all_chunks\n",
    "    \n",
    "    # Select evenly distributed chunks for better coverage\n",
    "    indices = [int(i * (len(all_chunks) - 1) / (num_chunks - 1)) for i in range(num_chunks)]\n",
    "    return [all_chunks[i] for i in indices]\n",
    "\n",
    "def run_extraction_tests(document, chunks, config):\n",
    "    \"\"\"Run extraction tests on a small sample of chunks\"\"\"\n",
    "    results = []\n",
    "    extracted_content = {\n",
    "        \"claims\": [],\n",
    "        \"methodologies\": [],\n",
    "        \"contributions\": [],\n",
    "        \"directions\": []\n",
    "    }\n",
    "    \n",
    "    # Test only a few chunks instead of the entire document\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Testing chunk {i+1}/{len(chunks)} (length: {len(chunk)} chars)\")\n",
    "        \n",
    "        # Test claim extraction\n",
    "        claims_result = run_sample_test(\n",
    "            \"extract_scientific_claims\",\n",
    "            extract_scientific_claims,\n",
    "            chunk, config, document.source_id\n",
    "        )\n",
    "        results.append(claims_result)\n",
    "        if claims_result[\"success\"] and claims_result[\"result\"]:\n",
    "            extracted_content[\"claims\"].extend(claims_result[\"result\"])\n",
    "        \n",
    "        # Test methodology extraction\n",
    "        methods_result = run_sample_test(\n",
    "            \"extract_methodologies\",\n",
    "            extract_methodologies,\n",
    "            chunk, config, document.source_id\n",
    "        )\n",
    "        results.append(methods_result)\n",
    "        if methods_result[\"success\"] and methods_result[\"result\"]:\n",
    "            extracted_content[\"methodologies\"].extend(methods_result[\"result\"])\n",
    "        \n",
    "        # Test contribution extraction\n",
    "        contribs_result = run_sample_test(\n",
    "            \"extract_key_contributions\",\n",
    "            extract_key_contributions,\n",
    "            chunk, config, document.source_id\n",
    "        )\n",
    "        results.append(contribs_result)\n",
    "        if contribs_result[\"success\"] and contribs_result[\"result\"]:\n",
    "            extracted_content[\"contributions\"].extend(contribs_result[\"result\"])\n",
    "        \n",
    "        # Test direction extraction\n",
    "        dirs_result = run_sample_test(\n",
    "            \"extract_research_directions\",\n",
    "            extract_research_directions,\n",
    "            chunk, config, document.source_id\n",
    "        )\n",
    "        results.append(dirs_result)\n",
    "        if dirs_result[\"success\"] and dirs_result[\"result\"]:\n",
    "            extracted_content[\"directions\"].extend(dirs_result[\"result\"])\n",
    "    \n",
    "    # Display summary of extracted content\n",
    "    print(\"\\nExtracted Content Summary:\")\n",
    "    for category, items in extracted_content.items():\n",
    "        print(f\"- {category.title()}: {len(items)} items\")\n",
    "        \n",
    "        # Display first 2 items of each category\n",
    "        for idx, item in enumerate(items[:2]):\n",
    "            if hasattr(item, 'claim_text'):\n",
    "                print(f\"  {idx+1}. Claim: \\\"{item.claim_text}\\\"\")\n",
    "            elif hasattr(item, 'method_name'):\n",
    "                print(f\"  {idx+1}. Method: \\\"{item.method_name}\\\" - {item.description[:100]}...\")\n",
    "            elif hasattr(item, 'contribution_text'):\n",
    "                print(f\"  {idx+1}. Contribution: \\\"{item.contribution_text}\\\"\")\n",
    "            elif hasattr(item, 'direction_text'):\n",
    "                print(f\"  {idx+1}. Direction: \\\"{item.direction_text}\\\"\")\n",
    "    \n",
    "    return results, extracted_content\n",
    "\n",
    "def run_state_tests(config):\n",
    "    \"\"\"Test state management functionality\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Create test state\n",
    "    state = LitSynthMultiState(config=config)\n",
    "    \n",
    "    # Test document addition\n",
    "    results.append(run_sample_test(\n",
    "        \"state_add_document\",\n",
    "        state.add_document,\n",
    "        DocumentSource(\n",
    "            source_id=\"test_doc\",\n",
    "            title=\"Test Document\",\n",
    "            authors=[\"Test Author\"],\n",
    "            source_type=\"text\",\n",
    "            content=\"Test content\"\n",
    "        )\n",
    "    ))\n",
    "    \n",
    "    # Test state serialization\n",
    "    temp_file = os.path.join(tempfile.mkdtemp(), \"test_state.json\")\n",
    "    results.append(run_sample_test(\n",
    "        \"save_session\",\n",
    "        save_session,\n",
    "        state, temp_file\n",
    "    ))\n",
    "    \n",
    "    # Test state loading\n",
    "    if os.path.exists(temp_file):\n",
    "        results.append(run_sample_test(\n",
    "            \"load_session\",\n",
    "            load_session,\n",
    "            temp_file\n",
    "        ))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ===== MAIN TEST RUNNER =====\n",
    "\n",
    "def run_quick_tests(pdf_files=None):\n",
    "    \"\"\"Run simplified tests focusing on core functionality\"\"\"\n",
    "    print(\"Running simplified LitSynth-Multidoc tests...\")\n",
    "    print(\"Function\\tStatus\\tLatency\\tDetails\")\n",
    "    \n",
    "    all_results = []\n",
    "    test_docs = []\n",
    "    all_extracted_content = {}\n",
    "    \n",
    "    # Create config with sensible defaults for testing\n",
    "    config = LitSynthMultiConfig().model_dump()\n",
    "    \n",
    "    # Test data models\n",
    "    try:\n",
    "        DocumentSource(source_id=\"test\", title=\"Test\", source_type=\"text\", content=\"test\")\n",
    "        print(\"DocumentSource Model\\t✅ success\\t0.00s\\t-\")\n",
    "    except Exception as e:\n",
    "        print(f\"DocumentSource Model\\t❌ error\\t0.00s\\t{str(e)}\")\n",
    "    \n",
    "    try:\n",
    "        LitSynthMultiConfig()\n",
    "        print(\"LitSynthMultiConfig Model\\t✅ success\\t0.00s\\t-\")\n",
    "    except Exception as e:\n",
    "        print(f\"LitSynthMultiConfig Model\\t❌ error\\t0.00s\\t{str(e)}\")\n",
    "    \n",
    "    try:\n",
    "        LitSynthMultiState(config=config)\n",
    "        print(\"State Management Model\\t✅ success\\t0.00s\\t-\")\n",
    "    except Exception as e:\n",
    "        print(f\"State Management Model\\t❌ error\\t0.00s\\t{str(e)}\")\n",
    "    \n",
    "    # Prepare test documents\n",
    "    if pdf_files:\n",
    "        # Use uploaded PDFs\n",
    "        for i, pdf_file in enumerate(pdf_files[:2]):\n",
    "            doc = DocumentSource(\n",
    "                source_id=f\"doc_{i}\",\n",
    "                title=f\"Test Document {i+1}\",\n",
    "                authors=[\"Test Author\"],\n",
    "                source_type=\"pdf\",\n",
    "                content=pdf_file,\n",
    "                processed=False\n",
    "            )\n",
    "            test_docs.append(doc)\n",
    "    else:\n",
    "        # Create a synthetic test document\n",
    "        test_docs.append(DocumentSource(\n",
    "            source_id=\"test_doc\",\n",
    "            title=\"Test Document\",\n",
    "            authors=[\"Test Author\"],\n",
    "            source_type=\"text\",\n",
    "            content=\"\"\"\n",
    "            ABSTRACT\n",
    "            This study introduces a novel approach for climate prediction using transformer models.\n",
    "            \n",
    "            METHODOLOGY\n",
    "            We combined transformer architecture with physics-informed constraints.\n",
    "            \n",
    "            RESULTS\n",
    "            The model achieved 89% accuracy in predicting extreme weather events.\n",
    "            \n",
    "            FUTURE DIRECTIONS\n",
    "            Future research should explore applications to extreme weather event prediction.\n",
    "            \"\"\",\n",
    "            processed=False\n",
    "        ))\n",
    "    \n",
    "    # Test document loading\n",
    "    all_results.append(run_sample_test(\n",
    "        \"load_document_collection\",\n",
    "        load_document_collection,\n",
    "        test_docs\n",
    "    ))\n",
    "    \n",
    "    # Sample chunks for testing\n",
    "    for doc in test_docs:\n",
    "        chunks = sample_chunks_from_document(doc, config, num_chunks=2)  # Reduced to 2 chunks for faster testing\n",
    "        if chunks:\n",
    "            print(f\"Sampled {len(chunks)} chunks from {doc.title}\")\n",
    "            # Run extraction tests on sample chunks\n",
    "            extraction_results, extracted_content = run_extraction_tests(doc, chunks, config)\n",
    "            all_results.extend(extraction_results)\n",
    "            all_extracted_content[doc.source_id] = extracted_content\n",
    "    \n",
    "    # Test state management\n",
    "    state_results = run_state_tests(config)\n",
    "    all_results.extend(state_results)\n",
    "    \n",
    "    # Calculate summary metrics\n",
    "    total_tests = len(all_results)\n",
    "    success_tests = sum(1 for r in all_results if r.get(\"success\", True))\n",
    "    llm_calls = sum(1 for r in all_results if r.get(\"is_llm\", False))\n",
    "    avg_time = sum(r.get(\"duration\", 0) for r in all_results) / total_tests if total_tests > 0 else 0\n",
    "    \n",
    "    # Get extraction counts\n",
    "    total_extracted = sum(\n",
    "        sum(len(items) for items in content.values())\n",
    "        for content in all_extracted_content.values()\n",
    "    )\n",
    "    \n",
    "    # Display summary\n",
    "    summary = f\"\"\"\n",
    "    # Test Summary\n",
    "    Total Tests: {total_tests} | Passed: {success_tests}/{total_tests} | LLM Calls: {llm_calls} | Avg Time: {avg_time:.2f}s\n",
    "    \n",
    "    Total Extracted Elements: {total_extracted}\n",
    "    \n",
    "    All core functions tested successfully. Empty results for some extractions are expected\n",
    "    when the sample chunks don't contain relevant information.\n",
    "    \"\"\"\n",
    "    \n",
    "    show(summary, \"info\")\n",
    "    return all_results, all_extracted_content\n",
    "\n",
    "# ===== TEST INTERFACE =====\n",
    "\n",
    "def create_simple_test_interface():\n",
    "    \"\"\"Create simplified test interface with default Gradio styling\"\"\"\n",
    "    \n",
    "    def handle_pdf_upload(files):\n",
    "        \"\"\"Process uploaded PDF files\"\"\"\n",
    "        if not files:\n",
    "            return \"Please upload PDF files to test.\"\n",
    "        \n",
    "        pdf_paths = [f.name for f in files[:2]]\n",
    "        return f\"Ready to test with {len(pdf_paths)} documents.\"\n",
    "    \n",
    "    def run_tests(files):\n",
    "        \"\"\"Run tests with uploaded PDFs\"\"\"\n",
    "        if not files:\n",
    "            return \"Please upload at least one PDF file.\"\n",
    "        \n",
    "        pdf_paths = [f.name for f in files[:2]]\n",
    "        \n",
    "        # Create test documents\n",
    "        test_docs = []\n",
    "        for i, path in enumerate(pdf_paths[:2]):\n",
    "            doc = DocumentSource(\n",
    "                source_id=f\"doc_{i}\",\n",
    "                title=f\"Test Document {i+1}\",\n",
    "                authors=[\"Test Author\"],\n",
    "                source_type=\"pdf\",\n",
    "                content=path,\n",
    "                processed=False\n",
    "            )\n",
    "            test_docs.append(doc)\n",
    "        \n",
    "        # Run the tests and capture output\n",
    "        import io\n",
    "        from contextlib import redirect_stdout\n",
    "        \n",
    "        output = io.StringIO()\n",
    "        with redirect_stdout(output):\n",
    "            results, extracted_content = run_quick_tests(pdf_paths)\n",
    "        \n",
    "        # Format results as text\n",
    "        success_count = sum(1 for r in results if r.get(\"success\", False))\n",
    "        llm_calls = sum(1 for r in results if r.get(\"is_llm\", False))\n",
    "        avg_time = sum(r.get(\"duration\", 0) for r in results) / len(results) if results else 0\n",
    "        \n",
    "        # Count total extracted items\n",
    "        total_extracted = sum(\n",
    "            sum(len(items) for items in content.values())\n",
    "            for content in extracted_content.values()\n",
    "        )\n",
    "        \n",
    "        output_text = f\"\"\"\n",
    "## Quick Test Results\n",
    "\n",
    "**Summary:** Passed {success_count}/{len(results)} tests | LLM Calls: {llm_calls} | Avg Time: {avg_time:.2f}s\n",
    "\n",
    "**Total Extracted Elements:** {total_extracted}\n",
    "\n",
    "### Test Output\n",
    "{output.getvalue()}\n",
    "\n",
    "### Extraction Samples\n",
    "\n",
    "\"\"\"\n",
    "        # Add extracted content samples\n",
    "        for doc_id, content in extracted_content.items():\n",
    "            output_text += f\"\\n#### Document: {doc_id}\\n\\n\"\n",
    "            \n",
    "            for category, items in content.items():\n",
    "                output_text += f\"**{category.title()}:** {len(items)} items\\n\\n\"\n",
    "                \n",
    "                for idx, item in enumerate(items[:3]):  # Show up to 3 items per category\n",
    "                    if hasattr(item, 'claim_text'):\n",
    "                        output_text += f\"{idx+1}. **Claim:** {item.claim_text}\\n\\n\"\n",
    "                    elif hasattr(item, 'method_name'):\n",
    "                        output_text += f\"{idx+1}. **Method:** {item.method_name} - {item.description[:100]}...\\n\\n\"\n",
    "                    elif hasattr(item, 'contribution_text'):\n",
    "                        output_text += f\"{idx+1}. **Contribution:** {item.contribution_text}\\n\\n\"\n",
    "                    elif hasattr(item, 'direction_text'):\n",
    "                        output_text += f\"{idx+1}. **Direction:** {item.direction_text}\\n\\n\"\n",
    "        \n",
    "        return output_text\n",
    "    \n",
    "    # Create interface with default Gradio styling\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"## LitSynth-Multidoc Testing\")\n",
    "        gr.Markdown(\"Tests core functionality and displays extraction results\")\n",
    "        \n",
    "        file_input = gr.Files(\n",
    "            label=\"Upload PDF Documents (1-2 files)\",\n",
    "            file_types=[\".pdf\"],\n",
    "            file_count=\"multiple\"\n",
    "        )\n",
    "        \n",
    "        test_btn = gr.Button(\"Run Tests\", variant=\"primary\")\n",
    "        status = gr.Textbox(label=\"Status\", value=\"Upload PDFs to begin\")\n",
    "        results_output = gr.Markdown(\"Results will appear here\")\n",
    "        \n",
    "        # Connect components\n",
    "        file_input.change(\n",
    "            fn=handle_pdf_upload,\n",
    "            inputs=[file_input],\n",
    "            outputs=[status]\n",
    "        )\n",
    "        \n",
    "        test_btn.click(\n",
    "            fn=run_tests,\n",
    "            inputs=[file_input],\n",
    "            outputs=[results_output]\n",
    "        )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "# Launch the interface\n",
    "test_interface = create_simple_test_interface()\n",
    "test_interface.launch(inline=True, share=False)\n",
    "\n",
    "show(\"Simplified testing framework initialized successfully\", \"success\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
